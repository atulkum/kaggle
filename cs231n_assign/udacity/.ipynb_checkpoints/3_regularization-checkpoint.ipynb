{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseTensorFlow:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 256\n",
    "        self.NUM_CLASSES = 10\n",
    "        self.starting_learning_rate = 0.01\n",
    "        self.train_dir = './'\n",
    "        self.num_steps = 50001\n",
    "        self.IMAGE_PIXELS = 784\n",
    "        \n",
    "    def model(self, images, input_size, output_size, isEval=None):\n",
    "        raise Exception('Error', 'Not implemented')\n",
    "    \n",
    "    def loadData(self):\n",
    "        pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "          save = pickle.load(f)\n",
    "          self.train_dataset = save['train_dataset']\n",
    "          self.train_labels = save['train_labels']\n",
    "          self.valid_dataset = save['valid_dataset']\n",
    "          self.valid_labels = save['valid_labels']\n",
    "          self.test_dataset = save['test_dataset']\n",
    "          self.test_labels = save['test_labels']\n",
    "          del save  # hint to help gc free up memory\n",
    "    \n",
    "          self.train_dataset = self.train_dataset.reshape((-1, self.IMAGE_PIXELS)).astype(np.float32)\n",
    "          self.valid_dataset = self.valid_dataset.reshape((-1, self.IMAGE_PIXELS)).astype(np.float32)\n",
    "          self.test_dataset = self.test_dataset.reshape((-1, self.IMAGE_PIXELS)).astype(np.float32)\n",
    "\n",
    "          print 'Training set', self.train_dataset.shape, self.train_labels.shape\n",
    "          print 'Validation set', self.valid_dataset.shape, self.valid_labels.shape\n",
    "          print 'Test set', self.test_dataset.shape, self.test_labels.shape\n",
    "\n",
    "    def loss_function(self,logits, labels):\n",
    "        labels = tf.expand_dims(labels, 1)\n",
    "        indices = tf.expand_dims(tf.range(0, self.batch_size), 1)\n",
    "        concated = tf.concat(1, [indices, labels])\n",
    "        onehot_labels = tf.sparse_to_dense(\n",
    "              concated, tf.pack([self.batch_size, self.NUM_CLASSES]), 1.0, 0.0)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,\n",
    "                                                          onehot_labels,\n",
    "                                                          name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        return loss\n",
    "\n",
    "    def training(self,loss):\n",
    "        train_size = self.train_dataset.shape[0]\n",
    "        tf.scalar_summary(loss.op.name, loss)\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            self.starting_learning_rate,      \n",
    "            global_step * self.batch_size,  \n",
    "            train_size,          \n",
    "            0.95,                \n",
    "            staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op\n",
    "\n",
    "    def evaluation(self,logits, labels):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "\n",
    "    def preapare_placeholder_inputs(self):\n",
    "        self.images_placeholder = tf.placeholder(tf.float32, shape=(self.batch_size,self.IMAGE_PIXELS))\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, shape=(self.batch_size))\n",
    "\n",
    "    def fill_feed_dict(self, dataset, labels, step):\n",
    "        if labels.shape[0] - self.batch_size > 0:\n",
    "            offset = (step * self.batch_size) % (labels.shape[0] - self.batch_size)\n",
    "        else:\n",
    "            offset = 0\n",
    "        images_feed = dataset[offset:(offset + self.batch_size), :]\n",
    "        labels_feed = labels[offset:(offset + self.batch_size)]\n",
    "        feed_dict = {\n",
    "            self.images_placeholder: images_feed,\n",
    "            self.labels_placeholder: labels_feed,\n",
    "        }\n",
    "        return feed_dict\n",
    "    \n",
    "    def do_eval(self,sess,\n",
    "            eval_correct,\n",
    "            dataset, \n",
    "            labels):\n",
    "        true_count = 0  \n",
    "        steps_per_epoch = labels.shape[0] // self.batch_size\n",
    "        num_examples = steps_per_epoch * self.batch_size\n",
    "        for step in xrange(steps_per_epoch):\n",
    "            feed_dict = self.fill_feed_dict(dataset, labels, step)\n",
    "            true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "        \n",
    "        precision = 1.0*true_count / num_examples\n",
    "        print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "            (num_examples, true_count, precision))\n",
    "    \n",
    "    def run_training(self,sess, eval_correct, train_op, loss):\n",
    "        \n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        summary_writer = tf.train.SummaryWriter(self.train_dir, graph_def=sess.graph_def)\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "        feed_dict = self.fill_feed_dict(self.train_dataset, self.train_labels, 0)\n",
    "    \n",
    "        for step in xrange(self.num_steps):\n",
    "            start_time = time.time()\n",
    "            _, loss_value = sess.run([train_op, loss],\n",
    "                                       feed_dict=feed_dict)\n",
    "            \n",
    "            feed_dict = self.fill_feed_dict(self.train_dataset, self.train_labels, step+1)\n",
    "           \n",
    "            duration = time.time() - start_time\n",
    "            if step % 100 == 0:\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                \n",
    "            if (step + 1) % 1000 == 0 or (step + 1) == self.num_steps:\n",
    "                saver.save(sess, self.train_dir, global_step=step)\n",
    "                print('Training Data Eval:')\n",
    "                self.do_eval(sess, eval_correct,\n",
    "                    feed_dict[self.images_placeholder], feed_dict[self.labels_placeholder])\n",
    "                print('Validation Data Eval:')\n",
    "                self.do_eval(sess, eval_correct, self.valid_dataset, self.valid_labels)\n",
    "        \n",
    "    def process(self):\n",
    "        with tf.Graph().as_default():\n",
    "            self.preapare_placeholder_inputs()\n",
    "        \n",
    "            logits_train, regularizer = self.model(self.images_placeholder, \n",
    "                                    self.IMAGE_PIXELS, self.NUM_CLASSES)\n",
    "            loss = self.loss_function(logits_train, self.labels_placeholder)\n",
    "            loss += 5e-4 * regularizer\n",
    "            train_op = self.training(loss)\n",
    "        \n",
    "            logits_eval = self.model(self.images_placeholder, \n",
    "                                     self.IMAGE_PIXELS, self.NUM_CLASSES, isEval=True)\n",
    "            eval_correct = self.evaluation(logits_eval, self.labels_placeholder)\n",
    "    \n",
    "            with tf.Session() as sess:\n",
    "                init = tf.initialize_all_variables()\n",
    "                sess.run(init)\n",
    "            \n",
    "                self.run_training(sess, eval_correct, train_op, loss)\n",
    "                print('Test Data Eval:')\n",
    "                self.do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    self.test_dataset, self.test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compue the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (190000, 784) (190000,)\n",
      "Validation set (10000, 784) (10000,)\n",
      "Test set (18724, 784) (18724,)\n",
      "Step 0: loss = 2.37 (0.057 sec)\n",
      "Step 100: loss = 1.51 (0.010 sec)\n",
      "Step 200: loss = 1.20 (0.022 sec)\n",
      "Step 300: loss = 1.04 (0.011 sec)\n",
      "Step 400: loss = 0.82 (0.023 sec)\n",
      "Step 500: loss = 0.81 (0.010 sec)\n",
      "Step 600: loss = 0.82 (0.010 sec)\n",
      "Step 700: loss = 0.76 (0.013 sec)\n",
      "Step 800: loss = 0.70 (0.021 sec)\n",
      "Step 900: loss = 0.91 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 203  Precision @ 1: 0.7930\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8071  Precision @ 1: 0.8084\n",
      "Step 1000: loss = 0.81 (0.012 sec)\n",
      "Step 1100: loss = 0.73 (0.011 sec)\n",
      "Step 1200: loss = 0.59 (0.010 sec)\n",
      "Step 1300: loss = 0.73 (0.011 sec)\n",
      "Step 1400: loss = 0.74 (0.010 sec)\n",
      "Step 1500: loss = 0.66 (0.010 sec)\n",
      "Step 1600: loss = 0.64 (0.011 sec)\n",
      "Step 1700: loss = 0.65 (0.011 sec)\n",
      "Step 1800: loss = 0.72 (0.010 sec)\n",
      "Step 1900: loss = 0.60 (0.017 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 212  Precision @ 1: 0.8281\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8208  Precision @ 1: 0.8221\n",
      "Step 2000: loss = 0.62 (0.023 sec)\n",
      "Step 2100: loss = 0.70 (0.010 sec)\n",
      "Step 2200: loss = 0.78 (0.015 sec)\n",
      "Step 2300: loss = 0.63 (0.012 sec)\n",
      "Step 2400: loss = 0.63 (0.014 sec)\n",
      "Step 2500: loss = 0.61 (0.010 sec)\n",
      "Step 2600: loss = 0.60 (0.010 sec)\n",
      "Step 2700: loss = 0.52 (0.010 sec)\n",
      "Step 2800: loss = 0.83 (0.013 sec)\n",
      "Step 2900: loss = 0.67 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 214  Precision @ 1: 0.8359\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8278  Precision @ 1: 0.8291\n",
      "Step 3000: loss = 0.69 (0.013 sec)\n",
      "Step 3100: loss = 0.66 (0.010 sec)\n",
      "Step 3200: loss = 0.69 (0.012 sec)\n",
      "Step 3300: loss = 0.71 (0.010 sec)\n",
      "Step 3400: loss = 0.56 (0.012 sec)\n",
      "Step 3500: loss = 0.65 (0.019 sec)\n",
      "Step 3600: loss = 0.58 (0.010 sec)\n",
      "Step 3700: loss = 0.58 (0.011 sec)\n",
      "Step 3800: loss = 0.68 (0.013 sec)\n",
      "Step 3900: loss = 0.60 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 223  Precision @ 1: 0.8711\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8326  Precision @ 1: 0.8339\n",
      "Step 4000: loss = 0.53 (0.017 sec)\n",
      "Step 4100: loss = 0.67 (0.010 sec)\n",
      "Step 4200: loss = 0.54 (0.013 sec)\n",
      "Step 4300: loss = 0.64 (0.010 sec)\n",
      "Step 4400: loss = 0.65 (0.022 sec)\n",
      "Step 4500: loss = 0.64 (0.013 sec)\n",
      "Step 4600: loss = 0.58 (0.011 sec)\n",
      "Step 4700: loss = 0.62 (0.016 sec)\n",
      "Step 4800: loss = 0.60 (0.019 sec)\n",
      "Step 4900: loss = 0.62 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 211  Precision @ 1: 0.8242\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8358  Precision @ 1: 0.8371\n",
      "Step 5000: loss = 0.66 (0.021 sec)\n",
      "Step 5100: loss = 0.64 (0.010 sec)\n",
      "Step 5200: loss = 0.54 (0.010 sec)\n",
      "Step 5300: loss = 0.54 (0.010 sec)\n",
      "Step 5400: loss = 0.68 (0.010 sec)\n",
      "Step 5500: loss = 0.59 (0.013 sec)\n",
      "Step 5600: loss = 0.62 (0.018 sec)\n",
      "Step 5700: loss = 0.76 (0.017 sec)\n",
      "Step 5800: loss = 0.50 (0.010 sec)\n",
      "Step 5900: loss = 0.58 (0.014 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 216  Precision @ 1: 0.8438\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8379  Precision @ 1: 0.8392\n",
      "Step 6000: loss = 0.56 (0.017 sec)\n",
      "Step 6100: loss = 0.63 (0.010 sec)\n",
      "Step 6200: loss = 0.67 (0.012 sec)\n",
      "Step 6300: loss = 0.56 (0.011 sec)\n",
      "Step 6400: loss = 0.65 (0.011 sec)\n",
      "Step 6500: loss = 0.61 (0.012 sec)\n",
      "Step 6600: loss = 0.52 (0.021 sec)\n",
      "Step 6700: loss = 0.71 (0.010 sec)\n",
      "Step 6800: loss = 0.66 (0.010 sec)\n",
      "Step 6900: loss = 0.62 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 213  Precision @ 1: 0.8320\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8396  Precision @ 1: 0.8409\n",
      "Step 7000: loss = 0.69 (0.018 sec)\n",
      "Step 7100: loss = 0.63 (0.011 sec)\n",
      "Step 7200: loss = 0.55 (0.010 sec)\n",
      "Step 7300: loss = 0.55 (0.010 sec)\n",
      "Step 7400: loss = 0.60 (0.013 sec)\n",
      "Step 7500: loss = 0.59 (0.018 sec)\n",
      "Step 7600: loss = 0.64 (0.011 sec)\n",
      "Step 7700: loss = 0.60 (0.010 sec)\n",
      "Step 7800: loss = 0.62 (0.012 sec)\n",
      "Step 7900: loss = 0.61 (0.019 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 215  Precision @ 1: 0.8398\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8414  Precision @ 1: 0.8427\n",
      "Step 8000: loss = 0.62 (0.014 sec)\n",
      "Step 8100: loss = 0.57 (0.011 sec)\n",
      "Step 8200: loss = 0.75 (0.011 sec)\n",
      "Step 8300: loss = 0.62 (0.010 sec)\n",
      "Step 8400: loss = 0.61 (0.011 sec)\n",
      "Step 8500: loss = 0.45 (0.010 sec)\n",
      "Step 8600: loss = 0.57 (0.010 sec)\n",
      "Step 8700: loss = 0.58 (0.011 sec)\n",
      "Step 8800: loss = 0.58 (0.012 sec)\n",
      "Step 8900: loss = 0.59 (0.029 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 208  Precision @ 1: 0.8125\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8437  Precision @ 1: 0.8451\n",
      "Step 9000: loss = 0.62 (0.012 sec)\n",
      "Step 9100: loss = 0.66 (0.010 sec)\n",
      "Step 9200: loss = 0.49 (0.010 sec)\n",
      "Step 9300: loss = 0.65 (0.010 sec)\n",
      "Step 9400: loss = 0.52 (0.010 sec)\n",
      "Step 9500: loss = 0.53 (0.010 sec)\n",
      "Step 9600: loss = 0.67 (0.010 sec)\n",
      "Step 9700: loss = 0.59 (0.010 sec)\n",
      "Step 9800: loss = 0.51 (0.010 sec)\n",
      "Step 9900: loss = 0.61 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 222  Precision @ 1: 0.8672\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8437  Precision @ 1: 0.8451\n",
      "Step 10000: loss = 0.60 (0.012 sec)\n",
      "Step 10100: loss = 0.61 (0.010 sec)\n",
      "Step 10200: loss = 0.60 (0.010 sec)\n",
      "Step 10300: loss = 0.61 (0.010 sec)\n",
      "Step 10400: loss = 0.71 (0.010 sec)\n",
      "Step 10500: loss = 0.68 (0.010 sec)\n",
      "Step 10600: loss = 0.57 (0.010 sec)\n",
      "Step 10700: loss = 0.74 (0.014 sec)\n",
      "Step 10800: loss = 0.54 (0.010 sec)\n",
      "Step 10900: loss = 0.56 (0.023 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 221  Precision @ 1: 0.8633\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8438  Precision @ 1: 0.8452\n",
      "Step 11000: loss = 0.51 (0.012 sec)\n",
      "Step 11100: loss = 0.63 (0.010 sec)\n",
      "Step 11200: loss = 0.62 (0.010 sec)\n",
      "Step 11300: loss = 0.49 (0.026 sec)\n",
      "Step 11400: loss = 0.56 (0.010 sec)\n",
      "Step 11500: loss = 0.78 (0.013 sec)\n",
      "Step 11600: loss = 0.47 (0.011 sec)\n",
      "Step 11700: loss = 0.51 (0.011 sec)\n",
      "Step 11800: loss = 0.61 (0.014 sec)\n",
      "Step 11900: loss = 0.66 (0.012 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 223  Precision @ 1: 0.8711\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8461  Precision @ 1: 0.8475\n",
      "Step 12000: loss = 0.60 (0.013 sec)\n",
      "Step 12100: loss = 0.60 (0.010 sec)\n",
      "Step 12200: loss = 0.58 (0.011 sec)\n",
      "Step 12300: loss = 0.64 (0.020 sec)\n",
      "Step 12400: loss = 0.71 (0.011 sec)\n",
      "Step 12500: loss = 0.61 (0.012 sec)\n",
      "Step 12600: loss = 0.72 (0.011 sec)\n",
      "Step 12700: loss = 0.51 (0.010 sec)\n",
      "Step 12800: loss = 0.64 (0.010 sec)\n",
      "Step 12900: loss = 0.54 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 227  Precision @ 1: 0.8867\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8468  Precision @ 1: 0.8482\n",
      "Step 13000: loss = 0.50 (0.016 sec)\n",
      "Step 13100: loss = 0.47 (0.015 sec)\n",
      "Step 13200: loss = 0.57 (0.015 sec)\n",
      "Step 13300: loss = 0.62 (0.015 sec)\n",
      "Step 13400: loss = 0.60 (0.010 sec)\n",
      "Step 13500: loss = 0.68 (0.010 sec)\n",
      "Step 13600: loss = 0.60 (0.010 sec)\n",
      "Step 13700: loss = 0.59 (0.010 sec)\n",
      "Step 13800: loss = 0.50 (0.012 sec)\n",
      "Step 13900: loss = 0.60 (0.015 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 215  Precision @ 1: 0.8398\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8474  Precision @ 1: 0.8488\n",
      "Step 14000: loss = 0.64 (0.013 sec)\n",
      "Step 14100: loss = 0.58 (0.010 sec)\n",
      "Step 14200: loss = 0.58 (0.018 sec)\n",
      "Step 14300: loss = 0.55 (0.027 sec)\n",
      "Step 14400: loss = 0.55 (0.010 sec)\n",
      "Step 14500: loss = 0.56 (0.013 sec)\n",
      "Step 14600: loss = 0.58 (0.010 sec)\n",
      "Step 14700: loss = 0.60 (0.017 sec)\n",
      "Step 14800: loss = 0.66 (0.010 sec)\n",
      "Step 14900: loss = 0.60 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 219  Precision @ 1: 0.8555\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8484  Precision @ 1: 0.8498\n",
      "Step 15000: loss = 0.51 (0.012 sec)\n",
      "Step 15100: loss = 0.58 (0.010 sec)\n",
      "Step 15200: loss = 0.51 (0.013 sec)\n",
      "Step 15300: loss = 0.46 (0.010 sec)\n",
      "Step 15400: loss = 0.70 (0.010 sec)\n",
      "Step 15500: loss = 0.65 (0.010 sec)\n",
      "Step 15600: loss = 0.62 (0.010 sec)\n",
      "Step 15700: loss = 0.51 (0.010 sec)\n",
      "Step 15800: loss = 0.57 (0.010 sec)\n",
      "Step 15900: loss = 0.59 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 229  Precision @ 1: 0.8945\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8488  Precision @ 1: 0.8502\n",
      "Step 16000: loss = 0.48 (0.012 sec)\n",
      "Step 16100: loss = 0.54 (0.010 sec)\n",
      "Step 16200: loss = 0.51 (0.010 sec)\n",
      "Step 16300: loss = 0.56 (0.010 sec)\n",
      "Step 16400: loss = 0.53 (0.010 sec)\n",
      "Step 16500: loss = 0.52 (0.010 sec)\n",
      "Step 16600: loss = 0.51 (0.010 sec)\n",
      "Step 16700: loss = 0.58 (0.010 sec)\n",
      "Step 16800: loss = 0.51 (0.010 sec)\n",
      "Step 16900: loss = 0.61 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 230  Precision @ 1: 0.8984\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8484  Precision @ 1: 0.8498\n",
      "Step 17000: loss = 0.50 (0.014 sec)\n",
      "Step 17100: loss = 0.55 (0.019 sec)\n",
      "Step 17200: loss = 0.52 (0.010 sec)\n",
      "Step 17300: loss = 0.58 (0.010 sec)\n",
      "Step 17400: loss = 0.56 (0.011 sec)\n",
      "Step 17500: loss = 0.56 (0.011 sec)\n",
      "Step 17600: loss = 0.62 (0.010 sec)\n",
      "Step 17700: loss = 0.56 (0.010 sec)\n",
      "Step 17800: loss = 0.52 (0.010 sec)\n",
      "Step 17900: loss = 0.39 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 216  Precision @ 1: 0.8438\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8493  Precision @ 1: 0.8507\n",
      "Step 18000: loss = 0.58 (0.013 sec)\n",
      "Step 18100: loss = 0.56 (0.010 sec)\n",
      "Step 18200: loss = 0.59 (0.010 sec)\n",
      "Step 18300: loss = 0.73 (0.010 sec)\n",
      "Step 18400: loss = 0.51 (0.010 sec)\n",
      "Step 18500: loss = 0.49 (0.010 sec)\n",
      "Step 18600: loss = 0.48 (0.010 sec)\n",
      "Step 18700: loss = 0.53 (0.010 sec)\n",
      "Step 18800: loss = 0.65 (0.010 sec)\n",
      "Step 18900: loss = 0.54 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 217  Precision @ 1: 0.8477\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8499  Precision @ 1: 0.8513\n",
      "Step 19000: loss = 0.59 (0.012 sec)\n",
      "Step 19100: loss = 0.59 (0.029 sec)\n",
      "Step 19200: loss = 0.52 (0.010 sec)\n",
      "Step 19300: loss = 0.67 (0.010 sec)\n",
      "Step 19400: loss = 0.57 (0.010 sec)\n",
      "Step 19500: loss = 0.56 (0.010 sec)\n",
      "Step 19600: loss = 0.69 (0.010 sec)\n",
      "Step 19700: loss = 0.57 (0.011 sec)\n",
      "Step 19800: loss = 0.51 (0.012 sec)\n",
      "Step 19900: loss = 0.50 (0.017 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 217  Precision @ 1: 0.8477\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8506  Precision @ 1: 0.8520\n",
      "Step 20000: loss = 0.53 (0.015 sec)\n",
      "Step 20100: loss = 0.50 (0.013 sec)\n",
      "Step 20200: loss = 0.58 (0.010 sec)\n",
      "Step 20300: loss = 0.54 (0.010 sec)\n",
      "Step 20400: loss = 0.62 (0.010 sec)\n",
      "Step 20500: loss = 0.59 (0.015 sec)\n",
      "Step 20600: loss = 0.57 (0.010 sec)\n",
      "Step 20700: loss = 0.48 (0.011 sec)\n",
      "Step 20800: loss = 0.70 (0.016 sec)\n",
      "Step 20900: loss = 0.63 (0.011 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 227  Precision @ 1: 0.8867\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8503  Precision @ 1: 0.8517\n",
      "Step 21000: loss = 0.55 (0.013 sec)\n",
      "Step 21100: loss = 0.49 (0.010 sec)\n",
      "Step 21200: loss = 0.51 (0.010 sec)\n",
      "Step 21300: loss = 0.55 (0.010 sec)\n",
      "Step 21400: loss = 0.54 (0.011 sec)\n",
      "Step 21500: loss = 0.50 (0.011 sec)\n",
      "Step 21600: loss = 0.61 (0.010 sec)\n",
      "Step 21700: loss = 0.67 (0.016 sec)\n",
      "Step 21800: loss = 0.49 (0.011 sec)\n",
      "Step 21900: loss = 0.66 (0.012 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 225  Precision @ 1: 0.8789\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8507  Precision @ 1: 0.8521\n",
      "Step 22000: loss = 0.49 (0.014 sec)\n",
      "Step 22100: loss = 0.53 (0.020 sec)\n",
      "Step 22200: loss = 0.60 (0.010 sec)\n",
      "Step 22300: loss = 0.51 (0.010 sec)\n",
      "Step 22400: loss = 0.50 (0.010 sec)\n",
      "Step 22500: loss = 0.58 (0.010 sec)\n",
      "Step 22600: loss = 0.57 (0.010 sec)\n",
      "Step 22700: loss = 0.58 (0.010 sec)\n",
      "Step 22800: loss = 0.57 (0.010 sec)\n",
      "Step 22900: loss = 0.54 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 217  Precision @ 1: 0.8477\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8509  Precision @ 1: 0.8523\n",
      "Step 23000: loss = 0.63 (0.012 sec)\n",
      "Step 23100: loss = 0.64 (0.011 sec)\n",
      "Step 23200: loss = 0.56 (0.010 sec)\n",
      "Step 23300: loss = 0.67 (0.012 sec)\n",
      "Step 23400: loss = 0.52 (0.014 sec)\n",
      "Step 23500: loss = 0.53 (0.011 sec)\n",
      "Step 23600: loss = 0.48 (0.014 sec)\n",
      "Step 23700: loss = 0.59 (0.010 sec)\n",
      "Step 23800: loss = 0.56 (0.011 sec)\n",
      "Step 23900: loss = 0.45 (0.021 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 220  Precision @ 1: 0.8594\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8516  Precision @ 1: 0.8530\n",
      "Step 24000: loss = 0.56 (0.013 sec)\n",
      "Step 24100: loss = 0.68 (0.010 sec)\n",
      "Step 24200: loss = 0.45 (0.010 sec)\n",
      "Step 24300: loss = 0.57 (0.011 sec)\n",
      "Step 24400: loss = 0.51 (0.010 sec)\n",
      "Step 24500: loss = 0.64 (0.010 sec)\n",
      "Step 24600: loss = 0.56 (0.017 sec)\n",
      "Step 24700: loss = 0.58 (0.021 sec)\n",
      "Step 24800: loss = 0.52 (0.010 sec)\n",
      "Step 24900: loss = 0.65 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 212  Precision @ 1: 0.8281\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8510  Precision @ 1: 0.8524\n",
      "Step 25000: loss = 0.64 (0.013 sec)\n",
      "Step 25100: loss = 0.55 (0.010 sec)\n",
      "Step 25200: loss = 0.64 (0.010 sec)\n",
      "Step 25300: loss = 0.54 (0.014 sec)\n",
      "Step 25400: loss = 0.60 (0.014 sec)\n",
      "Step 25500: loss = 0.50 (0.010 sec)\n",
      "Step 25600: loss = 0.46 (0.011 sec)\n",
      "Step 25700: loss = 0.45 (0.010 sec)\n",
      "Step 25800: loss = 0.49 (0.010 sec)\n",
      "Step 25900: loss = 0.58 (0.012 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 217  Precision @ 1: 0.8477\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8519  Precision @ 1: 0.8533\n",
      "Step 26000: loss = 0.58 (0.013 sec)\n",
      "Step 26100: loss = 0.68 (0.010 sec)\n",
      "Step 26200: loss = 0.57 (0.026 sec)\n",
      "Step 26300: loss = 0.61 (0.011 sec)\n",
      "Step 26400: loss = 0.52 (0.010 sec)\n",
      "Step 26500: loss = 0.59 (0.011 sec)\n",
      "Step 26600: loss = 0.61 (0.011 sec)\n",
      "Step 26700: loss = 0.53 (0.010 sec)\n",
      "Step 26800: loss = 0.58 (0.011 sec)\n",
      "Step 26900: loss = 0.55 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 223  Precision @ 1: 0.8711\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8523  Precision @ 1: 0.8537\n",
      "Step 27000: loss = 0.52 (0.014 sec)\n",
      "Step 27100: loss = 0.56 (0.012 sec)\n",
      "Step 27200: loss = 0.62 (0.013 sec)\n",
      "Step 27300: loss = 0.57 (0.014 sec)\n",
      "Step 27400: loss = 0.63 (0.018 sec)\n",
      "Step 27500: loss = 0.63 (0.015 sec)\n",
      "Step 27600: loss = 0.54 (0.010 sec)\n",
      "Step 27700: loss = 0.66 (0.017 sec)\n",
      "Step 27800: loss = 0.50 (0.011 sec)\n",
      "Step 27900: loss = 0.43 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 217  Precision @ 1: 0.8477\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8525  Precision @ 1: 0.8539\n",
      "Step 28000: loss = 0.60 (0.013 sec)\n",
      "Step 28100: loss = 0.58 (0.010 sec)\n",
      "Step 28200: loss = 0.63 (0.037 sec)\n",
      "Step 28300: loss = 0.46 (0.013 sec)\n",
      "Step 28400: loss = 0.52 (0.011 sec)\n",
      "Step 28500: loss = 0.59 (0.011 sec)\n",
      "Step 28600: loss = 0.50 (0.011 sec)\n",
      "Step 28700: loss = 0.54 (0.010 sec)\n",
      "Step 28800: loss = 0.53 (0.014 sec)\n",
      "Step 28900: loss = 0.49 (0.013 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 230  Precision @ 1: 0.8984\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8533  Precision @ 1: 0.8547\n",
      "Step 29000: loss = 0.47 (0.019 sec)\n",
      "Step 29100: loss = 0.48 (0.027 sec)\n",
      "Step 29200: loss = 0.48 (0.012 sec)\n",
      "Step 29300: loss = 0.54 (0.011 sec)\n",
      "Step 29400: loss = 0.50 (0.010 sec)\n",
      "Step 29500: loss = 0.60 (0.010 sec)\n",
      "Step 29600: loss = 0.51 (0.010 sec)\n",
      "Step 29700: loss = 0.52 (0.010 sec)\n",
      "Step 29800: loss = 0.54 (0.013 sec)\n",
      "Step 29900: loss = 0.54 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 217  Precision @ 1: 0.8477\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8528  Precision @ 1: 0.8542\n",
      "Step 30000: loss = 0.55 (0.012 sec)\n",
      "Step 30100: loss = 0.46 (0.012 sec)\n",
      "Step 30200: loss = 0.58 (0.011 sec)\n",
      "Step 30300: loss = 0.49 (0.010 sec)\n",
      "Step 30400: loss = 0.63 (0.020 sec)\n",
      "Step 30500: loss = 0.41 (0.014 sec)\n",
      "Step 30600: loss = 0.51 (0.010 sec)\n",
      "Step 30700: loss = 0.58 (0.010 sec)\n",
      "Step 30800: loss = 0.58 (0.010 sec)\n",
      "Step 30900: loss = 0.68 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 225  Precision @ 1: 0.8789\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8530  Precision @ 1: 0.8544\n",
      "Step 31000: loss = 0.52 (0.014 sec)\n",
      "Step 31100: loss = 0.54 (0.010 sec)\n",
      "Step 31200: loss = 0.52 (0.013 sec)\n",
      "Step 31300: loss = 0.55 (0.011 sec)\n",
      "Step 31400: loss = 0.70 (0.010 sec)\n",
      "Step 31500: loss = 0.63 (0.010 sec)\n",
      "Step 31600: loss = 0.54 (0.020 sec)\n",
      "Step 31700: loss = 0.58 (0.014 sec)\n",
      "Step 31800: loss = 0.54 (0.010 sec)\n",
      "Step 31900: loss = 0.67 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 219  Precision @ 1: 0.8555\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8532  Precision @ 1: 0.8546\n",
      "Step 32000: loss = 0.54 (0.014 sec)\n",
      "Step 32100: loss = 0.55 (0.010 sec)\n",
      "Step 32200: loss = 0.74 (0.010 sec)\n",
      "Step 32300: loss = 0.51 (0.010 sec)\n",
      "Step 32400: loss = 0.50 (0.010 sec)\n",
      "Step 32500: loss = 0.48 (0.010 sec)\n",
      "Step 32600: loss = 0.50 (0.010 sec)\n",
      "Step 32700: loss = 0.49 (0.010 sec)\n",
      "Step 32800: loss = 0.55 (0.010 sec)\n",
      "Step 32900: loss = 0.51 (0.014 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 214  Precision @ 1: 0.8359\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8534  Precision @ 1: 0.8548\n",
      "Step 33000: loss = 0.59 (0.012 sec)\n",
      "Step 33100: loss = 0.57 (0.018 sec)\n",
      "Step 33200: loss = 0.55 (0.020 sec)\n",
      "Step 33300: loss = 0.50 (0.012 sec)\n",
      "Step 33400: loss = 0.65 (0.010 sec)\n",
      "Step 33500: loss = 0.64 (0.011 sec)\n",
      "Step 33600: loss = 0.56 (0.010 sec)\n",
      "Step 33700: loss = 0.45 (0.013 sec)\n",
      "Step 33800: loss = 0.50 (0.010 sec)\n",
      "Step 33900: loss = 0.54 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 219  Precision @ 1: 0.8555\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8532  Precision @ 1: 0.8546\n",
      "Step 34000: loss = 0.59 (0.012 sec)\n",
      "Step 34100: loss = 0.56 (0.011 sec)\n",
      "Step 34200: loss = 0.57 (0.010 sec)\n",
      "Step 34300: loss = 0.62 (0.010 sec)\n",
      "Step 34400: loss = 0.46 (0.010 sec)\n",
      "Step 34500: loss = 0.62 (0.010 sec)\n",
      "Step 34600: loss = 0.51 (0.012 sec)\n",
      "Step 34700: loss = 0.57 (0.010 sec)\n",
      "Step 34800: loss = 0.49 (0.010 sec)\n",
      "Step 34900: loss = 0.48 (0.011 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 223  Precision @ 1: 0.8711\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8539  Precision @ 1: 0.8553\n",
      "Step 35000: loss = 0.53 (0.013 sec)\n",
      "Step 35100: loss = 0.51 (0.010 sec)\n",
      "Step 35200: loss = 0.55 (0.012 sec)\n",
      "Step 35300: loss = 0.59 (0.010 sec)\n",
      "Step 35400: loss = 0.67 (0.013 sec)\n",
      "Step 35500: loss = 0.50 (0.012 sec)\n",
      "Step 35600: loss = 0.64 (0.010 sec)\n",
      "Step 35700: loss = 0.60 (0.010 sec)\n",
      "Step 35800: loss = 0.58 (0.010 sec)\n",
      "Step 35900: loss = 0.66 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 222  Precision @ 1: 0.8672\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8536  Precision @ 1: 0.8550\n",
      "Step 36000: loss = 0.53 (0.017 sec)\n",
      "Step 36100: loss = 0.52 (0.010 sec)\n",
      "Step 36200: loss = 0.50 (0.010 sec)\n",
      "Step 36300: loss = 0.55 (0.010 sec)\n",
      "Step 36400: loss = 0.45 (0.010 sec)\n",
      "Step 36500: loss = 0.43 (0.011 sec)\n",
      "Step 36600: loss = 0.51 (0.010 sec)\n",
      "Step 36700: loss = 0.66 (0.015 sec)\n",
      "Step 36800: loss = 0.44 (0.022 sec)\n",
      "Step 36900: loss = 0.56 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 223  Precision @ 1: 0.8711\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8538  Precision @ 1: 0.8552\n",
      "Step 37000: loss = 0.47 (0.014 sec)\n",
      "Step 37100: loss = 0.66 (0.010 sec)\n",
      "Step 37200: loss = 0.54 (0.010 sec)\n",
      "Step 37300: loss = 0.61 (0.016 sec)\n",
      "Step 37400: loss = 0.44 (0.010 sec)\n",
      "Step 37500: loss = 0.62 (0.013 sec)\n",
      "Step 37600: loss = 0.60 (0.020 sec)\n",
      "Step 37700: loss = 0.51 (0.010 sec)\n",
      "Step 37800: loss = 0.61 (0.010 sec)\n",
      "Step 37900: loss = 0.59 (0.011 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 220  Precision @ 1: 0.8594\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8544  Precision @ 1: 0.8558\n",
      "Step 38000: loss = 0.53 (0.012 sec)\n",
      "Step 38100: loss = 0.50 (0.010 sec)\n",
      "Step 38200: loss = 0.44 (0.010 sec)\n",
      "Step 38300: loss = 0.45 (0.016 sec)\n",
      "Step 38400: loss = 0.46 (0.013 sec)\n",
      "Step 38500: loss = 0.57 (0.010 sec)\n",
      "Step 38600: loss = 0.59 (0.012 sec)\n",
      "Step 38700: loss = 0.67 (0.012 sec)\n",
      "Step 38800: loss = 0.66 (0.011 sec)\n",
      "Step 38900: loss = 0.61 (0.011 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 225  Precision @ 1: 0.8789\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8537  Precision @ 1: 0.8551\n",
      "Step 39000: loss = 0.50 (0.013 sec)\n",
      "Step 39100: loss = 0.50 (0.010 sec)\n",
      "Step 39200: loss = 0.64 (0.010 sec)\n",
      "Step 39300: loss = 0.48 (0.011 sec)\n",
      "Step 39400: loss = 0.65 (0.020 sec)\n",
      "Step 39500: loss = 0.58 (0.010 sec)\n",
      "Step 39600: loss = 0.53 (0.016 sec)\n",
      "Step 39700: loss = 0.53 (0.011 sec)\n",
      "Step 39800: loss = 0.63 (0.010 sec)\n",
      "Step 39900: loss = 0.51 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 219  Precision @ 1: 0.8555\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8543  Precision @ 1: 0.8557\n",
      "Step 40000: loss = 0.55 (0.019 sec)\n",
      "Step 40100: loss = 0.67 (0.013 sec)\n",
      "Step 40200: loss = 0.55 (0.015 sec)\n",
      "Step 40300: loss = 0.65 (0.014 sec)\n",
      "Step 40400: loss = 0.48 (0.010 sec)\n",
      "Step 40500: loss = 0.46 (0.010 sec)\n",
      "Step 40600: loss = 0.55 (0.010 sec)\n",
      "Step 40700: loss = 0.60 (0.010 sec)\n",
      "Step 40800: loss = 0.62 (0.011 sec)\n",
      "Step 40900: loss = 0.48 (0.011 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 223  Precision @ 1: 0.8711\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8542  Precision @ 1: 0.8556\n",
      "Step 41000: loss = 0.54 (0.014 sec)\n",
      "Step 41100: loss = 0.54 (0.012 sec)\n",
      "Step 41200: loss = 0.48 (0.010 sec)\n",
      "Step 41300: loss = 0.50 (0.014 sec)\n",
      "Step 41400: loss = 0.50 (0.010 sec)\n",
      "Step 41500: loss = 0.50 (0.018 sec)\n",
      "Step 41600: loss = 0.46 (0.012 sec)\n",
      "Step 41700: loss = 0.57 (0.010 sec)\n",
      "Step 41800: loss = 0.49 (0.011 sec)\n",
      "Step 41900: loss = 0.49 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 222  Precision @ 1: 0.8672\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8541  Precision @ 1: 0.8555\n",
      "Step 42000: loss = 0.52 (0.013 sec)\n",
      "Step 42100: loss = 0.55 (0.011 sec)\n",
      "Step 42200: loss = 0.44 (0.013 sec)\n",
      "Step 42300: loss = 0.55 (0.010 sec)\n",
      "Step 42400: loss = 0.53 (0.013 sec)\n",
      "Step 42500: loss = 0.52 (0.010 sec)\n",
      "Step 42600: loss = 0.50 (0.010 sec)\n",
      "Step 42700: loss = 0.51 (0.010 sec)\n",
      "Step 42800: loss = 0.60 (0.010 sec)\n",
      "Step 42900: loss = 0.49 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 214  Precision @ 1: 0.8359\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8545  Precision @ 1: 0.8559\n",
      "Step 43000: loss = 0.71 (0.015 sec)\n",
      "Step 43100: loss = 0.47 (0.010 sec)\n",
      "Step 43200: loss = 0.47 (0.010 sec)\n",
      "Step 43300: loss = 0.63 (0.021 sec)\n",
      "Step 43400: loss = 0.59 (0.010 sec)\n",
      "Step 43500: loss = 0.61 (0.010 sec)\n",
      "Step 43600: loss = 0.52 (0.010 sec)\n",
      "Step 43700: loss = 0.50 (0.010 sec)\n",
      "Step 43800: loss = 0.51 (0.010 sec)\n",
      "Step 43900: loss = 0.57 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 218  Precision @ 1: 0.8516\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8546  Precision @ 1: 0.8560\n",
      "Step 44000: loss = 0.61 (0.013 sec)\n",
      "Step 44100: loss = 0.66 (0.010 sec)\n",
      "Step 44200: loss = 0.48 (0.010 sec)\n",
      "Step 44300: loss = 0.55 (0.010 sec)\n",
      "Step 44400: loss = 0.44 (0.010 sec)\n",
      "Step 44500: loss = 0.67 (0.010 sec)\n",
      "Step 44600: loss = 0.53 (0.010 sec)\n",
      "Step 44700: loss = 0.56 (0.010 sec)\n",
      "Step 44800: loss = 0.70 (0.010 sec)\n",
      "Step 44900: loss = 0.53 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 222  Precision @ 1: 0.8672\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8541  Precision @ 1: 0.8555\n",
      "Step 45000: loss = 0.55 (0.012 sec)\n",
      "Step 45100: loss = 0.48 (0.011 sec)\n",
      "Step 45200: loss = 0.46 (0.010 sec)\n",
      "Step 45300: loss = 0.49 (0.010 sec)\n",
      "Step 45400: loss = 0.56 (0.010 sec)\n",
      "Step 45500: loss = 0.43 (0.010 sec)\n",
      "Step 45600: loss = 0.59 (0.011 sec)\n",
      "Step 45700: loss = 0.52 (0.011 sec)\n",
      "Step 45800: loss = 0.52 (0.010 sec)\n",
      "Step 45900: loss = 0.46 (0.011 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 217  Precision @ 1: 0.8477\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8547  Precision @ 1: 0.8561\n",
      "Step 46000: loss = 0.56 (0.012 sec)\n",
      "Step 46100: loss = 0.65 (0.010 sec)\n",
      "Step 46200: loss = 0.58 (0.010 sec)\n",
      "Step 46300: loss = 0.53 (0.010 sec)\n",
      "Step 46400: loss = 0.50 (0.029 sec)\n",
      "Step 46500: loss = 0.54 (0.010 sec)\n",
      "Step 46600: loss = 0.52 (0.010 sec)\n",
      "Step 46700: loss = 0.58 (0.010 sec)\n",
      "Step 46800: loss = 0.60 (0.010 sec)\n",
      "Step 46900: loss = 0.61 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 229  Precision @ 1: 0.8945\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8546  Precision @ 1: 0.8560\n",
      "Step 47000: loss = 0.39 (0.022 sec)\n",
      "Step 47100: loss = 0.56 (0.012 sec)\n",
      "Step 47200: loss = 0.49 (0.014 sec)\n",
      "Step 47300: loss = 0.62 (0.010 sec)\n",
      "Step 47400: loss = 0.45 (0.010 sec)\n",
      "Step 47500: loss = 0.56 (0.012 sec)\n",
      "Step 47600: loss = 0.54 (0.010 sec)\n",
      "Step 47700: loss = 0.48 (0.010 sec)\n",
      "Step 47800: loss = 0.56 (0.010 sec)\n",
      "Step 47900: loss = 0.57 (0.012 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 216  Precision @ 1: 0.8438\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8546  Precision @ 1: 0.8560\n",
      "Step 48000: loss = 0.70 (0.013 sec)\n",
      "Step 48100: loss = 0.57 (0.010 sec)\n",
      "Step 48200: loss = 0.56 (0.012 sec)\n",
      "Step 48300: loss = 0.53 (0.017 sec)\n",
      "Step 48400: loss = 0.57 (0.013 sec)\n",
      "Step 48500: loss = 0.65 (0.010 sec)\n",
      "Step 48600: loss = 0.50 (0.010 sec)\n",
      "Step 48700: loss = 0.53 (0.012 sec)\n",
      "Step 48800: loss = 0.51 (0.010 sec)\n",
      "Step 48900: loss = 0.48 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 225  Precision @ 1: 0.8789\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8549  Precision @ 1: 0.8563\n",
      "Step 49000: loss = 0.44 (0.013 sec)\n",
      "Step 49100: loss = 0.49 (0.010 sec)\n",
      "Step 49200: loss = 0.56 (0.010 sec)\n",
      "Step 49300: loss = 0.60 (0.010 sec)\n",
      "Step 49400: loss = 0.39 (0.010 sec)\n",
      "Step 49500: loss = 0.58 (0.010 sec)\n",
      "Step 49600: loss = 0.55 (0.012 sec)\n",
      "Step 49700: loss = 0.65 (0.019 sec)\n",
      "Step 49800: loss = 0.58 (0.030 sec)\n",
      "Step 49900: loss = 0.56 (0.023 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 230  Precision @ 1: 0.8984\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8546  Precision @ 1: 0.8560\n",
      "Step 50000: loss = 0.42 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 256  Num correct: 222  Precision @ 1: 0.8672\n",
      "Validation Data Eval:\n",
      "  Num examples: 9984  Num correct: 8547  Precision @ 1: 0.8561\n",
      "Test Data Eval:\n",
      "  Num examples: 18688  Num correct: 17206  Precision @ 1: 0.9207\n"
     ]
    }
   ],
   "source": [
    "class inference_hidden0(BaseTensorFlow):\n",
    "    def __init__(self):\n",
    "        BaseTensorFlow.__init__(self)\n",
    "        self.SEED = 66478 \n",
    "\n",
    "    def model(self, images, input_size, output_size, isEval=None):\n",
    "\n",
    "        with tf.variable_scope('softmax_linear', reuse=isEval):\n",
    "            weights = tf.get_variable(\"weights\", [input_size, output_size],\n",
    "                initializer=tf.random_normal_initializer(0.0, 1.0 / math.sqrt(float(input_size)),\n",
    "                          seed=self.SEED))\n",
    "        \n",
    "            biases = tf.get_variable(\"biases\", [output_size],\n",
    "                initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "            logits = tf.matmul(images, weights) + biases\n",
    "            reg_linear = tf.nn.l2_loss(weights) \n",
    "    \n",
    "            if isEval:  \n",
    "                return logits\n",
    "            else:\n",
    "                regularizers = reg_linear\n",
    "                return (logits, regularizers)\n",
    "        \n",
    "class inference_hidden1(BaseTensorFlow):\n",
    "    def __init__(self):\n",
    "        BaseTensorFlow.__init__(self)\n",
    "        self.hidden1_units = 256\n",
    "        self.SEED = 66478 \n",
    "\n",
    "    def model(self, images, input_size, output_size, isEval=None):\n",
    "        with tf.variable_scope('hidden1', reuse=isEval):\n",
    "            weights = tf.get_variable(\"weights\", [input_size, self.hidden1_units],\n",
    "                initializer=tf.random_normal_initializer(0.0, 1.0 / math.sqrt(float(input_size)),\n",
    "                          seed=self.SEED))    \n",
    "            biases = tf.get_variable(\"biases\", [self.hidden1_units],\n",
    "                initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "        reg_hidden1 = tf.nn.l2_loss(weights) \n",
    "        hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "   \n",
    "        with tf.variable_scope('softmax_linear', reuse=isEval):\n",
    "            weights = tf.get_variable(\"weights\", [self.hidden1_units, output_size],\n",
    "                initializer=tf.random_normal_initializer(0.0, 1.0 / math.sqrt(float(self.hidden1_units)),\n",
    "                          seed=self.SEED))\n",
    "        \n",
    "            biases = tf.get_variable(\"biases\", [output_size],\n",
    "                initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "            logits = tf.matmul(hidden1, weights) + biases\n",
    "            reg_linear = tf.nn.l2_loss(weights) \n",
    "        \n",
    "            if isEval:  \n",
    "                return logits\n",
    "            else:\n",
    "                regularizers = (reg_hidden1 + reg_linear)\n",
    "                return (logits, regularizers)\n",
    "        \n",
    "#if __name__ == '__main__':\n",
    "model0 = inference_hidden1()\n",
    "model0.loadData()\n",
    "model0.process()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset1 = train_dataset[:5*batch_size, :]\n",
    "train_labels1 = train_labels[:5*batch_size]\n",
    "\n",
    "print 'Training set', train_dataset1.shape, train_labels1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process(train_dataset1, train_labels1, inference_hidden1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden1_units = 256\n",
    "def inference_hidden2(images, isEval=None):\n",
    "  # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [IMAGE_PIXELS, hidden1_units],\n",
    "            initializer=tf.random_normal_initializer(0.0, 1.0 / math.sqrt(float(IMAGE_PIXELS)),\n",
    "                          seed=SEED))    \n",
    "        biases = tf.get_variable(\"biases\", [hidden1_units],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "        reg_hidden1 = tf.nn.l2_loss(weights) \n",
    "        hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "        if not isEval:\n",
    "            print 'drop out added'\n",
    "            hidden1 = tf.nn.dropout(hidden1, 0.5, seed=SEED)\n",
    "  # Linear\n",
    "    with tf.variable_scope('softmax_linear', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [hidden1_units, NUM_CLASSES],\n",
    "            initializer=tf.random_normal_initializer(0.0, 1.0 / math.sqrt(float(hidden1_units)),\n",
    "                          seed=SEED))\n",
    "        \n",
    "        biases = tf.get_variable(\"biases\", [NUM_CLASSES],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "        logits = tf.matmul(hidden1, weights) + biases\n",
    "        reg_linear = tf.nn.l2_loss(weights) \n",
    "    \n",
    "        if isEval:  \n",
    "            return logits\n",
    "        else:\n",
    "            regularizers = (reg_hidden1 + reg_linear)\n",
    "            return (logits, regularizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process(train_dataset1, train_labels1, inference_hidden2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hi, myself and Zhen Zhou from the LISA lab at Université de Montréal trained a couple of 4 \n",
    "#layer MLPs with 1024-300-50 hidden neurons respectively. We divided the noisy set into 5/6 \n",
    "#train 1/6 valid and kept the clean set for testing. We 97.1% accuracy on the test set at 412 \n",
    "#epoch with early stopping, linear decay of the learning rate, a hard constraint on the norm \n",
    "#of the weights and tanh activation units. We get approximately 93 on valid and 98 on train. \n",
    "#The train set is easy to overfit (you can get 100% accuracy on train if you continue training). \n",
    "#One could probably do better if they pursue hyper-optimization further. We used Torch 7.\n",
    "\n",
    "hidden1_units = 256\n",
    "hidden2_units = 128\n",
    "def inference_hidden3(images, isEval=None):\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [IMAGE_PIXELS, hidden1_units],\n",
    "            initializer=tf.random_normal_initializer(0.0, 1.0 / math.sqrt(float(IMAGE_PIXELS)),\n",
    "                          seed=SEED))    \n",
    "        biases = tf.get_variable(\"biases\", [hidden1_units],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "        reg_hidden1 = tf.nn.l2_loss(weights) \n",
    "        hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "        if not isEval:\n",
    "            hidden1 = tf.nn.dropout(hidden1, 0.5, seed=SEED)\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [hidden1_units, hidden2_units],\n",
    "            initializer=tf.random_normal_initializer(0.0, 1.0 / math.sqrt(float(hidden1_units)),\n",
    "                          seed=SEED))    \n",
    "        biases = tf.get_variable(\"biases\", [hidden2_units],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "        reg_hidden2 = tf.nn.l2_loss(weights) \n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "        if not isEval:\n",
    "            hidden2 = tf.nn.dropout(hidden2, 0.5, seed=SEED)\n",
    "    # Linear\n",
    "    with tf.variable_scope('softmax_linear', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [hidden2_units, NUM_CLASSES],\n",
    "            initializer=tf.random_normal_initializer(0.0, 1.0 / math.sqrt(float(hidden2_units)),\n",
    "                          seed=SEED))\n",
    "        biases = tf.get_variable(\"biases\", [NUM_CLASSES],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "        reg_linear = tf.nn.l2_loss(weights) \n",
    "    \n",
    "    if isEval:  \n",
    "        return logits\n",
    "    else:\n",
    "        regularizers = (reg_hidden1 +reg_hidden2+ reg_linear)\n",
    "        return (logits, regularizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process(train_dataset, train_labels, inference_hidden3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
