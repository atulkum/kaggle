{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (190000, 784) (190000,)\n",
      "Validation set (10000, 784) (10000,)\n",
      "Test set (18724, 784) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "IMAGE_PIXELS = 784\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "    \n",
    "  train_dataset = train_dataset.reshape((-1, IMAGE_PIXELS)).astype(np.float32)\n",
    "  valid_dataset = valid_dataset.reshape((-1, IMAGE_PIXELS)).astype(np.float32)\n",
    "  test_dataset = test_dataset.reshape((-1, IMAGE_PIXELS)).astype(np.float32)\n",
    "\n",
    "  print 'Training set', train_dataset.shape, train_labels.shape\n",
    "  print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "  print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseTensorFlow:\n",
    "    def __init__(self, batch_size=16, starting_learning_rate=0.01,  learning_rate_decay=0.99, num_steps=1001):\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = 28\n",
    "        self.NUM_CLASSES = 10\n",
    "        self.num_channels = 1 \n",
    "        self.starting_learning_rate = starting_learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.train_dir = './output'\n",
    "        self.num_steps = num_steps\n",
    "        self.IMAGE_PIXELS = 784\n",
    "        \n",
    "    def model(self, images, image_size, num_channels, output_size, isEval=None):\n",
    "        raise Exception('Error', 'Not implemented')\n",
    "    \n",
    "    def loadData(self):\n",
    "        pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "          save = pickle.load(f)\n",
    "          self.train_dataset = save['train_dataset']\n",
    "          self.train_labels = save['train_labels']\n",
    "          self.valid_dataset = save['valid_dataset']\n",
    "          self.valid_labels = save['valid_labels']\n",
    "          self.test_dataset = save['test_dataset']\n",
    "          self.test_labels = save['test_labels']\n",
    "          del save  # hint to help gc free up memory\n",
    "    \n",
    "          self.train_dataset = self.train_dataset.reshape((-1, self.IMAGE_PIXELS)).astype(np.float32)\n",
    "          self.valid_dataset = self.valid_dataset.reshape((-1, self.IMAGE_PIXELS)).astype(np.float32)\n",
    "          self.test_dataset = self.test_dataset.reshape((-1, self.IMAGE_PIXELS)).astype(np.float32)\n",
    "\n",
    "          print 'Training set', self.train_dataset.shape, self.train_labels.shape\n",
    "          print 'Validation set', self.valid_dataset.shape, self.valid_labels.shape\n",
    "          print 'Test set', self.test_dataset.shape, self.test_labels.shape\n",
    "\n",
    "    def loss_function(self,logits, labels):        \n",
    "        labels = tf.expand_dims(labels, 1)\n",
    "        indices = tf.expand_dims(tf.range(0, self.batch_size), 1)\n",
    "        concated = tf.concat(1, [indices, labels])\n",
    "        onehot_labels = tf.sparse_to_dense(\n",
    "              concated, tf.pack([self.batch_size, self.NUM_CLASSES]), 1.0, 0.0)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,\n",
    "                                                          onehot_labels,\n",
    "                                                          name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        return loss\n",
    "\n",
    "    def training(self,loss, train_size):\n",
    "        tf.scalar_summary(loss.op.name, loss)\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            self.starting_learning_rate,      \n",
    "            global_step * self.batch_size,  \n",
    "            train_size,          \n",
    "            self.learning_rate_decay,                \n",
    "            staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op\n",
    "\n",
    "    def evaluation(self,logits, labels):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "\n",
    "    def preapare_placeholder_inputs(self):\n",
    "        self.images_placeholder = tf.placeholder(tf.float32, shape=(self.batch_size, self.image_size, self.image_size, self.num_channels))\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, shape=(self.batch_size))\n",
    " \n",
    "    def fill_feed_dict(self, dataset, labels, step):\n",
    "        if labels.shape[0] - self.batch_size > 0:\n",
    "            offset = (step * self.batch_size) % (labels.shape[0] - self.batch_size)\n",
    "        else:\n",
    "            offset = 0\n",
    "        images_feed = dataset[offset:(offset + self.batch_size), :]\n",
    "        images_feed = images_feed.reshape((self.batch_size, self.image_size, self.image_size, self.num_channels)).astype(np.float32)\n",
    "\n",
    "        labels_feed = labels[offset:(offset + self.batch_size)]\n",
    "        feed_dict = {\n",
    "            self.images_placeholder: images_feed,\n",
    "            self.labels_placeholder: labels_feed,\n",
    "        }\n",
    "        return feed_dict\n",
    "        \n",
    "    def do_eval(self,sess, eval_correct, dataset, labels):\n",
    "        true_count = 0  \n",
    "        steps_per_epoch = labels.shape[0] // self.batch_size\n",
    "        num_examples = steps_per_epoch * self.batch_size\n",
    "        for step in xrange(steps_per_epoch):\n",
    "            feed_dict = self.fill_feed_dict(dataset, labels, step)\n",
    "            true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "        \n",
    "        precision = 1.0*true_count / num_examples\n",
    "        print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "            (num_examples, true_count, precision))\n",
    "    \n",
    "     \n",
    "    def run_training(self,sess, eval_correct, train_op, loss):\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        summary_writer = tf.train.SummaryWriter(self.train_dir, graph_def=sess.graph_def)\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "        feed_dict = self.fill_feed_dict(self.train_dataset, self.train_labels, 0)\n",
    "    \n",
    "        for step in xrange(self.num_steps):\n",
    "            start_time = time.time()\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "            \n",
    "            feed_dict = self.fill_feed_dict(self.train_dataset, self.train_labels, step+1)\n",
    "           \n",
    "            duration = time.time() - start_time\n",
    "            if step % 50 == 0:\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                \n",
    "            if (step + 1) % 1000 == 0 or (step + 1) == self.num_steps:\n",
    "                saver.save(sess, self.train_dir, global_step=step)\n",
    "                print('Training Data Eval:')\n",
    "                self.do_eval(sess, eval_correct,\n",
    "                    feed_dict[self.images_placeholder], feed_dict[self.labels_placeholder])\n",
    "                print('Validation Data Eval:')\n",
    "                self.do_eval(sess, eval_correct, self.valid_dataset, self.valid_labels)\n",
    "           \n",
    "    def process(self):\n",
    "        with tf.Graph().as_default():\n",
    "            self.preapare_placeholder_inputs()\n",
    "        \n",
    "            logits_train, regularizer = self.model(self.images_placeholder, self.NUM_CLASSES)\n",
    "            loss = self.loss_function(logits_train, self.labels_placeholder)\n",
    "            if regularizer:\n",
    "                loss += 5e-4 * regularizer\n",
    "                \n",
    "            train_op = self.training(loss,self.train_dataset.shape[0])\n",
    "        \n",
    "            logits_eval = self.model(self.images_placeholder, self.NUM_CLASSES, isEval=True)\n",
    "            eval_correct = self.evaluation(logits_eval, self.labels_placeholder)\n",
    "    \n",
    "            with tf.Session() as sess:\n",
    "                init = tf.initialize_all_variables()\n",
    "                sess.run(init)\n",
    "            \n",
    "                self.run_training(sess, eval_correct, train_op, loss)\n",
    "                print('Test Data Eval:')\n",
    "                self.do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    self.test_dataset, self.test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class inference2(BaseTensorFlow):\n",
    "    def __init__(self, batch_size=16, starting_learning_rate=0.01,  learning_rate_decay=0.99, num_steps=1001):\n",
    "        BaseTensorFlow.__init__(self, batch_size, starting_learning_rate,  learning_rate_decay, num_steps)\n",
    "        self.SEED = 66478\n",
    "        self.patch_size = 5\n",
    "        self.depth = 16\n",
    "        self.num_hidden = 64\n",
    "        \n",
    "    def model(self, images, output_size, isEval=None):\n",
    "        images_shape = images.get_shape().as_list()\n",
    "        image_size = images_shape[1]\n",
    "        num_channels = images_shape[3]\n",
    "        \n",
    "        regularizers =None\n",
    "        with tf.variable_scope('layer1', reuse=isEval):\n",
    "            weights = tf.get_variable(\"weights\", [self.patch_size, self.patch_size, num_channels, self.depth],\n",
    "                initializer=tf.random_normal_initializer(0.0, 0.1, seed= self.SEED))    \n",
    "            biases = tf.get_variable(\"biases\", [self.depth], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "            conv = tf.nn.conv2d(images, weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "            pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "        \n",
    "        with tf.variable_scope('layer2', reuse=isEval):\n",
    "            weights = tf.get_variable(\"weights\", [self.patch_size, self.patch_size, self.depth, self.depth],\n",
    "                initializer=tf.random_normal_initializer(0.0, 0.1, seed= self.SEED))    \n",
    "            biases = tf.get_variable(\"biases\", [self.depth], initializer=tf.constant_initializer(1.0))\n",
    "\n",
    "            conv = tf.nn.conv2d(pool, weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(tf.nn.bias_add(conv , biases))\n",
    "            pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')    \n",
    "                  \n",
    "        with tf.variable_scope('layer3', reuse=isEval):\n",
    "            weights = tf.get_variable(\"weights\", [image_size / 4 * image_size / 4 * self.depth, self.num_hidden],\n",
    "                initializer=tf.random_normal_initializer(0.0, 0.1, seed= self.SEED))\n",
    "            biases = tf.get_variable(\"biases\", [self.num_hidden], initializer=tf.constant_initializer(1.0))\n",
    "\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            relu = tf.nn.relu(tf.matmul(reshape, weights) + biases)\n",
    "            if not isEval:\n",
    "                relu = tf.nn.dropout(relu, 0.5, seed=self.SEED)\n",
    "            regularizers = tf.nn.l2_loss(weights) \n",
    "            \n",
    "        with tf.variable_scope('layer4', reuse=isEval):\n",
    "            weights = tf.get_variable(\"weights\", [self.num_hidden, output_size],\n",
    "                initializer=tf.random_normal_initializer(0.0, 0.1, seed= self.SEED))\n",
    "            biases = tf.get_variable(\"biases\", [output_size], initializer=tf.constant_initializer(1.0))\n",
    "            \n",
    "            logits = tf.matmul(relu, weights) + biases\n",
    "            regularizers += tf.nn.l2_loss(weights) \n",
    "\n",
    "        if isEval:  \n",
    "            return logits\n",
    "        else:\n",
    "            return (logits, regularizers)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pylab.gray();\n",
    "\n",
    "pylab.subplot(1, 2, 1); pylab.axis('off'); pylab.imshow(np.hstack(list(train_dataset[0:4]) + [blank]))\n",
    "pylab.subplot(1, 2, 2); pylab.axis('off'); pylab.imshow(np.hstack( [blank] + list(train_dataset[1:4]) + [blank]))\n",
    "#pylab.subplot(1, 4, 3); pylab.axis('off'); pylab.imshow(np.hstack(train_dataset[10:15]))\n",
    "#pylab.subplot(1, 4, 4); pylab.axis('off'); pylab.imshow(np.hstack(train_dataset[15:20]))\n",
    "pylab.show()\n",
    "\n",
    "blank = np.random.rand(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 3.91 (0.084 sec)\n",
      "Step 50: loss = 0.72 (0.009 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 8  Precision @ 1: 0.5000\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 6533  Precision @ 1: 0.6533\n",
      "Step 100: loss = 1.63 (0.011 sec)\n",
      "Step 150: loss = 0.83 (0.009 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 12  Precision @ 1: 0.7500\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 7702  Precision @ 1: 0.7702\n",
      "Step 200: loss = 0.50 (0.016 sec)\n",
      "Step 250: loss = 0.56 (0.008 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 10  Precision @ 1: 0.6250\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 7727  Precision @ 1: 0.7727\n",
      "Step 300: loss = 1.32 (0.019 sec)\n",
      "Step 350: loss = 0.83 (0.009 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 16  Precision @ 1: 1.0000\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 7920  Precision @ 1: 0.7920\n",
      "Step 400: loss = 0.12 (0.010 sec)\n",
      "Step 450: loss = 0.67 (0.008 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 13  Precision @ 1: 0.8125\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8026  Precision @ 1: 0.8026\n",
      "Step 500: loss = 0.59 (0.020 sec)\n",
      "Step 550: loss = 0.89 (0.009 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 10  Precision @ 1: 0.6250\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8128  Precision @ 1: 0.8128\n",
      "Step 600: loss = 0.97 (0.027 sec)\n",
      "Step 650: loss = 0.95 (0.038 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 14  Precision @ 1: 0.8750\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8130  Precision @ 1: 0.8130\n",
      "Step 700: loss = 0.58 (0.009 sec)\n",
      "Step 750: loss = 1.12 (0.008 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 11  Precision @ 1: 0.6875\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8227  Precision @ 1: 0.8227\n",
      "Step 800: loss = 0.98 (0.010 sec)\n",
      "Step 850: loss = 1.10 (0.008 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 15  Precision @ 1: 0.9375\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8210  Precision @ 1: 0.8210\n",
      "Step 900: loss = 0.34 (0.012 sec)\n",
      "Step 950: loss = 0.54 (0.008 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 12  Precision @ 1: 0.7500\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8300  Precision @ 1: 0.8300\n",
      "Step 1000: loss = 0.79 (0.010 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 15  Precision @ 1: 0.9375\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8241  Precision @ 1: 0.8241\n",
      "Test Data Eval:\n",
      "  Num examples: 18720  Num correct: 16756  Precision @ 1: 0.8951\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == '__main__':\n",
    "model1 = inference2(batch_size=16,\n",
    "                    starting_learning_rate=0.005,  \n",
    "                    learning_rate_decay=0.99, \n",
    "                    num_steps=10001)\n",
    "model1.loadData()\n",
    "model1.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_hidden5(images, isEval=None):\n",
    "    with tf.variable_scope('layer1', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [patch_size, patch_size, num_channels, depth],\n",
    "            initializer=tf.random_normal_initializer(0.0, 0.1, seed=SEED))    \n",
    "        biases = tf.get_variable(\"biases\", [depth], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        conv = tf.nn.conv2d(images, weights, [1, 1, 1, 1], padding='SAME')    \n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "        \n",
    "    with tf.variable_scope('layer2', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [patch_size, patch_size, depth, depth],\n",
    "            initializer=tf.random_normal_initializer(0.0, 0.1, seed=SEED))    \n",
    "        biases = tf.get_variable(\"biases\", [depth], initializer=tf.constant_initializer(1.0))\n",
    "        \n",
    "        conv = tf.nn.conv2d(pool, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv , biases))\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "         \n",
    "    with tf.variable_scope('layer3', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [image_size / 4 * image_size / 4 * depth, num_hidden],\n",
    "            initializer=tf.random_normal_initializer(0.0, 0.1, seed=SEED))\n",
    "        biases = tf.get_variable(\"biases\", [num_hidden], initializer=tf.constant_initializer(1.0))\n",
    "    \n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        relu = tf.nn.relu(tf.matmul(reshape, weights) + biases)\n",
    "        \n",
    "    with tf.variable_scope('layer4', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [num_hidden, NUM_CLASSES],\n",
    "            initializer=tf.random_normal_initializer(0.0, 0.1, seed=SEED))\n",
    "        biases = tf.get_variable(\"biases\", [NUM_CLASSES], initializer=tf.constant_initializer(1.0))\n",
    "        \n",
    "        logits = tf.matmul(relu, weights) + biases\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 4.14 (0.074 sec)\n",
      "Step 50: loss = 0.89 (0.023 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 8  Precision @ 1: 0.5000\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 6451  Precision @ 1: 0.6451\n",
      "Step 100: loss = 1.52 (0.039 sec)\n",
      "Step 150: loss = 0.87 (0.033 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 10  Precision @ 1: 0.6250\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 7672  Precision @ 1: 0.7672\n",
      "Step 200: loss = 0.59 (0.042 sec)\n",
      "Step 250: loss = 0.59 (0.024 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 9  Precision @ 1: 0.5625\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 7793  Precision @ 1: 0.7793\n",
      "Step 300: loss = 1.13 (0.029 sec)\n",
      "Step 350: loss = 0.75 (0.022 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 14  Precision @ 1: 0.8750\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8122  Precision @ 1: 0.8122\n",
      "Step 400: loss = 0.31 (0.027 sec)\n",
      "Step 450: loss = 0.51 (0.023 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 12  Precision @ 1: 0.7500\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8172  Precision @ 1: 0.8172\n",
      "Step 500: loss = 0.76 (0.027 sec)\n",
      "Step 550: loss = 0.72 (0.024 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 11  Precision @ 1: 0.6875\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8288  Precision @ 1: 0.8288\n",
      "Step 600: loss = 0.91 (0.043 sec)\n",
      "Step 650: loss = 1.06 (0.026 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 14  Precision @ 1: 0.8750\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8327  Precision @ 1: 0.8327\n",
      "Step 700: loss = 0.37 (0.033 sec)\n",
      "Step 750: loss = 1.17 (0.023 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 12  Precision @ 1: 0.7500\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8351  Precision @ 1: 0.8351\n",
      "Step 800: loss = 0.67 (0.036 sec)\n",
      "Step 850: loss = 1.10 (0.092 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 15  Precision @ 1: 0.9375\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8392  Precision @ 1: 0.8392\n",
      "Step 900: loss = 0.35 (0.030 sec)\n",
      "Step 950: loss = 0.55 (0.027 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 12  Precision @ 1: 0.7500\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8442  Precision @ 1: 0.8442\n",
      "Step 1000: loss = 0.91 (0.040 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 16  Num correct: 14  Precision @ 1: 0.8750\n",
      "Validation Data Eval:\n",
      "  Num examples: 10000  Num correct: 8388  Precision @ 1: 0.8388\n",
      "Test Data Eval:\n",
      "  Num examples: 18720  Num correct: 16915  Precision @ 1: 0.9036\n"
     ]
    }
   ],
   "source": [
    "process(train_dataset, train_labels, inference_hidden5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "def inference_hidden6(images, isEval=None):\n",
    "    with tf.variable_scope('layer1', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [patch_size, patch_size, num_channels, depth],\n",
    "            initializer=tf.random_normal_initializer(0.0, 0.1, seed=SEED))    \n",
    "        biases = tf.get_variable(\"biases\", [depth], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        conv = tf.nn.conv2d(images, weights, [1, 1, 1, 1], padding='SAME')    \n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "        \n",
    "    with tf.variable_scope('layer2', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [patch_size, patch_size, depth, depth/2],\n",
    "            initializer=tf.random_normal_initializer(0.0, 1.0/math.sqrt(patch_size*patch_size), seed=SEED))    \n",
    "        biases = tf.get_variable(\"biases\", [depth/2], initializer=tf.constant_initializer(1.0))\n",
    "        \n",
    "        conv = tf.nn.conv2d(pool, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv , biases))\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "         \n",
    "    with tf.variable_scope('layer3', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [image_size / 4 * image_size / 4 * depth, num_hidden],\n",
    "            initializer=tf.random_normal_initializer(0.0, 1.0/math.sqrt(image_size / 4 * image_size / 4 * depth), seed=SEED))\n",
    "        biases = tf.get_variable(\"biases\", [num_hidden], initializer=tf.constant_initializer(1.0))\n",
    "    \n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        relu = tf.nn.relu(tf.matmul(reshape, weights) + biases)\n",
    "        \n",
    "    with tf.variable_scope('layer4', reuse=isEval):\n",
    "        weights = tf.get_variable(\"weights\", [num_hidden, NUM_CLASSES],\n",
    "            initializer=tf.random_normal_initializer(0.0, 0.1, seed=SEED))\n",
    "        biases = tf.get_variable(\"biases\", [NUM_CLASSES], initializer=tf.constant_initializer(1.0))\n",
    "        \n",
    "        logits = tf.matmul(relu, weights) + biases\n",
    "        \n",
    "    return logits"
   ]
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
