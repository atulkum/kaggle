{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "import theano\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import TrainSplit\n",
    "from lasagne.updates import nesterov_momentum, adagrad\n",
    "from lasagne.objectives import categorical_crossentropy, aggregate\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "%run 'XGBoost_class.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(pred, y):\n",
    "    eps = 1e-15\n",
    "    total = 0.\n",
    "    for i in range(len(y)):\n",
    "        p = max(min(pred[i][y[i]], (1 - eps)), eps)\n",
    "        total += math.log(p)\n",
    "    return -(total/len(y))\n",
    "\n",
    "def prepareForCountVector(df, columnName, dictCount=2000, topk_dict=None):\n",
    "    if not topk_dict:\n",
    "        col = df[columnName].dropna()\n",
    "        counts = col.value_counts()\n",
    "        topk_dict = counts.iloc[0:min(dictCount, len(col))].index\n",
    "    \n",
    "        topk_dict = set(topk_dict).union(set(topk_dict))\n",
    "        \n",
    "    col = col.fillna('')\n",
    "    \n",
    "    topk = df[columnName].apply(lambda x: '%s%d'%(columnName, x) if x in topk_dict else '%sother'%(columnName))\n",
    " \n",
    "    topk_se = pd.Series(topk, name=columnName)\n",
    "    df_topk = pd.concat([topk_se, df['VisitNumber']], axis=1)\n",
    "    return topk_dict, df_topk\n",
    "\n",
    "def getCountVector(df, columnName, isWords, vec=None):\n",
    "    if isWords:\n",
    "        df[columnName] = df[columnName].fillna('')\n",
    "    df_topk_gpy = df.groupby('VisitNumber')\n",
    "    df_topk_list = df_topk_gpy.apply(lambda x: list(x[columnName]))\n",
    "    topk_flat = df_topk_list.str.join(' ')\n",
    "    \n",
    "    if not vec: \n",
    "        vec = CountVectorizer() \n",
    "        vec.fit(topk_flat)    \n",
    "    \n",
    "    wc = vec.transform(topk_flat)\n",
    "    wcar = wc.toarray()\n",
    "    \n",
    "    words_count = topk_flat.apply(lambda x : len(x.split(' '))).reshape(-1,1)\n",
    "    ret = None\n",
    "    if isWords:\n",
    "        words_len = topk_flat.apply(lambda x : len(x)).reshape(-1,1)\n",
    "        ret = np.column_stack([wcar, words_count, words_len])\n",
    "    else:\n",
    "        ret = np.column_stack([wcar, words_count])\n",
    "    \n",
    "    return vec, ret\n",
    "\n",
    "def make_submission(clf, X_test, ids, encoder, prefix):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    outCols = ['TripType_' + col for col in encoder.classes_]\n",
    "    \n",
    "    millis = int(round(time.time() * 1000))\n",
    "    filename = '%s_%d'%(prefix, millis)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('VisitNumber,')\n",
    "        f.write(','.join(outCols))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    f.close()\n",
    "    print(\"Wrote submission to file {}.\".format(filename))\n",
    "    \n",
    "def make_submission_ensemble(y_prob, ids, encoder, prefix):\n",
    "    outCols = ['TripType_' + str(col) for col in encoder.classes_]\n",
    "    \n",
    "    millis = int(round(time.time() * 1000))\n",
    "    filename = '%s_%d'%(prefix, millis)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('VisitNumber,')\n",
    "        f.write(','.join(outCols))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([str(id)] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def getY(train_df):\n",
    "    df_y = train_df[['VisitNumber', 'TripType']].groupby('VisitNumber').first()\n",
    "    df_y = df_y.reset_index()\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(df_y.TripType).astype(np.int32)\n",
    "    return encoder, y\n",
    "def preprocessDataTrain(df):\n",
    "    df_w = df[['VisitNumber', 'Weekday']].groupby('VisitNumber').first()\n",
    "    df_w = df_w.reset_index()\n",
    "    dict_df_w = df_w[['Weekday']].T.to_dict().values()\n",
    "    \n",
    "    dictVec = DictVectorizer()\n",
    "    dictVec.fit(dict_df_w)\n",
    "        \n",
    "    week = dictVec.transform(dict_df_w)\n",
    "    \n",
    "    is_wknd = np.array((df_w['Weekday']=='Sunday') | (df_w['Weekday']=='Saturday'))\n",
    "    is_wknd = is_wknd.reshape(-1,1)\n",
    "\n",
    "    df_upc = prepareForCountVector(df, 'Upc')\n",
    "    upc = getCountVector(df_upc[1], 'Upc', False)\n",
    "\n",
    "    df_fln = prepareForCountVector(df, 'FinelineNumber')\n",
    "    fln = getCountVector(df_fln[1], 'FinelineNumber', False)\n",
    "\n",
    "    words = getCountVector(df, 'DepartmentDescription', True)\n",
    "\n",
    "    df_ScanCount = df[['VisitNumber', 'ScanCount']].groupby('VisitNumber').sum()\n",
    "    df_ScanCount = df_ScanCount.reset_index()\n",
    "    scancount = np.array(df_ScanCount.ScanCount)\n",
    "    scancount = scancount.reshape(-1,1)\n",
    "    \n",
    "    feature_matrix = []\n",
    "    feature_matrix.append(week)\n",
    "    feature_matrix.append(is_wknd)\n",
    "    feature_matrix.append(upc[1])\n",
    "    feature_matrix.append(fln[1])\n",
    "    feature_matrix.append(words[1])\n",
    "    feature_matrix.append(scancount)\n",
    "\n",
    "    feature_matrix = sparse.hstack(feature_matrix).tocsr()\n",
    "\n",
    "    ret_params = {\n",
    "        'week_dictVec': dictVec,\n",
    "        'upc_vec':upc[0],\n",
    "        'upc_dict':df_upc[0],\n",
    "        'fln_vec':fln[0],\n",
    "        'fln_dict':df_fln[0],\n",
    "        'words_vec':words[0]\n",
    "    }\n",
    "    return feature_matrix, ret_params\n",
    "\n",
    "def preprocessDataTest(df, params):\n",
    "    df_w = df[['VisitNumber', 'Weekday']].groupby('VisitNumber').first()\n",
    "    df_w = df_w.reset_index()\n",
    "    dict_df_w = df_w[['Weekday']].T.to_dict().values()\n",
    "    dictVec = params['week_dictVec']\n",
    "    week = dictVec.transform(dict_df_w)\n",
    "    \n",
    "    is_wknd = np.array((df_w['Weekday']=='Sunday') | (df_w['Weekday']=='Saturday'))\n",
    "    is_wknd = is_wknd.reshape(-1,1)\n",
    "            \n",
    "    df_upc = prepareForCountVector(df, 'Upc', params['upc_dict'])\n",
    "    upc = getCountVector(df_upc[1], 'Upc', False, params['upc_vec'])\n",
    "            \n",
    "    df_fln = prepareForCountVector(df, 'FinelineNumber', params['fln_dict'])\n",
    "    fln = getCountVector(df_fln[1], 'FinelineNumber', False, params['fln_vec'])\n",
    "\n",
    "    words = getCountVector(df, 'DepartmentDescription', True, params['words_vec'])\n",
    "\n",
    "    df_ScanCount = df[['VisitNumber', 'ScanCount']].groupby('VisitNumber').sum()\n",
    "    df_ScanCount = df_ScanCount.reset_index()\n",
    "    scancount = np.array(df_ScanCount.ScanCount)\n",
    "    scancount = scancount.reshape(-1,1)\n",
    "    \n",
    "    feature_matrix = []\n",
    "    feature_matrix.append(week)\n",
    "    feature_matrix.append(is_wknd)\n",
    "    feature_matrix.append(upc[1])\n",
    "    feature_matrix.append(fln[1])\n",
    "    feature_matrix.append(words[1])\n",
    "    feature_matrix.append(scancount)\n",
    "\n",
    "    feature_matrix = sparse.hstack(feature_matrix).tocsr()\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TripType\tVisitNumber\tWeekday\tUpc\tScanCount\tDepartmentDescription\tFinelineNumber\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_matrix, params = preprocessDataTrain(train_df)\n",
    "encoder, y = getY(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "feature_matrix_test = preprocessDataTest(test_df,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 4127)\n",
      "95674 4127 38\n"
     ]
    }
   ],
   "source": [
    "num_test, num_features_test = feature_matrix_test.shape\n",
    "num_train, num_features = feature_matrix.shape\n",
    "assert(num_features_test == num_features)\n",
    "num_classes = len(encoder.classes_)\n",
    "print feature_matrix_test.shape\n",
    "print num_train, num_features, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "layers = [('input', InputLayer),\n",
    "           ('dropout1', DropoutLayer), \n",
    "           ('hidden1', DenseLayer),\n",
    "           ('dropout2', DropoutLayer), \n",
    "           ('hidden2', DenseLayer), \n",
    "           ('dropout3', DropoutLayer),\n",
    "           ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "nn = NeuralNet(layers=layers,\n",
    "                 objective_loss_function=categorical_crossentropy,\n",
    "                 input_shape=(None, num_features),\n",
    "                 \n",
    "                 dropout1_p=0.15,\n",
    "                 dropout2_p=0.25,\n",
    "                 dropout3_p=0.25,\n",
    "                 \n",
    "                 hidden1_num_units=1000,\n",
    "                 hidden2_num_units=500,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=adagrad,     #nesterov_momentum,\n",
    "                 #update_learning_rate=theano.shared(np.float32(0.01)),   \n",
    "                 update_learning_rate=theano.shared(np.float32(0.01)),\n",
    "                 #update_momentum=0.04,\n",
    "                 \n",
    "                 train_split=TrainSplit(eval_size=0.2),\n",
    "                 verbose=1,\n",
    "                 max_epochs=50)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, max_depth=17) \n",
    "et = ExtraTreesClassifier(n_estimators=300, n_jobs=-1, max_depth=25)\n",
    "params = {   \n",
    "              'objective': 'multi:softprob',\n",
    "              'eval_metric': 'mlogloss',\n",
    "              'num_class': num_classes,\n",
    "              'eta': 0.0825,\n",
    "              'max_depth': 10,\n",
    "              'num_round': 2000,\n",
    "              'subsample':0.85, \n",
    "              'colsample_bytree':0.8, \n",
    "              'min_child_weight':5.2475,\n",
    "              'silent':1\n",
    "    }\n",
    "clfxgb = XGBoostClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, y, test_size=0.5, random_state=56)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 4647538 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input       4127\n",
      "  1  dropout1    4127\n",
      "  2  hidden1     1000\n",
      "  3  dropout2    1000\n",
      "  4  hidden2      500\n",
      "  5  dropout3     500\n",
      "  6  output        38\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.19536\u001b[0m       \u001b[32m1.43264\u001b[0m      1.53238      0.57410  85.04s\n",
      "      2       \u001b[36m1.18087\u001b[0m       \u001b[32m1.32627\u001b[0m      0.89037      0.59953  80.35s\n",
      "      3       \u001b[36m0.91240\u001b[0m       \u001b[32m1.31042\u001b[0m      0.69627      0.60761  76.29s\n",
      "      4       \u001b[36m0.75473\u001b[0m       1.35137      0.55849      0.61214  67.07s\n",
      "      5       \u001b[36m0.65576\u001b[0m       1.36196      0.48148      0.61533  59.01s\n",
      "      6       \u001b[36m0.57524\u001b[0m       1.39900      0.41118      0.61391  58.78s\n",
      "      7       \u001b[36m0.53053\u001b[0m       1.42872      0.37133      0.61667  57.51s\n",
      "      8       \u001b[36m0.49462\u001b[0m       1.43065      0.34573      0.61964  58.35s\n",
      "      9       \u001b[36m0.46090\u001b[0m       1.47750      0.31194      0.61828  57.15s\n",
      "     10       \u001b[36m0.43446\u001b[0m       1.47830      0.29389      0.61627  57.34s\n",
      "     11       \u001b[36m0.41071\u001b[0m       1.53828      0.26700      0.61434  57.31s\n",
      "     12       \u001b[36m0.39374\u001b[0m       1.54815      0.25433      0.61892  57.25s\n",
      "     13       \u001b[36m0.38275\u001b[0m       1.55519      0.24611      0.61585  58.15s\n",
      "     14       \u001b[36m0.37327\u001b[0m       1.55251      0.24043      0.61873  64.09s\n",
      "     15       \u001b[36m0.35824\u001b[0m       1.58453      0.22609      0.61549  57.72s\n",
      "     16       \u001b[36m0.35120\u001b[0m       1.59853      0.21970      0.61918  57.67s\n",
      "     17       \u001b[36m0.33488\u001b[0m       1.62430      0.20617      0.61686  57.97s\n",
      "     18       \u001b[36m0.33211\u001b[0m       1.62110      0.20487      0.61530  59.29s\n",
      "     19       \u001b[36m0.32960\u001b[0m       1.61594      0.20397      0.61734  57.10s\n",
      "     20       \u001b[36m0.32020\u001b[0m       1.63017      0.19642      0.61842  57.20s\n",
      "     21       \u001b[36m0.31207\u001b[0m       1.64273      0.18997      0.61859  59.32s\n",
      "     22       \u001b[36m0.30267\u001b[0m       1.65757      0.18260      0.62057  57.33s\n",
      "     23       0.30350       1.66378      0.18241      0.61892  57.28s\n",
      "     24       \u001b[36m0.29926\u001b[0m       1.66369      0.17988      0.61995  57.19s\n",
      "     25       \u001b[36m0.29362\u001b[0m       1.68688      0.17406      0.61630  57.35s\n",
      "     26       \u001b[36m0.28952\u001b[0m       1.66732      0.17365      0.61738  57.72s\n",
      "     27       \u001b[36m0.28440\u001b[0m       1.67435      0.16986      0.61703  59.47s\n",
      "     28       \u001b[36m0.27203\u001b[0m       1.69972      0.16004      0.61988  60.18s\n",
      "     29       0.27264       1.71432      0.15904      0.61842  58.39s\n",
      "     30       0.27580       1.71674      0.16066      0.61870  57.38s\n",
      "     31       \u001b[36m0.26728\u001b[0m       1.72573      0.15488      0.61328  57.11s\n",
      "     32       \u001b[36m0.26512\u001b[0m       1.73250      0.15303      0.61582  64.64s\n",
      "     33       0.26955       1.74115      0.15481      0.61259  59.92s\n",
      "     34       \u001b[36m0.26350\u001b[0m       1.71088      0.15401      0.61792  64.84s\n",
      "     35       \u001b[36m0.25741\u001b[0m       1.73326      0.14851      0.61738  60.75s\n",
      "     36       \u001b[36m0.25288\u001b[0m       1.74869      0.14461      0.61641  67.46s\n",
      "     37       0.25638       1.73227      0.14800      0.61715  60.92s\n",
      "     38       \u001b[36m0.24530\u001b[0m       1.76034      0.13935      0.61616  59.71s\n",
      "     39       \u001b[36m0.24480\u001b[0m       1.77095      0.13823      0.61859  60.31s\n",
      "     40       0.25349       1.74042      0.14565      0.61530  60.29s\n",
      "     41       \u001b[36m0.24160\u001b[0m       1.76823      0.13663      0.61418  61.63s\n",
      "     42       \u001b[36m0.24077\u001b[0m       1.75898      0.13688      0.61717  64.26s\n",
      "     43       \u001b[36m0.23515\u001b[0m       1.77821      0.13224      0.61651  59.09s\n",
      "     44       \u001b[36m0.22871\u001b[0m       1.79370      0.12751      0.61589  65.33s\n",
      "     45       0.23802       1.78236      0.13354      0.61479  64.66s\n",
      "     46       0.23188       1.78860      0.12964      0.61769  59.05s\n",
      "     47       \u001b[36m0.22769\u001b[0m       1.79768      0.12666      0.61821  63.30s\n",
      "     48       0.23238       1.80564      0.12870      0.61653  62.24s\n",
      "     49       \u001b[36m0.22427\u001b[0m       1.81643      0.12347      0.61726  60.60s\n",
      "     50       \u001b[36m0.22264\u001b[0m       1.82904      0.12172      0.61820  60.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atulkumar/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:417: Warning: The least populated class in y has only 3 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-49417c659ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my_prob_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclfxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_data_for2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob_et\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob_xgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrain_data_y_for2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/atulkumar/anaconda/lib/python2.7/site-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train.toarray())\n",
    "X_test_std = scaler.transform(X_test.toarray())\n",
    "nn.fit(X_train_std, y_train)\n",
    "y_prob_nn = nn.predict_proba(X_test_std)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_prob_rf = rf.predict_proba(X_test)\n",
    "\n",
    "et.fit(X_train, y_train)\n",
    "y_prob_et = et.predict_proba(X_test)\n",
    "\n",
    "clfxgb.fit(X_train, y_train)\n",
    "y_prob_xgb = clfxgb.predict_proba(X_test)\n",
    "\n",
    "train_data_for2 = sparse.hstack((X_test, y_prob_nn, y_prob_rf, y_prob_et, y_prob_xgb))\n",
    "\n",
    "train_data_y_for2 = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix_test_std = scaler.transform(feature_matrix_test.toarray())\n",
    "\n",
    "test_data_pred_nn = nn.predict_proba(feature_matrix_test_std)\n",
    "test_data_pred_rf = rf.predict_proba(feature_matrix_test)\n",
    "test_data_pred_et = et.predict_proba(feature_matrix_test)\n",
    "test_data_pred_clfxgb = clfxgb.predict_proba(feature_matrix_test)\n",
    "\n",
    "test_data_for2 = sparse.hstack((feature_matrix_test, test_data_pred_nn, test_data_pred_rf, test_data_pred_et, test_data_pred_clfxgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ids = test_df[['VisitNumber']].groupby('VisitNumber').first()\n",
    "df_ids = df_ids.reset_index()\n",
    "ids = df_ids.VisitNumber\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "train_data_for2_std = scaler2.fit_transform(train_data_for2.toarray())\n",
    "test_data_for2_std = scaler2.transform(test_data_for2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process2(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, prefix, layers, ids):\n",
    "    num_test, num_features = test_data_for2.shape\n",
    "    num_classes = 38\n",
    "    nn2 = NeuralNet(layers=layers,\n",
    "                 objective_loss_function=categorical_crossentropy,\n",
    "                 input_shape=(None, num_features),\n",
    "                 \n",
    "                 dropout1_p=0.15,\n",
    "                 dropout2_p=0.25,\n",
    "                 dropout3_p=0.25,\n",
    "                 \n",
    "                 hidden1_num_units=1000,\n",
    "                 hidden2_num_units=500,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=adagrad,     #nesterov_momentum,  \n",
    "                 update_learning_rate=theano.shared(np.float32(0.01)),\n",
    "                 #update_momentum=0.04,\n",
    "                 \n",
    "                 train_split=TrainSplit(eval_size=0.2),\n",
    "                 verbose=1,\n",
    "                 max_epochs=18)\n",
    "\n",
    "    clfxgb2 = XGBoostClassifier(**params)\n",
    "\n",
    "    pred1 = np.zeros((num_test, num_classes)).astype(np.float32)\n",
    "    pred2 = np.zeros((num_test, num_classes)).astype(np.float32)\n",
    "\n",
    "    for i in range(10):\n",
    "        clfxgb2.fit(train_data_for2, train_data_y_for2)\n",
    "        pred1 += clfxgb2.predict_proba(test_data_for2)\n",
    "        nn2.fit(train_data_for2_std, train_data_y_for2)\n",
    "        pred2 += nn2.predict_proba(test_data_for2_std)\n",
    "\n",
    "    pred11 = pred1/10\n",
    "    #pred21 = pred2/10\n",
    "    #pred = (pred11 + pred21)/2\n",
    "    pred = pred1\n",
    "    make_submission_ensemble(pred, ids, encoder, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import cPickle \n",
    "#f = file('train_data_for2.save', 'wb')\n",
    "#cPickle.dump(train_data_for2, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f.close()\n",
    "#f1 = file('train_data_for2_std.save', 'wb')\n",
    "#cPickle.dump(train_data_for2_std, f1, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f1.close()\n",
    "#f2 = file('train_data_y_for2.save', 'wb')\n",
    "#cPickle.dump(train_data_y_for2, f2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f2.close()\n",
    "#f3 = file('test_data_for2.save', 'wb')\n",
    "#cPickle.dump(test_data_for2, f3, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f3.close()\n",
    "#f4 = file('test_data_for2_std.save', 'wb')\n",
    "#cPickle.dump(test_data_for2_std, f4, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f4.close()\n",
    "\n",
    "#f5 = file('ids.save', 'wb')\n",
    "#cPickle.dump(ids, f5, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f5.close()\n",
    "\n",
    "#f = file('obj.save', 'rb')\n",
    "#loaded_obj = cPickle.load(f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 4799538 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input       4279\n",
      "  1  dropout1    4279\n",
      "  2  hidden1     1000\n",
      "  3  dropout2    1000\n",
      "  4  hidden2      500\n",
      "  5  dropout3     500\n",
      "  6  output        38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atulkumar/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:417: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1b3eb50fb8c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpxgb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_for2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_for2_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_y_for2\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0mtest_data_for2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_for2_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mega_ensemble20\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpxgb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpxgb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpxgb3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_for2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_for2_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_y_for2\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0mtest_data_for2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_for2_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mega_ensemble30\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/atulkumar/anaconda/lib/python2.7/multiprocessing/process.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0m_current_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/atulkumar/anaconda/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mdeadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/atulkumar/anaconda/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pxgb1 = Process(target=process2, args=(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble1\", layers, ids))\n",
    "pxgb1.start()\n",
    "pxgb1.join()\n",
    "\n",
    "pxgb2 = Process(target=process2, args=(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble2\", layers, ids))\n",
    "pxgb2.start()\n",
    "pxgb2.join()\n",
    "\n",
    "pxgb3 = Process(target=process2, args=(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble3\", layers, ids))\n",
    "pxgb3.start()\n",
    "pxgb3.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
