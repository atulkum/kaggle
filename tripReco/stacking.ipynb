{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "import theano\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import TrainSplit\n",
    "from lasagne.updates import nesterov_momentum, adagrad\n",
    "from lasagne.objectives import categorical_crossentropy, aggregate\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "%run 'XGBoost_class.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(pred, y):\n",
    "    eps = 1e-15\n",
    "    total = 0.\n",
    "    for i in range(len(y)):\n",
    "        p = max(min(pred[i][y[i]], (1 - eps)), eps)\n",
    "        total += math.log(p)\n",
    "    return -(total/len(y))\n",
    "\n",
    "def prepareForCountVector(df, columnName, dictCount=2000, topk_dict=None):\n",
    "    if not topk_dict:\n",
    "        col = df[columnName].dropna()\n",
    "        counts = col.value_counts()\n",
    "        topk_dict = counts.iloc[0:min(dictCount, len(col))].index\n",
    "    \n",
    "        topk_dict = set(topk_dict).union(set(topk_dict))\n",
    "        \n",
    "    col = col.fillna('')\n",
    "    \n",
    "    topk = df[columnName].apply(lambda x: '%s%d'%(columnName, x) if x in topk_dict else '%sother'%(columnName))\n",
    " \n",
    "    topk_se = pd.Series(topk, name=columnName)\n",
    "    df_topk = pd.concat([topk_se, df['VisitNumber']], axis=1)\n",
    "    return topk_dict, df_topk\n",
    "\n",
    "def getCountVector(df, columnName, isWords, vec=None):\n",
    "    if isWords:\n",
    "        df[columnName] = df[columnName].fillna('')\n",
    "    df_topk_gpy = df.groupby('VisitNumber')\n",
    "    df_topk_list = df_topk_gpy.apply(lambda x: list(x[columnName]))\n",
    "    topk_flat = df_topk_list.str.join(' ')\n",
    "    \n",
    "    if not vec: \n",
    "        vec = CountVectorizer() \n",
    "        vec.fit(topk_flat)    \n",
    "    \n",
    "    wc = vec.transform(topk_flat)\n",
    "    wcar = wc.toarray()\n",
    "    \n",
    "    words_count = topk_flat.apply(lambda x : len(x.split(' '))).reshape(-1,1)\n",
    "    ret = None\n",
    "    if isWords:\n",
    "        words_len = topk_flat.apply(lambda x : len(x)).reshape(-1,1)\n",
    "        ret = np.column_stack([wcar, words_count, words_len])\n",
    "    else:\n",
    "        ret = np.column_stack([wcar, words_count])\n",
    "    \n",
    "    return vec, ret\n",
    "\n",
    "def make_submission(clf, X_test, ids, encoder, prefix):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    outCols = ['TripType_' + str(col) for col in encoder.classes_]\n",
    "    \n",
    "    millis = int(round(time.time() * 1000))\n",
    "    filename = '%s_%d'%(prefix, millis)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('VisitNumber,')\n",
    "        f.write(','.join(outCols))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([str(id)] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    f.close()\n",
    "    print(\"Wrote submission to file {}.\".format(filename))\n",
    "    \n",
    "def make_submission_ensemble(y_prob, ids, encoder, prefix):\n",
    "    outCols = ['TripType_' + str(col) for col in encoder.classes_]\n",
    "    \n",
    "    millis = int(round(time.time() * 1000))\n",
    "    filename = '%s_%d'%(prefix, millis)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('VisitNumber,')\n",
    "        f.write(','.join(outCols))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([str(id)] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def getY(train_df):\n",
    "    df_y = train_df[['VisitNumber', 'TripType']].groupby('VisitNumber').first()\n",
    "    df_y = df_y.reset_index()\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(df_y.TripType).astype(np.int32)\n",
    "    return encoder, y\n",
    "def preprocessDataTrain(df):\n",
    "    df_w = df[['VisitNumber', 'Weekday']].groupby('VisitNumber').first()\n",
    "    df_w = df_w.reset_index()\n",
    "    dict_df_w = df_w[['Weekday']].T.to_dict().values()\n",
    "    \n",
    "    dictVec = DictVectorizer()\n",
    "    dictVec.fit(dict_df_w)\n",
    "        \n",
    "    week = dictVec.transform(dict_df_w)\n",
    "    \n",
    "    is_wknd = np.array((df_w['Weekday']=='Sunday') | (df_w['Weekday']=='Saturday'))\n",
    "    is_wknd = is_wknd.reshape(-1,1)\n",
    "\n",
    "    df_upc = prepareForCountVector(df, 'Upc')\n",
    "    upc = getCountVector(df_upc[1], 'Upc', False)\n",
    "\n",
    "    df_fln = prepareForCountVector(df, 'FinelineNumber')\n",
    "    fln = getCountVector(df_fln[1], 'FinelineNumber', False)\n",
    "\n",
    "    words = getCountVector(df, 'DepartmentDescription', True)\n",
    "\n",
    "    df_ScanCount = df[['VisitNumber', 'ScanCount']].groupby('VisitNumber').sum()\n",
    "    df_ScanCount = df_ScanCount.reset_index()\n",
    "    scancount = np.array(df_ScanCount.ScanCount)\n",
    "    scancount = scancount.reshape(-1,1)\n",
    "    \n",
    "    feature_matrix = []\n",
    "    feature_matrix.append(week)\n",
    "    feature_matrix.append(is_wknd)\n",
    "    feature_matrix.append(upc[1])\n",
    "    feature_matrix.append(fln[1])\n",
    "    feature_matrix.append(words[1])\n",
    "    feature_matrix.append(scancount)\n",
    "\n",
    "    feature_matrix = sparse.hstack(feature_matrix).tocsr()\n",
    "\n",
    "    ret_params = {\n",
    "        'week_dictVec': dictVec,\n",
    "        'upc_vec':upc[0],\n",
    "        'upc_dict':df_upc[0],\n",
    "        'fln_vec':fln[0],\n",
    "        'fln_dict':df_fln[0],\n",
    "        'words_vec':words[0]\n",
    "    }\n",
    "    return feature_matrix, ret_params\n",
    "\n",
    "def preprocessDataTest(df, params):\n",
    "    df_w = df[['VisitNumber', 'Weekday']].groupby('VisitNumber').first()\n",
    "    df_w = df_w.reset_index()\n",
    "    dict_df_w = df_w[['Weekday']].T.to_dict().values()\n",
    "    dictVec = params['week_dictVec']\n",
    "    week = dictVec.transform(dict_df_w)\n",
    "    \n",
    "    is_wknd = np.array((df_w['Weekday']=='Sunday') | (df_w['Weekday']=='Saturday'))\n",
    "    is_wknd = is_wknd.reshape(-1,1)\n",
    "            \n",
    "    df_upc = prepareForCountVector(df, 'Upc', params['upc_dict'])\n",
    "    upc = getCountVector(df_upc[1], 'Upc', False, params['upc_vec'])\n",
    "            \n",
    "    df_fln = prepareForCountVector(df, 'FinelineNumber', params['fln_dict'])\n",
    "    fln = getCountVector(df_fln[1], 'FinelineNumber', False, params['fln_vec'])\n",
    "\n",
    "    words = getCountVector(df, 'DepartmentDescription', True, params['words_vec'])\n",
    "\n",
    "    df_ScanCount = df[['VisitNumber', 'ScanCount']].groupby('VisitNumber').sum()\n",
    "    df_ScanCount = df_ScanCount.reset_index()\n",
    "    scancount = np.array(df_ScanCount.ScanCount)\n",
    "    scancount = scancount.reshape(-1,1)\n",
    "    \n",
    "    feature_matrix = []\n",
    "    feature_matrix.append(week)\n",
    "    feature_matrix.append(is_wknd)\n",
    "    feature_matrix.append(upc[1])\n",
    "    feature_matrix.append(fln[1])\n",
    "    feature_matrix.append(words[1])\n",
    "    feature_matrix.append(scancount)\n",
    "\n",
    "    feature_matrix = sparse.hstack(feature_matrix).tocsr()\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TripType\tVisitNumber\tWeekday\tUpc\tScanCount\tDepartmentDescription\tFinelineNumber\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_matrix, params = preprocessDataTrain(train_df)\n",
    "encoder, y = getY(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "feature_matrix_test = preprocessDataTest(test_df,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_test, num_features_test = feature_matrix_test.shape\n",
    "num_train, num_features = feature_matrix.shape\n",
    "assert(num_features_test == num_features)\n",
    "num_classes = len(encoder.classes_)\n",
    "print feature_matrix_test.shape\n",
    "print num_train, num_features, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "layers = [('input', InputLayer),\n",
    "           ('dropout1', DropoutLayer), \n",
    "           ('hidden1', DenseLayer),\n",
    "           ('dropout2', DropoutLayer), \n",
    "           ('hidden2', DenseLayer), \n",
    "           ('dropout3', DropoutLayer),\n",
    "           ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "nn = NeuralNet(layers=layers,\n",
    "                 objective_loss_function=categorical_crossentropy,\n",
    "                 input_shape=(None, num_features),\n",
    "                 \n",
    "                 dropout1_p=0.15,\n",
    "                 dropout2_p=0.25,\n",
    "                 dropout3_p=0.25,\n",
    "                 \n",
    "                 hidden1_num_units=1000,\n",
    "                 hidden2_num_units=500,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=adagrad,     #nesterov_momentum,\n",
    "                 #update_learning_rate=theano.shared(np.float32(0.01)),   \n",
    "                 update_learning_rate=theano.shared(np.float32(0.01)),\n",
    "                 #update_momentum=0.04,\n",
    "                 \n",
    "                 train_split=TrainSplit(eval_size=0.2),\n",
    "                 verbose=1,\n",
    "                 max_epochs=50)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, max_depth=17) \n",
    "et = ExtraTreesClassifier(n_estimators=300, n_jobs=-1, max_depth=25)\n",
    "params = {   \n",
    "              'objective': 'multi:softprob',\n",
    "              'eval_metric': 'mlogloss',\n",
    "              'num_class': num_classes,\n",
    "              'eta': 0.0825,\n",
    "              'max_depth': 10,\n",
    "              'num_round': 2000,\n",
    "              'subsample':0.85, \n",
    "              'colsample_bytree':0.8, \n",
    "              'min_child_weight':5.2475,\n",
    "              'silent':1\n",
    "    }\n",
    "clfxgb = XGBoostClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, y, test_size=0.5, random_state=56)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train.toarray())\n",
    "X_test_std = scaler.transform(X_test.toarray())\n",
    "nn.fit(X_train_std, y_train)\n",
    "y_prob_nn = nn.predict_proba(X_test_std)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_prob_rf = rf.predict_proba(X_test)\n",
    "\n",
    "et.fit(X_train, y_train)\n",
    "y_prob_et = et.predict_proba(X_test)\n",
    "\n",
    "clfxgb.fit(X_train, y_train)\n",
    "y_prob_xgb = clfxgb.predict_proba(X_test)\n",
    "\n",
    "train_data_for2 = sparse.hstack((X_test, y_prob_nn, y_prob_rf, y_prob_et, y_prob_xgb))\n",
    "\n",
    "train_data_y_for2 = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix_test_std = scaler.transform(feature_matrix_test.toarray())\n",
    "\n",
    "test_data_pred_nn = nn.predict_proba(feature_matrix_test_std)\n",
    "test_data_pred_rf = rf.predict_proba(feature_matrix_test)\n",
    "test_data_pred_et = et.predict_proba(feature_matrix_test)\n",
    "test_data_pred_clfxgb = clfxgb.predict_proba(feature_matrix_test)\n",
    "\n",
    "test_data_for2 = sparse.hstack((feature_matrix_test, test_data_pred_nn, test_data_pred_rf, test_data_pred_et, test_data_pred_clfxgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ids = test_df[['VisitNumber']].groupby('VisitNumber').first()\n",
    "df_ids = df_ids.reset_index()\n",
    "ids = df_ids.VisitNumber\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "train_data_for2_std = scaler2.fit_transform(train_data_for2.toarray())\n",
    "test_data_for2_std = scaler2.transform(test_data_for2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process2(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, prefix, layers, ids):\n",
    "    num_test, num_features = test_data_for2.shape\n",
    "    num_classes = 38\n",
    "    nn2 = NeuralNet(layers=layers,\n",
    "                 objective_loss_function=categorical_crossentropy,\n",
    "                 input_shape=(None, num_features),\n",
    "                 \n",
    "                 dropout1_p=0.15,\n",
    "                 dropout2_p=0.25,\n",
    "                 dropout3_p=0.25,\n",
    "                 \n",
    "                 hidden1_num_units=1000,\n",
    "                 hidden2_num_units=500,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=adagrad,     #nesterov_momentum,  \n",
    "                 update_learning_rate=theano.shared(np.float32(0.01)),\n",
    "                 #update_momentum=0.04,\n",
    "                 \n",
    "                 train_split=TrainSplit(eval_size=0.2),\n",
    "                 verbose=1,\n",
    "                 max_epochs=18)\n",
    "\n",
    "    clfxgb2 = XGBoostClassifier(**params)\n",
    "\n",
    "    pred1 = np.zeros((num_test, num_classes)).astype(np.float32)\n",
    "    pred2 = np.zeros((num_test, num_classes)).astype(np.float32)\n",
    "\n",
    "    for i in range(10):\n",
    "        clfxgb2.fit(train_data_for2, train_data_y_for2)\n",
    "        pred1 += clfxgb2.predict_proba(test_data_for2)\n",
    "        nn2.fit(train_data_for2_std, train_data_y_for2)\n",
    "        pred2 += nn2.predict_proba(test_data_for2_std)\n",
    "\n",
    "    pred11 = pred1/10\n",
    "    pred21 = pred2/10\n",
    "    pred = (pred11 + pred21)/2\n",
    "    \n",
    "    make_submission_ensemble(pred, ids, encoder, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import cPickle \n",
    "#f = file('train_data_for2.save', 'wb')\n",
    "#cPickle.dump(train_data_for2, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f.close()\n",
    "#f1 = file('train_data_for2_std.save', 'wb')\n",
    "#cPickle.dump(train_data_for2_std, f1, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f1.close()\n",
    "#f2 = file('train_data_y_for2.save', 'wb')\n",
    "#cPickle.dump(train_data_y_for2, f2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f2.close()\n",
    "#f3 = file('test_data_for2.save', 'wb')\n",
    "#cPickle.dump(test_data_for2, f3, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f3.close()\n",
    "#f4 = file('test_data_for2_std.save', 'wb')\n",
    "#cPickle.dump(test_data_for2_std, f4, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f4.close()\n",
    "\n",
    "#f5 = file('ids.save', 'wb')\n",
    "#cPickle.dump(ids, f5, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f5.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle \n",
    "f = file('train_data_for2.save', 'rb')\n",
    "train_data_for2 = cPickle.load(f)\n",
    "f.close()\n",
    "f1 = file('train_data_y_for2.save', 'rb')\n",
    "train_data_y_for2 = cPickle.load(f1)\n",
    "f1.close()\n",
    "f2 = file('test_data_for2.save', 'rb')\n",
    "test_data_for2 = cPickle.load(f2)\n",
    "f2.close()\n",
    "f3 = file('ids.save', 'rb')\n",
    "ids = cPickle.load(f3)\n",
    "f3.close()\n",
    "scaler2 = StandardScaler()\n",
    "train_data_for2_std = scaler2.fit_transform(train_data_for2.toarray())\n",
    "test_data_for2_std = scaler2.transform(test_data_for2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pxgb1 = Process(target=process2, args=(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble1\", layers, ids))\n",
    "pxgb1.start()\n",
    "pxgb1.join()\n",
    "\n",
    "pxgb2 = Process(target=process2, args=(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble2\", layers, ids))\n",
    "pxgb2.start()\n",
    "pxgb2.join()\n",
    "\n",
    "pxgb3 = Process(target=process2, args=(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble3\", layers, ids))\n",
    "pxgb3.start()\n",
    "pxgb3.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process2(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble4\", layers, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean from two submission files\n",
    "df1 = pd.read_csv('mega_ensemble4_1451246644669', index_col= 'VisitNumber')\n",
    "df2 = pd.read_csv('mega_ensemble_no_upc_fln_1451222117007', index_col='VisitNumber')\n",
    "df_concat = pd.concat((df1, df2))\n",
    "by_row_index = df_concat.groupby(df_concat.index)\n",
    "df_means = by_row_index.mean()\n",
    "\n",
    "millis = int(round(time.time() * 1000))\n",
    "filename = 'mega_ensemble_ensemble%d'%(millis)\n",
    "df_means.to_csv(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
