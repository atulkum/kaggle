{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "import theano\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import TrainSplit\n",
    "from lasagne.updates import nesterov_momentum, adagrad\n",
    "from lasagne.objectives import categorical_crossentropy, aggregate\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "%run 'XGBoost_class.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(pred, y):\n",
    "    eps = 1e-15\n",
    "    total = 0.\n",
    "    for i in range(len(y)):\n",
    "        p = max(min(pred[i][y[i]], (1 - eps)), eps)\n",
    "        total += math.log(p)\n",
    "    return -(total/len(y))\n",
    "\n",
    "def prepareForCountVector(df, columnName, dictCount=2000, topk_dict=None):\n",
    "    if not topk_dict:\n",
    "        col = df[columnName].dropna()\n",
    "        counts = col.value_counts()\n",
    "        topk_dict = counts.iloc[0:min(dictCount, len(col))].index\n",
    "    \n",
    "        topk_dict = set(topk_dict).union(set(topk_dict))\n",
    "        \n",
    "    col = col.fillna('')\n",
    "    \n",
    "    topk = df[columnName].apply(lambda x: '%s%d'%(columnName, x) if x in topk_dict else '%sother'%(columnName))\n",
    " \n",
    "    topk_se = pd.Series(topk, name=columnName)\n",
    "    df_topk = pd.concat([topk_se, df['VisitNumber']], axis=1)\n",
    "    return topk_dict, df_topk\n",
    "\n",
    "def getCountVector(df, columnName, isWords, vec=None):\n",
    "    if isWords:\n",
    "        df[columnName] = df[columnName].fillna('')\n",
    "    df_topk_gpy = df.groupby('VisitNumber')\n",
    "    df_topk_list = df_topk_gpy.apply(lambda x: list(x[columnName]))\n",
    "    topk_flat = df_topk_list.str.join(' ')\n",
    "    \n",
    "    if not vec: \n",
    "        vec = CountVectorizer() \n",
    "        vec.fit(topk_flat)    \n",
    "    \n",
    "    wc = vec.transform(topk_flat)\n",
    "    wcar = wc.toarray()\n",
    "    \n",
    "    words_count = topk_flat.apply(lambda x : len(x.split(' '))).reshape(-1,1)\n",
    "    ret = None\n",
    "    if isWords:\n",
    "        words_len = topk_flat.apply(lambda x : len(x)).reshape(-1,1)\n",
    "        ret = np.column_stack([wcar, words_count, words_len])\n",
    "    else:\n",
    "        ret = np.column_stack([wcar, words_count])\n",
    "    \n",
    "    return vec, ret\n",
    "\n",
    "def make_submission(clf, X_test, ids, encoder, prefix):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    outCols = ['TripType_' + col for col in encoder.classes_]\n",
    "    \n",
    "    millis = int(round(time.time() * 1000))\n",
    "    filename = '%s_%d'%(prefix, millis)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('VisitNumber,')\n",
    "        f.write(','.join(outCols))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(filename))\n",
    "    \n",
    "def make_submission_ensemble(y_prob, ids, encoder, prefix):\n",
    "    outCols = ['TripType_' + col for col in encoder.classes_]\n",
    "    \n",
    "    millis = int(round(time.time() * 1000))\n",
    "    filename = '%s_%d'%(prefix, millis)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('VisitNumber,')\n",
    "        f.write(','.join(outCols))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def getY(train_df):\n",
    "    df_y = train_df[['VisitNumber', 'TripType']].groupby('VisitNumber').first()\n",
    "    df_y = df_y.reset_index()\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(df_y.TripType).astype(np.int32)\n",
    "    return encoder, y\n",
    "def preprocessDataTrain(df):\n",
    "    df_w = df[['VisitNumber', 'Weekday']].groupby('VisitNumber').first()\n",
    "    df_w = df_w.reset_index()\n",
    "    dict_df_w = df_w[['Weekday']].T.to_dict().values()\n",
    "    \n",
    "    dictVec = DictVectorizer()\n",
    "    dictVec.fit(dict_df_w)\n",
    "        \n",
    "    week = dictVec.transform(dict_df_w)\n",
    "    \n",
    "    is_wknd = np.array((df_w['Weekday']=='Sunday') | (df_w['Weekday']=='Saturday'))\n",
    "    is_wknd = is_wknd.reshape(-1,1)\n",
    "\n",
    "    df_upc = prepareForCountVector(df, 'Upc')\n",
    "    upc = getCountVector(df_upc[1], 'Upc', False)\n",
    "\n",
    "    df_fln = prepareForCountVector(df, 'FinelineNumber')\n",
    "    fln = getCountVector(df_fln[1], 'FinelineNumber', False)\n",
    "\n",
    "    words = getCountVector(df, 'DepartmentDescription', True)\n",
    "\n",
    "    df_ScanCount = df[['VisitNumber', 'ScanCount']].groupby('VisitNumber').sum()\n",
    "    df_ScanCount = df_ScanCount.reset_index()\n",
    "    scancount = np.array(df_ScanCount.ScanCount)\n",
    "    scancount = scancount.reshape(-1,1)\n",
    "    \n",
    "    feature_matrix = []\n",
    "    feature_matrix.append(week)\n",
    "    feature_matrix.append(is_wknd)\n",
    "    feature_matrix.append(upc[1])\n",
    "    feature_matrix.append(fln[1])\n",
    "    feature_matrix.append(words[1])\n",
    "    feature_matrix.append(scancount)\n",
    "\n",
    "    feature_matrix = sparse.hstack(feature_matrix).tocsr()\n",
    "\n",
    "    ret_params = {\n",
    "        'week_dictVec': dictVec,\n",
    "        'upc_vec':upc[0],\n",
    "        'upc_dict':df_upc[0],\n",
    "        'fln_vec':fln[0],\n",
    "        'fln_dict':df_fln[0],\n",
    "        'words_vec':words[0]\n",
    "    }\n",
    "    return feature_matrix, ret_params\n",
    "\n",
    "def preprocessDataTest(df, params):\n",
    "    df_w = df[['VisitNumber', 'Weekday']].groupby('VisitNumber').first()\n",
    "    df_w = df_w.reset_index()\n",
    "    dict_df_w = df_w[['Weekday']].T.to_dict().values()\n",
    "    dictVec = params['week_dictVec']\n",
    "    week = dictVec.transform(dict_df_w)\n",
    "    \n",
    "    is_wknd = np.array((df_w['Weekday']=='Sunday') | (df_w['Weekday']=='Saturday'))\n",
    "    is_wknd = is_wknd.reshape(-1,1)\n",
    "            \n",
    "    df_upc = prepareForCountVector(df, 'Upc', params['upc_dict'])\n",
    "    upc = getCountVector(df_upc[1], 'Upc', False, params['upc_vec'])\n",
    "            \n",
    "    df_fln = prepareForCountVector(df, 'FinelineNumber', params['fln_dict'])\n",
    "    fln = getCountVector(df_fln[1], 'FinelineNumber', False, params['fln_vec'])\n",
    "\n",
    "    words = getCountVector(df, 'DepartmentDescription', True, params['words_vec'])\n",
    "\n",
    "    df_ScanCount = df[['VisitNumber', 'ScanCount']].groupby('VisitNumber').sum()\n",
    "    df_ScanCount = df_ScanCount.reset_index()\n",
    "    scancount = np.array(df_ScanCount.ScanCount)\n",
    "    scancount = scancount.reshape(-1,1)\n",
    "    \n",
    "    feature_matrix = []\n",
    "    feature_matrix.append(week)\n",
    "    feature_matrix.append(is_wknd)\n",
    "    feature_matrix.append(upc[1])\n",
    "    feature_matrix.append(fln[1])\n",
    "    feature_matrix.append(words[1])\n",
    "    feature_matrix.append(scancount)\n",
    "\n",
    "    feature_matrix = sparse.hstack(feature_matrix).tocsr()\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TripType\tVisitNumber\tWeekday\tUpc\tScanCount\tDepartmentDescription\tFinelineNumber\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_matrix, params = preprocessDataTrain(train_df)\n",
    "encoder, y = getY(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "feature_matrix_test = preprocessDataTest(test_df,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 4127)\n",
      "95674 4127 38\n"
     ]
    }
   ],
   "source": [
    "num_test, num_features_test = feature_matrix_test.shape\n",
    "num_train, num_features = feature_matrix.shape\n",
    "assert(num_features_test == num_features)\n",
    "num_classes = len(encoder.classes_)\n",
    "print feature_matrix_test.shape\n",
    "print num_train, num_features, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "layers = [('input', InputLayer),\n",
    "           ('dropout1', DropoutLayer), \n",
    "           ('hidden1', DenseLayer),\n",
    "           ('dropout2', DropoutLayer), \n",
    "           ('hidden2', DenseLayer), \n",
    "           ('dropout3', DropoutLayer),\n",
    "           ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "nn = NeuralNet(layers=layers,\n",
    "                 objective_loss_function=categorical_crossentropy,\n",
    "                 input_shape=(None, num_features),\n",
    "                 \n",
    "                 dropout1_p=0.15,\n",
    "                 dropout2_p=0.25,\n",
    "                 dropout3_p=0.25,\n",
    "                 \n",
    "                 hidden1_num_units=1000,\n",
    "                 hidden2_num_units=500,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=adagrad,     #nesterov_momentum,\n",
    "                 #update_learning_rate=theano.shared(np.float32(0.01)),   \n",
    "                 update_learning_rate=theano.shared(np.float32(0.01)),\n",
    "                 #update_momentum=0.04,\n",
    "                 \n",
    "                 train_split=TrainSplit(eval_size=0.2),\n",
    "                 verbose=1,\n",
    "                 max_epochs=50)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, max_depth=17) \n",
    "et = ExtraTreesClassifier(n_estimators=300, n_jobs=-1, max_depth=25)\n",
    "params = {   \n",
    "              'objective': 'multi:softprob',\n",
    "              'eval_metric': 'mlogloss',\n",
    "              'num_class': num_classes,\n",
    "              'eta': 0.0825,\n",
    "              'max_depth': 10,\n",
    "              'num_round': 2000,\n",
    "              'subsample':0.85, \n",
    "              'colsample_bytree':0.8, \n",
    "              'min_child_weight':5.2475,\n",
    "              'silent':1\n",
    "    }\n",
    "clfxgb = XGBoostClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, y, test_size=0.5, random_state=56)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 4647538 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input       4127\n",
      "  1  dropout1    4127\n",
      "  2  hidden1     1000\n",
      "  3  dropout2    1000\n",
      "  4  hidden2      500\n",
      "  5  dropout3     500\n",
      "  6  output        38\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.19536\u001b[0m       \u001b[32m1.43264\u001b[0m      1.53238      0.57410  85.04s\n",
      "      2       \u001b[36m1.18087\u001b[0m       \u001b[32m1.32627\u001b[0m      0.89037      0.59953  80.35s\n",
      "      3       \u001b[36m0.91240\u001b[0m       \u001b[32m1.31042\u001b[0m      0.69627      0.60761  76.29s\n",
      "      4       \u001b[36m0.75473\u001b[0m       1.35137      0.55849      0.61214  67.07s\n",
      "      5       \u001b[36m0.65576\u001b[0m       1.36196      0.48148      0.61533  59.01s\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train.toarray())\n",
    "X_test_std = scaler.transform(X_test.toarray())\n",
    "nn.fit(X_train_std, y_train)\n",
    "y_prob_nn = nn.predict_proba(X_test_std)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_prob_rf = rf.predict_proba(X_test)\n",
    "\n",
    "et.fit(X_train, y_train)\n",
    "y_prob_et = et.predict_proba(X_test)\n",
    "\n",
    "clfxgb.fit(X_train, y_train)\n",
    "y_prob_xgb = clfxgb.predict_proba(X_test)\n",
    "\n",
    "train_data_for2 = np.hstack((X_test, y_prob_nn, y_prob_rf, y_prob_et, y_prob_xgb))\n",
    "\n",
    "train_data_y_for2 = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix_test_std = scaler.transform(feature_matrix_test.toarray())\n",
    "\n",
    "test_data_pred_nn = nn.predict_proba(feature_matrix_test_std)\n",
    "test_data_pred_rf = rf.predict_proba(feature_matrix_test)\n",
    "test_data_pred_et = et.predict_proba(feature_matrix_test)\n",
    "test_data_pred_clfxgb = clfxgb.predict_proba(feature_matrix_test)\n",
    "\n",
    "test_data_for2 = np.hstack((feature_matrix_test_std, test_data_pred_nn, test_data_pred_rf, test_data_pred_et, test_data_pred_clfxgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn2 = NeuralNet(layers=layers,\n",
    "                 objective_loss_function=categorical_crossentropy,\n",
    "                 input_shape=(None, num_features),\n",
    "                 \n",
    "                 dropout1_p=0.15,\n",
    "                 dropout2_p=0.25,\n",
    "                 dropout3_p=0.25,\n",
    "                 \n",
    "                 hidden1_num_units=1000,\n",
    "                 hidden2_num_units=500,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=adagrad,     #nesterov_momentum,\n",
    "                 #update_learning_rate=theano.shared(np.float32(0.01)),   \n",
    "                 update_learning_rate=theano.shared(np.float32(0.01)),\n",
    "                 #update_momentum=0.04,\n",
    "                 \n",
    "                 train_split=TrainSplit(eval_size=0.2),\n",
    "                 verbose=1,\n",
    "                 max_epochs=18)\n",
    "\n",
    "clfxgb2 = XGBoostClassifier(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler()\n",
    "train_data_for2_std = scaler2.fit_transform(train_data_for2.toarray())\n",
    "test_data_for2_std = scaler2.transform(test_data_for2.toarray())\n",
    "\n",
    "pred1 = np.zeros((num_test, num_classes)).astype(np.float32)\n",
    "pred2 = np.zeros((num_test, num_classes)).astype(np.float32)\n",
    "\n",
    "for i in range(30):\n",
    "    clfxgb2.fit(train_data_for2, train_data_y_for2)\n",
    "    pred1 += clfxgb2.predict_proba(test_data_for2)\n",
    "    nn2.fit(train_data_for2_std, train_data_y_for2)\n",
    "    pred2 += nn2.predict_proba(test_data_for2_std)\n",
    "\n",
    "pred1 = pred1/30\n",
    "pred2 = pred2/30\n",
    "pred = (pred1 + pred2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ids = test_df[['VisitNumber']].groupby('VisitNumber').first()\n",
    "df_ids = df_ids.reset_index()\n",
    "make_submission_ensemble(pred, df_ids.VisitNumber, encoder, \"mega_ensemble\"):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
