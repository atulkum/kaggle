{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "import theano\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import TrainSplit\n",
    "from lasagne.updates import nesterov_momentum, adagrad\n",
    "from lasagne.objectives import categorical_crossentropy, aggregate\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "%run 'XGBoost_class.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_function(pred, y):\n",
    "    eps = 1e-15\n",
    "    total = 0.\n",
    "    for i in range(len(y)):\n",
    "        p = max(min(pred[i][y[i]], (1 - eps)), eps)\n",
    "        total += math.log(p)\n",
    "    return -(total/len(y))\n",
    "\n",
    "def prepareForCountVector(df, columnName, dictCount=2000, topk_dict=None):\n",
    "    if topk_dict is None:\n",
    "        col = df[columnName].dropna()\n",
    "        counts = col.value_counts()\n",
    "        topk_dict = np.array(counts.iloc[0:min(dictCount, len(col))].index)\n",
    "    else:\n",
    "        print 'topk_dict is not none'\n",
    "    topk = df[columnName].apply(lambda x: '%s%d'%(columnName, x) if x in topk_dict else '%sother'%(columnName))\n",
    " \n",
    "    topk_se = pd.Series(topk, name=columnName)\n",
    "    df_topk = pd.concat([topk_se, df['VisitNumber']], axis=1)\n",
    "    return topk_dict, df_topk\n",
    "\n",
    "def getCountVector(df, columnName, isWords, vec=None):\n",
    "    if isWords:\n",
    "        df[columnName] = df[columnName].fillna('')\n",
    "    df_topk_gpy = df.groupby('VisitNumber')\n",
    "    df_topk_list = df_topk_gpy.apply(lambda x: list(x[columnName]))\n",
    "    topk_flat = df_topk_list.str.join(' ')\n",
    "    \n",
    "    if vec is None: \n",
    "        print 'building model for vec ......'\n",
    "        vec = CountVectorizer() \n",
    "        vec.fit(topk_flat)    \n",
    "    else:\n",
    "        print 'vec is not none'\n",
    "    wc = vec.transform(topk_flat)\n",
    "    wcar = wc.toarray()\n",
    "    \n",
    "    words_count = topk_flat.apply(lambda x : len(x.split(' '))).reshape(-1,1)\n",
    "    ret = None\n",
    "    if isWords:\n",
    "        words_len = topk_flat.apply(lambda x : len(x)).reshape(-1,1)\n",
    "        ret = np.column_stack([wcar, words_count, words_len])\n",
    "    else:\n",
    "        ret = np.column_stack([wcar, words_count])\n",
    "    \n",
    "    return vec, ret\n",
    "\n",
    "def make_submission(clf, X_test, ids, encoder, prefix):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    outCols = ['TripType_' + str(col) for col in encoder.classes_]\n",
    "    \n",
    "    millis = int(round(time.time() * 1000))\n",
    "    filename = '%s_%d'%(prefix, millis)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('VisitNumber,')\n",
    "        f.write(','.join(outCols))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([str(id)] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    f.close()\n",
    "    print(\"Wrote submission to file {}.\".format(filename))\n",
    "    \n",
    "def make_submission_ensemble(y_prob, ids, encoder, prefix):\n",
    "    outCols = ['TripType_' + str(col) for col in encoder.classes_]\n",
    "    \n",
    "    millis = int(round(time.time() * 1000))\n",
    "    filename = '%s_%d'%(prefix, millis)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('VisitNumber,')\n",
    "        f.write(','.join(outCols))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([str(id)] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def getY(train_df):\n",
    "    df_y = train_df[['VisitNumber', 'TripType']].groupby('VisitNumber').first()\n",
    "    df_y = df_y.reset_index()\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(df_y.TripType).astype(np.int32)\n",
    "    return encoder, y\n",
    "def preprocessDataTrain(df):\n",
    "    df_w = df[['VisitNumber', 'Weekday']].groupby('VisitNumber').first()\n",
    "    df_w = df_w.reset_index()\n",
    "    dict_df_w = df_w[['Weekday']].T.to_dict().values()\n",
    "    \n",
    "    dictVec = DictVectorizer()\n",
    "    dictVec.fit(dict_df_w)\n",
    "        \n",
    "    week = dictVec.transform(dict_df_w)\n",
    "    \n",
    "    is_wknd = np.array((df_w['Weekday']=='Sunday') | (df_w['Weekday']=='Saturday'))\n",
    "    is_wknd = is_wknd.reshape(-1,1)\n",
    "\n",
    "    df_upc = prepareForCountVector(df, 'Upc')\n",
    "    upc = getCountVector(df_upc[1], 'Upc', False)\n",
    "\n",
    "    df_fln = prepareForCountVector(df, 'FinelineNumber')\n",
    "    fln = getCountVector(df_fln[1], 'FinelineNumber', False)\n",
    "\n",
    "    words = getCountVector(df, 'DepartmentDescription', True)\n",
    "\n",
    "    df_ScanCount = df[['VisitNumber', 'ScanCount']].groupby('VisitNumber').sum()\n",
    "    df_ScanCount = df_ScanCount.reset_index()\n",
    "    scancount = np.array(df_ScanCount.ScanCount)\n",
    "    scancount = scancount.reshape(-1,1)\n",
    "    \n",
    "    feature_matrix = []\n",
    "    feature_matrix.append(week)\n",
    "    feature_matrix.append(is_wknd)\n",
    "    feature_matrix.append(upc[1])\n",
    "    feature_matrix.append(fln[1])\n",
    "    feature_matrix.append(words[1])\n",
    "    feature_matrix.append(scancount)\n",
    "\n",
    "    feature_matrix = sparse.hstack(feature_matrix).tocsr()\n",
    "\n",
    "    ret_params = {\n",
    "        'week_dictVec': dictVec,\n",
    "        'upc_vec':upc[0],\n",
    "        'upc_dict':df_upc[0],\n",
    "        'fln_vec':fln[0],\n",
    "        'fln_dict':df_fln[0],\n",
    "        'words_vec':words[0]\n",
    "    }\n",
    "    return feature_matrix, ret_params\n",
    "\n",
    "def preprocessDataTest(df, params):\n",
    "    df_w = df[['VisitNumber', 'Weekday']].groupby('VisitNumber').first()\n",
    "    df_w = df_w.reset_index()\n",
    "    dict_df_w = df_w[['Weekday']].T.to_dict().values()\n",
    "    dictVec = params['week_dictVec']\n",
    "    week = dictVec.transform(dict_df_w)\n",
    "    \n",
    "    is_wknd = np.array((df_w['Weekday']=='Sunday') | (df_w['Weekday']=='Saturday'))\n",
    "    is_wknd = is_wknd.reshape(-1,1)\n",
    "            \n",
    "    df_upc = prepareForCountVector(df, 'Upc', topk_dict=params['upc_dict'])\n",
    "    upc = getCountVector(df_upc[1], 'Upc', False, params['upc_vec'])\n",
    "            \n",
    "    df_fln = prepareForCountVector(df, 'FinelineNumber', topk_dict=params['fln_dict'])\n",
    "    fln = getCountVector(df_fln[1], 'FinelineNumber', False, params['fln_vec'])\n",
    "\n",
    "    words = getCountVector(df, 'DepartmentDescription', True, params['words_vec'])\n",
    "\n",
    "    df_ScanCount = df[['VisitNumber', 'ScanCount']].groupby('VisitNumber').sum()\n",
    "    df_ScanCount = df_ScanCount.reset_index()\n",
    "    scancount = np.array(df_ScanCount.ScanCount)\n",
    "    scancount = scancount.reshape(-1,1)\n",
    "    \n",
    "    feature_matrix = []\n",
    "    feature_matrix.append(week)\n",
    "    feature_matrix.append(is_wknd)\n",
    "    feature_matrix.append(upc[1])\n",
    "    feature_matrix.append(fln[1])\n",
    "    feature_matrix.append(words[1])\n",
    "    feature_matrix.append(scancount)\n",
    "\n",
    "    feature_matrix = sparse.hstack(feature_matrix).tocsr()\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TripType\tVisitNumber\tWeekday\tUpc\tScanCount\tDepartmentDescription\tFinelineNumber\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model for vec ......\n",
      "building model for vec ......\n",
      "building model for vec ......\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, params = preprocessDataTrain(train_df)\n",
    "encoder, y = getY(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk_dict is not none\n",
      "vec is not none\n",
      "topk_dict is not none\n",
      "vec is not none\n",
      "vec is not none\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "feature_matrix_test = preprocessDataTest(test_df,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 4127)\n",
      "95674 4127 38\n"
     ]
    }
   ],
   "source": [
    "num_test, num_features_test = feature_matrix_test.shape\n",
    "num_train, num_features = feature_matrix.shape\n",
    "assert(num_features_test == num_features)\n",
    "num_classes = len(encoder.classes_)\n",
    "print feature_matrix_test.shape\n",
    "print num_train, num_features, num_classes\n",
    "df_ids = test_df[['VisitNumber']].groupby('VisitNumber').first()\n",
    "df_ids = df_ids.reset_index()\n",
    "ids = df_ids.VisitNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "layers = [('input', InputLayer),\n",
    "           ('dropout1', DropoutLayer), \n",
    "           ('hidden1', DenseLayer),\n",
    "           ('dropout2', DropoutLayer), \n",
    "           ('hidden2', DenseLayer), \n",
    "           ('dropout3', DropoutLayer),\n",
    "           ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "nn = NeuralNet(layers=layers,\n",
    "                 objective_loss_function=categorical_crossentropy,\n",
    "                 input_shape=(None, num_features),\n",
    "                 \n",
    "                 dropout1_p=0.15,\n",
    "                 dropout2_p=0.25,\n",
    "                 dropout3_p=0.25,\n",
    "                 \n",
    "                 hidden1_num_units=1000,\n",
    "                 hidden2_num_units=500,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=adagrad,     #nesterov_momentum,\n",
    "                 #update_learning_rate=theano.shared(np.float32(0.01)),   \n",
    "                 update_learning_rate=theano.shared(np.float32(0.01)),\n",
    "                 #update_momentum=0.04,\n",
    "                 \n",
    "                 train_split=TrainSplit(eval_size=0.2),\n",
    "                 verbose=1,\n",
    "                 max_epochs=50)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, max_depth=17) \n",
    "et = ExtraTreesClassifier(n_estimators=300, n_jobs=-1, max_depth=25)\n",
    "params = {   \n",
    "              'objective': 'multi:softprob',\n",
    "              'eval_metric': 'mlogloss',\n",
    "              'num_class': num_classes,\n",
    "              'eta': 0.0825,\n",
    "              'max_depth': 10,\n",
    "              'num_round': 2000,\n",
    "              'subsample':0.85, \n",
    "              'colsample_bytree':0.8, \n",
    "              'min_child_weight':5.2475,\n",
    "              'silent':1\n",
    "    }\n",
    "clfxgb = XGBoostClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, y, test_size=0.5, random_state=56)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 4647538 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input       4127\n",
      "  1  dropout1    4127\n",
      "  2  hidden1     1000\n",
      "  3  dropout2    1000\n",
      "  4  hidden2      500\n",
      "  5  dropout3     500\n",
      "  6  output        38\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.26950\u001b[0m       \u001b[32m1.44072\u001b[0m      1.57525      0.57138  69.06s\n",
      "      2       \u001b[36m1.20048\u001b[0m       \u001b[32m1.32506\u001b[0m      0.90599      0.59768  67.45s\n",
      "      3       \u001b[36m0.92882\u001b[0m       1.32776      0.69954      0.60557  59.10s\n",
      "      4       \u001b[36m0.76391\u001b[0m       1.35318      0.56453      0.61181  62.92s\n",
      "      5       \u001b[36m0.66312\u001b[0m       1.37246      0.48316      0.61392  63.94s\n",
      "      6       \u001b[36m0.59062\u001b[0m       1.42153      0.41548      0.61330  58.35s\n",
      "      7       \u001b[36m0.53520\u001b[0m       1.43788      0.37221      0.61750  57.36s\n",
      "      8       \u001b[36m0.49331\u001b[0m       1.46885      0.33585      0.61498  57.58s\n",
      "      9       \u001b[36m0.46696\u001b[0m       1.46972      0.31772      0.61731  57.36s\n",
      "     10       \u001b[36m0.44705\u001b[0m       1.47493      0.30310      0.61342  57.84s\n",
      "     11       \u001b[36m0.42418\u001b[0m       1.52048      0.27898      0.61665  3112.34s\n",
      "     12       \u001b[36m0.40101\u001b[0m       1.54074      0.26027      0.61592  58.26s\n",
      "     13       \u001b[36m0.38484\u001b[0m       1.54826      0.24856      0.61707  57.56s\n",
      "     14       \u001b[36m0.37223\u001b[0m       1.56778      0.23743      0.61505  57.33s\n",
      "     15       \u001b[36m0.35552\u001b[0m       1.59059      0.22352      0.62050  57.51s\n",
      "     16       0.35931       1.59815      0.22483      0.61495  57.45s\n",
      "     17       \u001b[36m0.34403\u001b[0m       1.59739      0.21537      0.61577  57.44s\n",
      "     18       \u001b[36m0.33871\u001b[0m       1.61623      0.20957      0.61872  57.36s\n",
      "     19       \u001b[36m0.32860\u001b[0m       1.63652      0.20079      0.61267  57.44s\n",
      "     20       \u001b[36m0.32020\u001b[0m       1.64478      0.19468      0.61530  57.34s\n",
      "     21       \u001b[36m0.30981\u001b[0m       1.66642      0.18591      0.61490  57.35s\n",
      "     22       \u001b[36m0.30594\u001b[0m       1.68249      0.18184      0.61538  57.33s\n",
      "     23       0.30910       1.67823      0.18418      0.61653  57.42s\n",
      "     24       \u001b[36m0.29734\u001b[0m       1.67876      0.17712      0.61410  57.28s\n",
      "     25       \u001b[36m0.29327\u001b[0m       1.68484      0.17406      0.61498  58.52s\n",
      "     26       \u001b[36m0.29136\u001b[0m       1.70644      0.17074      0.61439  57.40s\n",
      "     27       \u001b[36m0.28839\u001b[0m       1.70469      0.16917      0.61686  57.57s\n",
      "     28       \u001b[36m0.28181\u001b[0m       1.70961      0.16484      0.61295  57.65s\n",
      "     29       \u001b[36m0.28111\u001b[0m       1.73627      0.16191      0.61386  57.31s\n",
      "     30       \u001b[36m0.27087\u001b[0m       1.72319      0.15719      0.61785  57.68s\n",
      "     31       \u001b[36m0.26810\u001b[0m       1.74375      0.15375      0.61700  57.56s\n",
      "     32       \u001b[36m0.26782\u001b[0m       1.74824      0.15319      0.61167  57.35s\n",
      "     33       \u001b[36m0.26341\u001b[0m       1.75209      0.15034      0.61630  57.55s\n",
      "     34       0.26702       1.74297      0.15320      0.61542  57.30s\n",
      "     35       \u001b[36m0.26272\u001b[0m       1.75032      0.15010      0.61479  57.36s\n",
      "     36       \u001b[36m0.25945\u001b[0m       1.77897      0.14584      0.61695  57.29s\n",
      "     37       \u001b[36m0.25349\u001b[0m       1.75497      0.14444      0.61590  57.78s\n",
      "     38       \u001b[36m0.24816\u001b[0m       1.76503      0.14060      0.61620  57.28s\n",
      "     39       0.24890       1.78077      0.13977      0.61431  57.55s\n",
      "     40       \u001b[36m0.24796\u001b[0m       1.79527      0.13812      0.61491  57.35s\n",
      "     41       \u001b[36m0.24653\u001b[0m       1.77623      0.13880      0.61545  57.86s\n",
      "     42       \u001b[36m0.24567\u001b[0m       1.80064      0.13644      0.61318  57.76s\n",
      "     43       \u001b[36m0.23971\u001b[0m       1.79886      0.13326      0.61693  57.47s\n",
      "     44       0.23976       1.81951      0.13177      0.61412  57.26s\n",
      "     45       \u001b[36m0.23407\u001b[0m       1.82629      0.12817      0.61422  57.40s\n",
      "     46       0.23824       1.83137      0.13009      0.61557  57.30s\n",
      "     47       \u001b[36m0.23110\u001b[0m       1.81497      0.12733      0.61274  57.57s\n",
      "     48       \u001b[36m0.22405\u001b[0m       1.83223      0.12228      0.61314  57.52s\n",
      "     49       0.22665       1.84458      0.12287      0.61662  57.61s\n",
      "     50       0.22603       1.86026      0.12150      0.61274  58.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atulkumar/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:417: Warning: The least populated class in y has only 3 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train.toarray())\n",
    "X_test_std = scaler.transform(X_test.toarray())\n",
    "nn.fit(X_train_std, y_train)\n",
    "y_prob_nn = nn.predict_proba(X_test_std)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_prob_rf = rf.predict_proba(X_test)\n",
    "\n",
    "et.fit(X_train, y_train)\n",
    "y_prob_et = et.predict_proba(X_test)\n",
    "\n",
    "clfxgb.fit(X_train, y_train)\n",
    "y_prob_xgb = clfxgb.predict_proba(X_test)\n",
    "\n",
    "train_data_for2 = sparse.hstack((X_test, y_prob_nn, y_prob_rf, y_prob_et, y_prob_xgb))\n",
    "\n",
    "train_data_y_for2 = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix_test_std = scaler.transform(feature_matrix_test.toarray())\n",
    "\n",
    "test_data_pred_nn = nn.predict_proba(feature_matrix_test_std)\n",
    "test_data_pred_rf = rf.predict_proba(feature_matrix_test)\n",
    "test_data_pred_et = et.predict_proba(feature_matrix_test)\n",
    "test_data_pred_clfxgb = clfxgb.predict_proba(feature_matrix_test)\n",
    "\n",
    "test_data_for2 = sparse.hstack((feature_matrix_test, test_data_pred_nn, test_data_pred_rf, test_data_pred_et, test_data_pred_clfxgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler()\n",
    "train_data_for2_std = scaler2.fit_transform(train_data_for2.toarray())\n",
    "test_data_for2_std = scaler2.transform(test_data_for2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process2(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, prefix, layers, ids):\n",
    "    num_test, num_features = test_data_for2.shape\n",
    "    num_classes = 38\n",
    "    nn2 = NeuralNet(layers=layers,\n",
    "                 objective_loss_function=categorical_crossentropy,\n",
    "                 input_shape=(None, num_features),\n",
    "                 \n",
    "                 dropout1_p=0.15,\n",
    "                 dropout2_p=0.25,\n",
    "                 dropout3_p=0.25,\n",
    "                 \n",
    "                 hidden1_num_units=1000,\n",
    "                 hidden2_num_units=500,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=adagrad,     #nesterov_momentum,  \n",
    "                 update_learning_rate=theano.shared(np.float32(0.01)),\n",
    "                 #update_momentum=0.04,\n",
    "                 \n",
    "                 train_split=TrainSplit(eval_size=0.2),\n",
    "                 verbose=1,\n",
    "                 max_epochs=18)\n",
    "\n",
    "    clfxgb2 = XGBoostClassifier(**params)\n",
    "\n",
    "    pred1 = np.zeros((num_test, num_classes)).astype(np.float32)\n",
    "    pred2 = np.zeros((num_test, num_classes)).astype(np.float32)\n",
    "\n",
    "    for i in range(30):\n",
    "        clfxgb2.fit(train_data_for2, train_data_y_for2)\n",
    "        pred1 += clfxgb2.predict_proba(test_data_for2)\n",
    "        nn2.fit(train_data_for2_std, train_data_y_for2)\n",
    "        pred2 += nn2.predict_proba(test_data_for2_std)\n",
    "\n",
    "    pred11 = pred1/30\n",
    "    pred21 = pred2/30\n",
    "    pred = (pred11 + pred21)/2\n",
    "    \n",
    "    make_submission_ensemble(pred, ids, encoder, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 4799538 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input       4279\n",
      "  1  dropout1    4279\n",
      "  2  hidden1     1000\n",
      "  3  dropout2    1000\n",
      "  4  hidden2      500\n",
      "  5  dropout3     500\n",
      "  6  output        38\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m1.52741\u001b[0m       \u001b[32m0.97990\u001b[0m      1.55874      0.71041  63.76s\n",
      "      2       \u001b[36m0.74181\u001b[0m       \u001b[32m0.94257\u001b[0m      0.78700      0.72002  59.19s\n",
      "      3       \u001b[36m0.55380\u001b[0m       0.96154      0.57595      0.72095  59.32s\n",
      "      4       \u001b[36m0.43594\u001b[0m       0.99964      0.43610      0.72137  59.95s\n",
      "      5       \u001b[36m0.36872\u001b[0m       1.03500      0.35625      0.72320  59.07s\n",
      "      6       \u001b[36m0.32435\u001b[0m       1.06691      0.30401      0.72334  59.25s\n",
      "      7       \u001b[36m0.28617\u001b[0m       1.10231      0.25961      0.72446  59.44s\n",
      "      8       \u001b[36m0.26778\u001b[0m       1.12757      0.23748      0.72330  59.31s\n",
      "      9       \u001b[36m0.24890\u001b[0m       1.15140      0.21617      0.72370  59.32s\n",
      "     10       \u001b[36m0.23046\u001b[0m       1.15756      0.19909      0.72593  59.37s\n",
      "     11       \u001b[36m0.22182\u001b[0m       1.18438      0.18729      0.72284  59.05s\n",
      "     12       \u001b[36m0.20625\u001b[0m       1.18843      0.17355      0.72456  59.44s\n",
      "     13       \u001b[36m0.19704\u001b[0m       1.21230      0.16253      0.72736  59.31s\n",
      "     14       \u001b[36m0.19025\u001b[0m       1.22794      0.15493      0.72381  59.47s\n",
      "     15       \u001b[36m0.17972\u001b[0m       1.24304      0.14458      0.72272  59.49s\n",
      "     16       \u001b[36m0.17436\u001b[0m       1.24748      0.13977      0.72412  59.20s\n",
      "     17       \u001b[36m0.16864\u001b[0m       1.26884      0.13291      0.72066  59.88s\n",
      "     18       \u001b[36m0.16839\u001b[0m       1.27562      0.13200      0.72493  59.27s\n",
      "     19       \u001b[36m0.16181\u001b[0m       1.30813      0.12370      0.72421  64.18s\n",
      "     20       \u001b[36m0.15841\u001b[0m       1.30139      0.12172      0.72379  59.72s\n",
      "     21       \u001b[36m0.15496\u001b[0m       1.31369      0.11795      0.72077  60.12s\n",
      "     22       \u001b[36m0.15121\u001b[0m       1.30629      0.11576      0.72422  60.29s\n",
      "     23       \u001b[36m0.14850\u001b[0m       1.32847      0.11178      0.72305  59.67s\n",
      "     24       \u001b[36m0.13911\u001b[0m       1.35083      0.10298      0.72210  59.80s\n",
      "     25       0.14226       1.35903      0.10468      0.72253  59.73s\n",
      "     26       \u001b[36m0.13770\u001b[0m       1.36359      0.10099      0.72288  59.28s\n",
      "     27       \u001b[36m0.13350\u001b[0m       1.35329      0.09865      0.72452  59.99s\n",
      "     28       0.13605       1.36453      0.09970      0.72473  60.06s\n",
      "     29       \u001b[36m0.13168\u001b[0m       1.38374      0.09516      0.72149  59.35s\n",
      "     30       \u001b[36m0.12780\u001b[0m       1.37521      0.09293      0.72149  59.99s\n",
      "     31       \u001b[36m0.12679\u001b[0m       1.38232      0.09172      0.72382  59.78s\n",
      "     32       0.12841       1.38801      0.09251      0.72343  59.74s\n",
      "     33       \u001b[36m0.12607\u001b[0m       1.40092      0.08999      0.72342  59.50s\n",
      "     34       \u001b[36m0.11946\u001b[0m       1.40958      0.08475      0.72393  60.37s\n",
      "     35       0.11948       1.42892      0.08362      0.72288  60.59s\n",
      "     36       \u001b[36m0.11701\u001b[0m       1.43225      0.08169      0.72278  60.01s\n",
      "     37       0.11927       1.43558      0.08308      0.72254  64.45s\n",
      "     38       \u001b[36m0.11464\u001b[0m       1.45703      0.07868      0.72109  63.87s\n",
      "     39       0.11665       1.43736      0.08116      0.72288  63.68s\n",
      "     40       0.11712       1.44873      0.08084      0.72184  61.65s\n",
      "     41       \u001b[36m0.11111\u001b[0m       1.44823      0.07672      0.72141  62.59s\n",
      "     42       0.11270       1.43957      0.07829      0.72071  61.82s\n",
      "     43       \u001b[36m0.10940\u001b[0m       1.45394      0.07524      0.71986  62.36s\n",
      "     44       0.11237       1.46722      0.07659      0.72047  64.58s\n",
      "     45       \u001b[36m0.10821\u001b[0m       1.46158      0.07404      0.72277  61.72s\n",
      "     46       \u001b[36m0.10631\u001b[0m       1.47576      0.07204      0.72223  62.26s\n",
      "     47       \u001b[36m0.10250\u001b[0m       1.51028      0.06787      0.72182  68.30s\n",
      "     48       \u001b[36m0.10121\u001b[0m       1.50586      0.06721      0.72265  62.04s\n",
      "     49       0.10568       1.47805      0.07150      0.72059  61.53s\n",
      "     50       0.10179       1.49547      0.06806      0.72058  62.35s\n",
      "     51       0.10138       1.50146      0.06752      0.72234  61.71s\n",
      "     52       0.10312       1.51014      0.06828      0.72106  61.71s\n",
      "     53       \u001b[36m0.09919\u001b[0m       1.50789      0.06578      0.72057  62.21s\n",
      "     54       0.10059       1.49689      0.06720      0.72025  61.74s\n",
      "     55       \u001b[36m0.09650\u001b[0m       1.52054      0.06346      0.72171  66.30s\n",
      "     56       \u001b[36m0.09618\u001b[0m       1.52811      0.06294      0.72192  61.72s\n",
      "     57       \u001b[36m0.09513\u001b[0m       1.54350      0.06163      0.72106  62.42s\n",
      "     58       0.09756       1.53017      0.06376      0.71964  64.07s\n",
      "     59       \u001b[36m0.09463\u001b[0m       1.54011      0.06144      0.71920  65.17s\n",
      "     60       \u001b[36m0.09301\u001b[0m       1.53681      0.06052      0.72077  68.89s\n",
      "     61       \u001b[36m0.09267\u001b[0m       1.55807      0.05948      0.72098  60.99s\n",
      "     62       0.09643       1.54301      0.06249      0.72153  61.41s\n",
      "     63       0.09307       1.55012      0.06004      0.72368  68.26s\n",
      "     64       \u001b[36m0.09030\u001b[0m       1.56452      0.05772      0.72252  66.68s\n",
      "     65       0.09251       1.55021      0.05967      0.72420  72.55s\n",
      "     66       \u001b[36m0.09023\u001b[0m       1.55832      0.05790      0.72110  70.20s\n",
      "     67       0.09168       1.55272      0.05904      0.72213  67.31s\n",
      "     68       0.09090       1.56960      0.05792      0.71995  67.66s\n",
      "     69       \u001b[36m0.08886\u001b[0m       1.56568      0.05676      0.72109  79.55s\n",
      "     70       0.08963       1.57269      0.05699      0.72057  62.16s\n",
      "     71       \u001b[36m0.08508\u001b[0m       1.58173      0.05379      0.71994  60.98s\n",
      "     72       0.08615       1.58944      0.05420      0.72058  76.93s\n",
      "     73       0.08867       1.57289      0.05637      0.71893  78.91s\n",
      "     74       0.08588       1.58564      0.05416      0.71869  69.34s\n",
      "     75       \u001b[36m0.08473\u001b[0m       1.59793      0.05303      0.71927  65.83s\n",
      "     76       \u001b[36m0.08339\u001b[0m       1.60417      0.05198      0.72042  68.24s\n",
      "     77       0.08706       1.60516      0.05424      0.72107  65.07s\n",
      "     78       0.08565       1.61426      0.05306      0.72118  65.04s\n",
      "     79       0.08638       1.58968      0.05434      0.72073  64.74s\n",
      "     80       0.08490       1.58921      0.05342      0.72037  68.22s\n",
      "     81       0.08542       1.58592      0.05386      0.71973  66.22s\n",
      "     82       0.08598       1.59090      0.05405      0.72222  78.20s\n",
      "     83       \u001b[36m0.08304\u001b[0m       1.60978      0.05158      0.71961  168.39s\n",
      "     84       \u001b[36m0.08164\u001b[0m       1.60822      0.05076      0.72180  66.86s\n",
      "     85       0.08294       1.61034      0.05151      0.72137  80.89s\n",
      "     86       \u001b[36m0.08154\u001b[0m       1.62473      0.05019      0.72149  63.52s\n",
      "     87       0.08431       1.62799      0.05179      0.71918  61.91s\n",
      "     88       0.08193       1.61646      0.05068      0.71774  63.24s\n",
      "     89       \u001b[36m0.07795\u001b[0m       1.63326      0.04773      0.71802  63.45s\n",
      "     90       0.08000       1.63886      0.04881      0.71732  61.08s\n",
      "     91       0.08227       1.61775      0.05085      0.72085  63.94s\n",
      "     92       0.08172       1.63253      0.05005      0.72003  60.84s\n",
      "     93       0.08227       1.62719      0.05056      0.71972  60.93s\n",
      "     94       0.08112       1.61555      0.05021      0.71939  62.13s\n",
      "     95       0.07860       1.62402      0.04840      0.72013  60.71s\n",
      "     96       0.07965       1.64860      0.04831      0.72138  62.37s\n",
      "     97       \u001b[36m0.07752\u001b[0m       1.64939      0.04700      0.72076  62.87s\n",
      "     98       0.07965       1.63854      0.04861      0.71942  61.07s\n",
      "     99       0.08057       1.64803      0.04889      0.72004  61.28s\n",
      "    100       0.07928       1.63587      0.04846      0.71982  61.95s\n",
      "    101       0.07993       1.62670      0.04913      0.72102  62.97s\n",
      "    102       0.07969       1.63921      0.04861      0.71964  62.67s\n",
      "    103       \u001b[36m0.07739\u001b[0m       1.64961      0.04691      0.71807  62.38s\n",
      "    104       \u001b[36m0.07623\u001b[0m       1.65411      0.04609      0.71976  74.80s\n",
      "    105       0.07817       1.64618      0.04749      0.72025  64.88s\n",
      "    106       \u001b[36m0.07492\u001b[0m       1.64862      0.04545      0.71963  65.11s\n",
      "    107       0.07510       1.66018      0.04523      0.72076  62.09s\n",
      "    108       0.07773       1.67258      0.04648      0.72141  62.55s\n",
      "    109       0.07524       1.67395      0.04495      0.71921  73.98s\n",
      "    110       0.07680       1.66123      0.04623      0.72045  71.23s\n",
      "    111       0.07588       1.67103      0.04541      0.71982  63.67s\n",
      "    112       0.07605       1.69803      0.04479      0.71909  68.23s\n",
      "    113       \u001b[36m0.07489\u001b[0m       1.66609      0.04495      0.71932  66.91s\n",
      "    114       \u001b[36m0.07460\u001b[0m       1.68702      0.04422      0.71943  61.26s\n",
      "    115       \u001b[36m0.07321\u001b[0m       1.67876      0.04361      0.72004  64.79s\n",
      "    116       \u001b[36m0.07275\u001b[0m       1.69460      0.04293      0.71966  62.55s\n",
      "    117       0.07422       1.68409      0.04407      0.71985  63.13s\n",
      "    118       0.07382       1.68033      0.04393      0.71930  62.05s\n",
      "    119       0.07333       1.68022      0.04364      0.71752  69.20s\n",
      "    120       0.07296       1.68276      0.04336      0.71918  65.90s\n",
      "    121       0.07459       1.68759      0.04420      0.71838  63.37s\n",
      "    122       0.07275       1.69427      0.04294      0.71869  65.12s\n",
      "    123       \u001b[36m0.07067\u001b[0m       1.69633      0.04166      0.71981  65.40s\n",
      "    124       \u001b[36m0.06879\u001b[0m       1.71198      0.04018      0.71857  64.03s\n",
      "    125       0.07150       1.71406      0.04172      0.71786  61.44s\n",
      "    126       0.07202       1.71997      0.04188      0.71795  63.54s\n",
      "    127       0.07279       1.70162      0.04277      0.71680  66.09s\n",
      "    128       0.06978       1.72003      0.04057      0.71719  63.07s\n",
      "    129       0.07066       1.72419      0.04098      0.71734  62.85s\n",
      "    130       0.07059       1.71235      0.04122      0.71835  67.42s\n",
      "    131       0.07163       1.71643      0.04173      0.71857  68.41s\n",
      "    132       0.06964       1.72255      0.04043      0.71857  62.21s\n",
      "    133       0.06903       1.71643      0.04022      0.71836  62.01s\n",
      "    134       \u001b[36m0.06836\u001b[0m       1.72056      0.03973      0.71868  62.68s\n",
      "    135       0.06908       1.74911      0.03950      0.71844  61.14s\n",
      "    136       0.06939       1.73968      0.03989      0.71856  63.78s\n",
      "    137       0.07122       1.72524      0.04128      0.71680  63.94s\n",
      "    138       0.06853       1.73696      0.03945      0.71868  62.70s\n",
      "    139       0.06839       1.73672      0.03938      0.72098  61.64s\n",
      "    140       0.06854       1.73818      0.03943      0.71907  63.26s\n",
      "    141       0.07060       1.72572      0.04091      0.71791  62.11s\n",
      "    142       \u001b[36m0.06700\u001b[0m       1.74139      0.03848      0.71991  62.67s\n",
      "    143       0.06794       1.76048      0.03859      0.71845  68.13s\n",
      "    144       0.06857       1.75422      0.03909      0.71888  63.68s\n",
      "    145       0.06827       1.75785      0.03884      0.71951  63.18s\n",
      "    146       \u001b[36m0.06543\u001b[0m       1.75352      0.03731      0.72009  62.75s\n",
      "    147       0.06777       1.74718      0.03879      0.71820  64.42s\n",
      "    148       0.06924       1.73049      0.04001      0.71883  62.42s\n",
      "    149       \u001b[36m0.06349\u001b[0m       1.76218      0.03603      0.71802  66.48s\n",
      "    150       0.07072       1.74594      0.04051      0.72002  62.43s\n",
      "    151       0.06392       1.76045      0.03631      0.71884  62.27s\n",
      "    152       0.06878       1.75914      0.03910      0.71854  63.86s\n",
      "    153       0.06879       1.79434      0.03834      0.71695  64.03s\n",
      "    154       0.06766       1.76059      0.03843      0.71780  63.01s\n",
      "    155       0.06613       1.77493      0.03726      0.71927  63.38s\n",
      "    156       0.06514       1.76636      0.03688      0.72020  60.81s\n",
      "    157       0.06703       1.76680      0.03794      0.72020  61.17s\n",
      "    158       0.06463       1.77377      0.03644      0.71886  60.91s\n",
      "    159       0.06594       1.76509      0.03736      0.71908  62.60s\n",
      "    160       0.06950       1.76060      0.03947      0.71999  61.55s\n",
      "    161       0.06665       1.77398      0.03757      0.71895  63.51s\n",
      "    162       0.06493       1.77783      0.03652      0.71868  62.86s\n",
      "    163       0.06539       1.78608      0.03661      0.71697  66.47s\n",
      "    164       0.06352       1.79634      0.03536      0.71780  68.61s\n",
      "    165       0.06419       1.79141      0.03583      0.71792  64.62s\n",
      "    166       0.06491       1.77680      0.03653      0.71873  63.19s\n",
      "    167       0.06435       1.78829      0.03598      0.71877  63.48s\n",
      "    168       0.06564       1.79373      0.03659      0.71887  63.92s\n",
      "    169       0.06626       1.78272      0.03717      0.71866  62.59s\n",
      "    170       0.06427       1.78041      0.03610      0.71991  60.82s\n",
      "    171       0.06385       1.78160      0.03584      0.72129  65.87s\n",
      "    172       \u001b[36m0.06095\u001b[0m       1.79669      0.03392      0.72109  69.14s\n",
      "    173       0.06552       1.77762      0.03686      0.71985  75.21s\n",
      "    174       0.06447       1.79144      0.03599      0.71966  65.51s\n",
      "    175       0.06407       1.79439      0.03570      0.71924  59.78s\n",
      "    176       0.06430       1.77689      0.03619      0.71863  59.90s\n",
      "    177       0.06337       1.79070      0.03539      0.71856  59.67s\n",
      "    178       0.06118       1.79411      0.03410      0.71723  59.50s\n",
      "    179       0.06234       1.80069      0.03462      0.71877  59.89s\n",
      "    180       0.06270       1.80165      0.03480      0.71827  59.93s\n",
      "    181       0.06146       1.81203      0.03392      0.71713  65.20s\n",
      "    182       0.06289       1.82291      0.03450      0.71796  60.83s\n",
      "    183       \u001b[36m0.05978\u001b[0m       1.82026      0.03284      0.71814  60.87s\n",
      "    184       0.06207       1.81348      0.03423      0.71869  65.53s\n",
      "    185       0.06215       1.80620      0.03441      0.72006  61.90s\n",
      "    186       0.06406       1.80179      0.03555      0.71902  61.64s\n",
      "    187       0.06151       1.83095      0.03360      0.71848  61.62s\n",
      "    188       0.06272       1.81243      0.03461      0.71970  61.37s\n",
      "    189       0.06005       1.81602      0.03307      0.71783  61.53s\n",
      "    190       0.06215       1.80978      0.03434      0.71772  61.56s\n",
      "    191       0.06273       1.81956      0.03448      0.71680  61.63s\n",
      "    192       0.06189       1.80275      0.03433      0.71710  61.43s\n",
      "    193       \u001b[36m0.05942\u001b[0m       1.82500      0.03256      0.71711  61.68s\n",
      "    194       0.06264       1.81991      0.03442      0.71859  61.58s\n",
      "    195       0.06438       1.83649      0.03505      0.71744  61.70s\n",
      "    196       0.06326       1.83657      0.03445      0.71689  61.70s\n",
      "    197       0.06197       1.81659      0.03412      0.71775  62.77s\n",
      "    198       0.06243       1.83315      0.03406      0.71607  63.84s\n",
      "    199       0.06342       1.81834      0.03488      0.71702  66.94s\n",
      "    200       0.06051       1.82419      0.03317      0.71702  66.45s\n",
      "    201       0.06177       1.82290      0.03388      0.71700  64.07s\n",
      "    202       \u001b[36m0.05937\u001b[0m       1.82409      0.03255      0.71658  62.36s\n",
      "    203       \u001b[36m0.05873\u001b[0m       1.83414      0.03202      0.71710  65.70s\n",
      "    204       0.06044       1.84127      0.03282      0.71573  62.89s\n",
      "    205       0.05882       1.83666      0.03203      0.71613  65.15s\n",
      "    206       \u001b[36m0.05795\u001b[0m       1.83200      0.03163      0.71700  63.67s\n",
      "    207       0.05999       1.85790      0.03229      0.71709  62.17s\n",
      "    208       0.06173       1.84953      0.03338      0.71991  65.02s\n",
      "    209       0.06109       1.83489      0.03329      0.71732  62.30s\n",
      "    210       0.05833       1.83130      0.03185      0.71827  64.17s\n",
      "    211       0.06043       1.83266      0.03297      0.71929  63.80s\n",
      "    212       0.06079       1.84003      0.03303      0.71816  68.53s\n",
      "    213       0.05834       1.83886      0.03173      0.71749  73.33s\n",
      "    214       0.05995       1.84642      0.03247      0.71819  95.90s\n",
      "    215       0.05950       1.85629      0.03205      0.71793  98.82s\n",
      "    216       0.05953       1.84655      0.03224      0.71952  61.35s\n",
      "    217       \u001b[36m0.05579\u001b[0m       1.85377      0.03010      0.71788  64.27s\n",
      "    218       0.05965       1.86679      0.03196      0.71857  59.98s\n",
      "    219       0.05944       1.86152      0.03193      0.71950  60.21s\n",
      "    220       0.05893       1.84634      0.03191      0.72025  60.01s\n",
      "    221       0.05980       1.85342      0.03226      0.71866  60.05s\n",
      "    222       0.05908       1.84585      0.03200      0.71805  60.16s\n",
      "    223       0.05858       1.86309      0.03144      0.71908  60.21s\n",
      "    224       0.05874       1.85379      0.03169      0.72064  60.25s\n",
      "    225       \u001b[36m0.05569\u001b[0m       1.85192      0.03007      0.72110  60.40s\n",
      "    226       0.05719       1.85285      0.03086      0.71930  60.20s\n",
      "    227       0.05865       1.87221      0.03133      0.72000  60.13s\n",
      "    228       0.05871       1.85972      0.03157      0.72137  60.08s\n",
      "    229       0.05868       1.85044      0.03171      0.72055  60.39s\n",
      "    230       0.05817       1.84836      0.03147      0.71954  60.31s\n",
      "    231       0.05726       1.86183      0.03076      0.71850  60.25s\n",
      "    232       0.05630       1.86334      0.03022      0.71779  60.52s\n",
      "    233       0.05891       1.85897      0.03169      0.71692  60.45s\n",
      "    234       0.05935       1.86234      0.03187      0.71839  60.38s\n",
      "    235       0.05789       1.86648      0.03102      0.71796  62.56s\n",
      "    236       0.05931       1.85990      0.03189      0.71818  60.29s\n",
      "    237       0.05651       1.86200      0.03035      0.71817  59.81s\n",
      "    238       0.05740       1.87250      0.03066      0.71951  59.78s\n",
      "    239       0.06008       1.84760      0.03252      0.71784  59.86s\n",
      "    240       0.05663       1.87392      0.03022      0.71917  59.65s\n",
      "    241       0.05703       1.86192      0.03063      0.71875  60.20s\n",
      "    242       \u001b[36m0.05492\u001b[0m       1.86122      0.02951      0.71805  60.12s\n",
      "    243       0.05662       1.86503      0.03036      0.71911  60.67s\n",
      "    244       \u001b[36m0.05468\u001b[0m       1.85117      0.02954      0.71952  60.94s\n",
      "    245       0.05717       1.87008      0.03057      0.71969  60.11s\n",
      "    246       0.06021       1.86583      0.03227      0.72000  60.00s\n",
      "    247       0.05767       1.87659      0.03073      0.72002  60.14s\n",
      "    248       0.05711       1.86805      0.03057      0.72003  60.03s\n",
      "    249       0.05695       1.87513      0.03037      0.71878  61.09s\n",
      "    250       0.05724       1.86372      0.03071      0.71909  60.02s\n",
      "    251       0.05660       1.86585      0.03034      0.71835  59.89s\n",
      "    252       0.05682       1.86295      0.03050      0.71719  59.91s\n",
      "    253       0.05766       1.85260      0.03112      0.71825  63.01s\n",
      "    254       0.05623       1.85423      0.03032      0.71950  60.44s\n",
      "    255       0.05584       1.86740      0.02990      0.71908  60.62s\n",
      "    256       0.05676       1.87132      0.03033      0.71948  60.14s\n",
      "    257       0.05945       1.87222      0.03175      0.71961  61.16s\n",
      "    258       0.05610       1.86810      0.03003      0.71957  60.08s\n",
      "    259       0.05732       1.88113      0.03047      0.71908  60.35s\n",
      "    260       0.05615       1.86440      0.03012      0.71981  60.28s\n",
      "    261       0.05608       1.88065      0.02982      0.71917  59.97s\n",
      "    262       \u001b[36m0.05356\u001b[0m       1.88678      0.02839      0.72042  59.99s\n",
      "    263       0.05464       1.89187      0.02888      0.72011  59.74s\n",
      "    264       0.05608       1.90027      0.02951      0.71884  59.98s\n",
      "    265       0.05460       1.89957      0.02874      0.71951  60.32s\n",
      "    266       0.05397       1.90564      0.02832      0.71896  59.98s\n",
      "    267       0.05670       1.91129      0.02967      0.71899  59.88s\n",
      "    268       0.05691       1.90146      0.02993      0.72012  60.22s\n",
      "    269       0.05696       1.89689      0.03003      0.72033  59.88s\n",
      "    270       0.05447       1.88077      0.02896      0.71961  60.06s\n",
      "    271       0.05473       1.89033      0.02895      0.71839  65.15s\n",
      "    272       0.05544       1.90579      0.02909      0.72012  61.30s\n",
      "    273       0.05576       1.90250      0.02931      0.72098  67.62s\n",
      "    274       0.05534       1.90630      0.02903      0.71932  61.67s\n",
      "    275       0.05724       1.88923      0.03030      0.72004  62.61s\n",
      "    276       0.05698       1.88744      0.03019      0.72013  64.11s\n",
      "    277       0.05372       1.88171      0.02855      0.71941  62.19s\n",
      "    278       0.05639       1.88200      0.02997      0.71890  67.24s\n",
      "    279       0.05523       1.87768      0.02942      0.71973  66.31s\n",
      "    280       \u001b[36m0.05189\u001b[0m       1.88491      0.02753      0.71897  4377.16s\n",
      "    281       0.05336       1.89280      0.02819      0.72034  73.96s\n",
      "    282       0.05240       1.91336      0.02739      0.71836  64.53s\n",
      "    283       0.05418       1.91254      0.02833      0.71659  61.54s\n",
      "    284       0.05367       1.90545      0.02817      0.71668  62.43s\n",
      "    285       0.05508       1.89395      0.02908      0.71745  63.23s\n",
      "    286       0.05807       1.91448      0.03033      0.71691  61.48s\n",
      "    287       0.05387       1.91277      0.02816      0.71689  68.36s\n",
      "    288       0.05305       1.91316      0.02773      0.71722  69.85s\n",
      "    289       0.05511       1.91255      0.02881      0.71743  63.06s\n",
      "    290       0.05412       1.91318      0.02829      0.71784  59.81s\n",
      "    291       0.05399       1.92722      0.02802      0.71868  60.04s\n",
      "    292       0.05351       1.91184      0.02799      0.71795  60.94s\n",
      "    293       0.05569       1.90796      0.02919      0.71899  59.68s\n",
      "    294       0.05429       1.93634      0.02804      0.71879  60.06s\n",
      "    295       0.05282       1.92458      0.02745      0.71932  59.51s\n",
      "    296       0.05564       1.92818      0.02886      0.71984  59.67s\n",
      "    297       0.05596       1.92656      0.02905      0.71994  59.43s\n",
      "    298       0.05319       1.91962      0.02771      0.72171  59.54s\n",
      "    299       0.05469       1.91392      0.02858      0.71942  60.51s\n",
      "    300       0.05425       1.90980      0.02840      0.72046  60.34s\n",
      "    301       0.05301       1.92525      0.02753      0.72036  59.82s\n",
      "    302       \u001b[36m0.05092\u001b[0m       1.93765      0.02628      0.72057  59.79s\n",
      "    303       0.05264       1.92475      0.02735      0.71848  60.30s\n",
      "    304       0.05233       1.92096      0.02724      0.71732  59.40s\n",
      "    305       \u001b[36m0.05080\u001b[0m       1.93323      0.02628      0.71774  59.51s\n",
      "    306       0.05123       1.92912      0.02656      0.71710  59.57s\n",
      "    307       0.05321       1.92670      0.02761      0.71671  62.89s\n",
      "    308       0.05095       1.91894      0.02655      0.71774  59.78s\n",
      "    309       0.05319       1.91508      0.02777      0.71869  59.52s\n",
      "    310       0.05158       1.91682      0.02691      0.71903  59.48s\n",
      "    311       0.05323       1.91685      0.02777      0.71943  59.76s\n",
      "    312       0.05350       1.92890      0.02773      0.71826  60.19s\n",
      "    313       0.05242       1.92031      0.02730      0.72098  59.74s\n",
      "    314       0.05224       1.91555      0.02727      0.71868  59.74s\n",
      "    315       0.05120       1.94551      0.02632      0.71774  59.75s\n",
      "    316       0.05271       1.94556      0.02709      0.71838  59.76s\n",
      "    317       0.05164       1.95224      0.02645      0.71725  60.50s\n",
      "    318       0.05199       1.95175      0.02664      0.71732  60.12s\n",
      "    319       \u001b[36m0.05004\u001b[0m       1.96747      0.02543      0.71847  59.82s\n",
      "    320       0.05183       1.96437      0.02639      0.71743  59.76s\n",
      "    321       0.05354       1.97076      0.02717      0.71878  59.80s\n",
      "    322       0.05235       1.95573      0.02677      0.71929  59.63s\n",
      "    323       0.05029       1.97224      0.02550      0.71877  60.25s\n",
      "    324       0.05297       1.95149      0.02714      0.71836  59.81s\n",
      "    325       0.05094       1.96397      0.02594      0.71807  68.32s\n",
      "    326       0.05225       1.96436      0.02660      0.71777  62.18s\n",
      "    327       0.05249       1.96615      0.02669      0.71860  60.22s\n",
      "    328       0.05550       1.94171      0.02859      0.71954  60.18s\n",
      "    329       0.05124       1.94861      0.02630      0.71933  60.75s\n",
      "    330       0.05035       1.95460      0.02576      0.71860  60.30s\n",
      "    331       0.05238       1.95264      0.02683      0.71879  61.99s\n",
      "    332       0.05011       1.96391      0.02551      0.71818  63.83s\n",
      "    333       0.05266       1.96177      0.02684      0.71797  62.75s\n",
      "    334       0.05298       1.95923      0.02704      0.71704  60.72s\n",
      "    335       0.05124       1.95454      0.02621      0.71868  60.80s\n",
      "    336       0.05108       1.94619      0.02625      0.71731  62.01s\n",
      "    337       0.05180       1.93032      0.02684      0.71735  61.22s\n",
      "    338       0.05221       1.93950      0.02692      0.71657  60.06s\n",
      "    339       0.05128       1.94600      0.02635      0.71777  60.63s\n",
      "    340       0.05294       1.94346      0.02724      0.71796  60.08s\n",
      "    341       0.05365       1.93610      0.02771      0.71692  60.01s\n",
      "    342       0.05233       1.96056      0.02669      0.71691  59.90s\n",
      "    343       0.05202       1.96877      0.02642      0.71729  62.86s\n",
      "    344       0.05237       1.97154      0.02656      0.71533  60.14s\n",
      "    345       0.05293       1.95366      0.02709      0.71668  59.90s\n",
      "    346       0.05022       1.96453      0.02556      0.71908  59.96s\n",
      "    347       \u001b[36m0.04969\u001b[0m       1.96942      0.02523      0.71848  60.16s\n",
      "    348       0.05061       1.96842      0.02571      0.71836  60.39s\n",
      "    349       0.05026       1.96443      0.02559      0.71772  59.99s\n",
      "    350       0.05004       1.96260      0.02550      0.71756  60.29s\n",
      "    351       0.05301       1.96299      0.02701      0.71784  60.02s\n",
      "    352       0.05005       1.96882      0.02542      0.71753  60.27s\n",
      "    353       0.05140       1.97676      0.02600      0.71513  60.10s\n",
      "    354       0.05048       1.97778      0.02552      0.71491  60.28s\n",
      "    355       \u001b[36m0.04866\u001b[0m       1.96255      0.02479      0.71557  59.85s\n",
      "    356       \u001b[36m0.04851\u001b[0m       1.96633      0.02467      0.71684  60.20s\n",
      "    357       0.05050       1.97444      0.02558      0.71527  59.90s\n",
      "    358       0.04974       1.97356      0.02520      0.71610  59.83s\n",
      "    359       0.05126       1.97787      0.02592      0.71807  60.14s\n",
      "    360       0.05203       1.98427      0.02622      0.71536  60.15s\n",
      "    361       0.05136       1.98785      0.02584      0.71713  61.89s\n",
      "    362       0.04891       1.98847      0.02460      0.71668  60.07s\n",
      "    363       0.05277       1.99447      0.02646      0.71637  59.57s\n",
      "    364       0.05041       1.99528      0.02526      0.71649  59.66s\n",
      "    365       0.05144       1.98849      0.02587      0.71659  59.92s\n",
      "    366       0.05083       1.99552      0.02547      0.71629  60.65s\n",
      "    367       0.05174       1.99117      0.02599      0.71638  59.93s\n",
      "    368       0.05079       1.99121      0.02551      0.71482  59.77s\n",
      "    369       0.04876       1.98752      0.02453      0.71524  59.68s\n",
      "    370       0.04852       1.98453      0.02445      0.71661  59.90s\n",
      "    371       0.04906       1.99602      0.02458      0.71702  59.71s\n",
      "    372       0.04904       1.98911      0.02466      0.71442  59.99s\n",
      "    373       0.04947       1.98118      0.02497      0.71607  59.83s\n",
      "    374       0.04903       1.98681      0.02468      0.71575  60.28s\n",
      "    375       0.05065       1.98832      0.02548      0.71584  59.76s\n",
      "    376       0.04875       2.00344      0.02433      0.71667  59.92s\n",
      "    377       0.05015       1.99634      0.02512      0.71613  59.90s\n",
      "    378       \u001b[36m0.04760\u001b[0m       2.00251      0.02377      0.71503  59.58s\n",
      "    379       0.05110       1.99511      0.02561      0.71491  64.76s\n",
      "    380       0.04954       2.01204      0.02462      0.71604  60.00s\n",
      "    381       0.04892       2.01562      0.02427      0.71720  59.98s\n",
      "    382       0.05040       2.01896      0.02496      0.71586  59.77s\n",
      "    383       0.04889       2.01347      0.02428      0.71545  59.85s\n",
      "    384       0.05040       1.99676      0.02524      0.71575  59.62s\n",
      "    385       0.05053       1.98230      0.02549      0.71441  59.85s\n",
      "    386       0.04952       1.99440      0.02483      0.71503  59.78s\n",
      "    387       \u001b[36m0.04710\u001b[0m       2.00469      0.02349      0.71573  59.84s\n",
      "    388       0.05035       2.00043      0.02517      0.71615  59.65s\n",
      "    389       0.04993       1.99507      0.02503      0.71661  60.00s\n",
      "    390       0.04985       1.99250      0.02502      0.71609  59.52s\n",
      "    391       0.04865       2.00064      0.02432      0.71732  59.62s\n",
      "    392       0.04730       2.00290      0.02362      0.71784  60.03s\n",
      "    393       0.04925       1.99659      0.02467      0.71649  59.60s\n",
      "    394       0.04970       1.99972      0.02485      0.71701  59.52s\n",
      "    395       0.05013       1.99545      0.02512      0.71775  59.92s\n",
      "    396       0.05076       1.99878      0.02540      0.71438  59.76s\n",
      "    397       0.04967       2.00232      0.02481      0.71593  61.76s\n",
      "    398       0.04750       2.02442      0.02346      0.71572  59.68s\n",
      "    399       0.05006       2.02893      0.02467      0.71530  59.61s\n",
      "    400       0.05201       2.01095      0.02586      0.71533  59.80s\n",
      "    401       0.04943       2.00427      0.02466      0.71511  59.99s\n",
      "    402       0.04799       2.00463      0.02394      0.71456  60.24s\n",
      "    403       0.05001       1.98880      0.02515      0.71728  59.63s\n",
      "    404       0.04712       2.00143      0.02354      0.71634  59.77s\n",
      "    405       0.04992       2.01575      0.02477      0.71700  67.55s\n",
      "    406       0.04874       2.01407      0.02420      0.71834  60.93s\n",
      "    407       0.04720       2.01536      0.02342      0.71720  59.66s\n",
      "    408       0.04999       2.01585      0.02480      0.71811  59.65s\n",
      "    409       0.04788       2.00802      0.02385      0.71718  59.80s\n",
      "    410       0.04776       2.01180      0.02374      0.71645  59.54s\n",
      "    411       0.04994       1.99232      0.02507      0.71791  59.63s\n",
      "    412       0.04909       2.01262      0.02439      0.71960  59.65s\n",
      "    413       0.04771       2.01450      0.02368      0.71771  59.95s\n",
      "    414       0.04985       2.02046      0.02467      0.71918  59.90s\n",
      "    415       0.04810       2.01806      0.02383      0.71813  63.07s\n",
      "    416       0.04908       2.00956      0.02442      0.71960  60.52s\n",
      "    417       \u001b[36m0.04676\u001b[0m       2.00105      0.02337      0.71825  60.61s\n",
      "    418       0.04781       2.00280      0.02387      0.71845  60.42s\n",
      "    419       0.04701       2.00266      0.02348      0.71863  60.80s\n",
      "    420       \u001b[36m0.04547\u001b[0m       2.01228      0.02260      0.71793  60.57s\n",
      "    421       0.04934       1.99827      0.02469      0.71877  60.41s\n",
      "    422       0.04814       1.99863      0.02408      0.71792  60.25s\n",
      "    423       0.04963       2.00607      0.02474      0.71710  60.46s\n",
      "    424       0.04710       2.00827      0.02345      0.71814  60.27s\n",
      "    425       0.04879       2.00430      0.02434      0.71897  61.22s\n",
      "    426       0.04702       1.99830      0.02353      0.71743  60.51s\n",
      "    427       0.04711       1.99710      0.02359      0.71722  60.31s\n",
      "    428       0.04952       1.99902      0.02477      0.71734  60.61s\n",
      "    429       0.04926       2.01610      0.02443      0.71713  60.58s\n",
      "    430       0.04835       2.02561      0.02387      0.71532  60.35s\n",
      "    431       0.04735       2.01887      0.02345      0.71782  60.34s\n",
      "    432       0.04644       2.02529      0.02293      0.71718  60.18s\n",
      "    433       0.04858       2.01784      0.02408      0.71782  62.08s\n",
      "    434       \u001b[36m0.04517\u001b[0m       2.01247      0.02245      0.71887  59.89s\n",
      "    435       0.04951       1.99745      0.02479      0.71930  60.10s\n",
      "    436       0.04795       2.00094      0.02396      0.71671  59.79s\n",
      "    437       0.04729       2.00341      0.02361      0.71666  59.64s\n",
      "    438       0.04616       2.01593      0.02290      0.71686  59.87s\n",
      "    439       0.04802       2.02339      0.02373      0.71768  60.01s\n",
      "    440       0.04593       2.01952      0.02274      0.71675  60.03s\n",
      "    441       0.04808       2.00768      0.02395      0.71616  59.83s\n",
      "    442       0.04913       1.99610      0.02461      0.71564  59.95s\n",
      "    443       0.04873       2.00485      0.02431      0.71616  60.06s\n",
      "    444       0.04805       2.00869      0.02392      0.71533  60.16s\n",
      "    445       0.04580       2.02776      0.02259      0.71564  59.99s\n",
      "    446       0.04720       2.02577      0.02330      0.71636  60.10s\n",
      "    447       0.04816       2.02099      0.02383      0.71763  59.91s\n",
      "    448       0.04644       2.02248      0.02296      0.71640  60.12s\n",
      "    449       0.04644       2.02068      0.02298      0.71731  59.87s\n",
      "    450       0.04810       2.04920      0.02347      0.71615  59.98s\n",
      "    451       0.04908       2.04882      0.02396      0.71594  61.87s\n",
      "    452       \u001b[36m0.04511\u001b[0m       2.04380      0.02207      0.71649  59.89s\n",
      "    453       0.04615       2.03446      0.02269      0.71682  60.10s\n",
      "    454       \u001b[36m0.04452\u001b[0m       2.04041      0.02182      0.71754  60.58s\n",
      "    455       0.04708       2.04177      0.02306      0.71827  60.10s\n",
      "    456       0.04790       2.03340      0.02355      0.71579  59.80s\n",
      "    457       0.04593       2.04235      0.02249      0.71628  59.84s\n",
      "    458       0.04908       2.04139      0.02404      0.71720  59.68s\n",
      "    459       0.04640       2.02693      0.02289      0.71470  59.96s\n",
      "    460       0.04823       2.02606      0.02381      0.71607  60.08s\n",
      "    461       0.04640       2.04761      0.02266      0.71668  59.73s\n",
      "    462       0.04685       2.04024      0.02297      0.71555  60.27s\n",
      "    463       0.04622       2.03003      0.02277      0.71585  66.18s\n",
      "    464       0.04748       2.02766      0.02342      0.71627  63.40s\n",
      "    465       0.04519       2.04066      0.02215      0.71618  63.17s\n",
      "    466       0.04791       2.03799      0.02351      0.71711  62.92s\n",
      "    467       \u001b[36m0.04420\u001b[0m       2.04392      0.02162      0.71698  62.87s\n",
      "    468       0.04865       2.04046      0.02384      0.71873  62.81s\n",
      "    469       0.04573       2.04431      0.02237      0.71616  63.85s\n",
      "    470       0.04654       2.03791      0.02284      0.71774  59.62s\n",
      "    471       0.04766       2.02897      0.02349      0.71618  59.40s\n",
      "    472       0.04610       2.04199      0.02257      0.71542  59.28s\n",
      "    473       0.04581       2.04611      0.02239      0.71482  59.52s\n",
      "    474       0.04640       2.04655      0.02267      0.71618  59.41s\n",
      "    475       0.04715       2.05433      0.02295      0.71631  59.35s\n",
      "    476       0.04495       2.07017      0.02171      0.71472  59.57s\n",
      "    477       0.04483       2.06144      0.02175      0.71622  59.53s\n",
      "    478       0.04731       2.05896      0.02298      0.71618  59.70s\n",
      "    479       \u001b[36m0.04321\u001b[0m       2.06730      0.02090      0.71609  59.72s\n",
      "    480       0.04664       2.06169      0.02262      0.71725  59.39s\n",
      "    481       0.04701       2.06066      0.02281      0.71672  59.37s\n",
      "    482       0.04829       2.07279      0.02330      0.71662  59.26s\n",
      "    483       0.04686       2.07168      0.02262      0.71684  59.63s\n",
      "    484       0.04521       2.07213      0.02182      0.71611  59.46s\n",
      "    485       0.04758       2.06967      0.02299      0.71579  59.63s\n",
      "    486       0.04715       2.07100      0.02277      0.71671  59.45s\n",
      "    487       0.04794       2.07296      0.02313      0.71579  61.72s\n",
      "    488       0.04671       2.07816      0.02248      0.71753  59.78s\n",
      "    489       0.04584       2.08758      0.02196      0.71838  59.54s\n",
      "    490       0.04517       2.07066      0.02181      0.71808  59.97s\n",
      "    491       0.04454       2.08099      0.02140      0.71714  59.94s\n",
      "    492       0.04410       2.08835      0.02112      0.71754  59.71s\n",
      "    493       0.04632       2.08288      0.02224      0.71879  59.90s\n",
      "    494       0.04622       2.06535      0.02238      0.71716  59.69s\n",
      "    495       0.04510       2.07571      0.02173      0.71629  59.75s\n",
      "    496       0.04706       2.07829      0.02265      0.71659  59.68s\n",
      "    497       0.04791       2.05992      0.02326      0.71638  59.60s\n",
      "    498       0.04874       2.06935      0.02355      0.71725  59.27s\n",
      "    499       0.04717       2.06334      0.02286      0.71668  59.33s\n",
      "    500       0.04786       2.04965      0.02335      0.71522  59.82s\n",
      "    501       0.04716       2.05220      0.02298      0.71659  59.49s\n",
      "    502       0.04469       2.06302      0.02166      0.71588  59.74s\n",
      "    503       0.04727       2.07038      0.02283      0.71641  59.59s\n",
      "    504       0.04569       2.06683      0.02211      0.71691  59.85s\n",
      "    505       0.04625       2.06470      0.02240      0.71619  62.04s\n",
      "    506       0.04476       2.06293      0.02170      0.71567  59.49s\n",
      "    507       0.04501       2.06854      0.02176      0.71577  59.48s\n",
      "    508       0.04629       2.08356      0.02222      0.71627  59.57s\n",
      "    509       0.04632       2.08035      0.02226      0.71659  59.56s\n",
      "    510       0.04737       2.08602      0.02271      0.71618  59.68s\n",
      "    511       0.04480       2.08927      0.02144      0.71731  59.69s\n",
      "    512       0.04760       2.08850      0.02279      0.71853  60.67s\n",
      "    513       0.04532       2.08263      0.02176      0.71723  59.81s\n",
      "    514       0.04361       2.09701      0.02080      0.71606  59.75s\n",
      "    515       0.04487       2.09350      0.02143      0.71658  59.69s\n",
      "    516       0.04827       2.06789      0.02334      0.71743  59.60s\n",
      "    517       0.04535       2.06694      0.02194      0.71817  59.81s\n",
      "    518       0.04608       2.07669      0.02219      0.71795  59.51s\n",
      "    519       0.04459       2.09057      0.02133      0.71868  59.96s\n",
      "    520       0.04606       2.07227      0.02223      0.71869  59.78s\n",
      "    521       0.04531       2.07742      0.02181      0.71808  59.65s\n",
      "    522       0.04416       2.06892      0.02134      0.71634  59.81s\n",
      "    523       0.04473       2.06783      0.02163      0.71756  62.40s\n",
      "    524       0.04456       2.08904      0.02133      0.71744  59.90s\n",
      "    525       0.04541       2.08224      0.02181      0.71629  60.07s\n",
      "    526       0.04613       2.08149      0.02216      0.71754  60.03s\n",
      "    527       0.04663       2.07724      0.02245      0.71735  60.11s\n",
      "    528       0.04515       2.06558      0.02186      0.71702  60.20s\n",
      "    529       0.04409       2.08132      0.02118      0.71768  60.13s\n",
      "    530       0.04332       2.07322      0.02089      0.71777  60.23s\n",
      "    531       0.04435       2.08818      0.02124      0.71805  60.17s\n",
      "    532       \u001b[36m0.04317\u001b[0m       2.09038      0.02065      0.71637  60.20s\n",
      "    533       0.04698       2.08249      0.02256      0.71765  60.41s\n",
      "    534       0.04704       2.07954      0.02262      0.71732  60.15s\n",
      "    535       0.04505       2.08482      0.02161      0.71786  60.21s\n",
      "    536       0.04442       2.08927      0.02126      0.71775  60.62s\n",
      "    537       0.04351       2.08899      0.02083      0.71619  60.62s\n",
      "    538       0.04751       2.09977      0.02263      0.71744  60.30s\n",
      "    539       0.04520       2.07687      0.02176      0.71799  60.13s\n",
      "    540       0.04651       2.08225      0.02233      0.71684  60.25s\n",
      "Wrote submission to file mega_ensemble4_1451826330751.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atulkumar/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:417: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "process2(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble4\", layers, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import cPickle \n",
    "#f = file('train_data_for2.save', 'wb')\n",
    "#cPickle.dump(train_data_for2, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f.close()\n",
    "#f1 = file('train_data_for2_std.save', 'wb')\n",
    "#cPickle.dump(train_data_for2_std, f1, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f1.close()\n",
    "#f2 = file('train_data_y_for2.save', 'wb')\n",
    "#cPickle.dump(train_data_y_for2, f2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f2.close()\n",
    "#f3 = file('test_data_for2.save', 'wb')\n",
    "#cPickle.dump(test_data_for2, f3, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f3.close()\n",
    "#f4 = file('test_data_for2_std.save', 'wb')\n",
    "#cPickle.dump(test_data_for2_std, f4, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f4.close()\n",
    "\n",
    "#f5 = file('ids.save', 'wb')\n",
    "#cPickle.dump(ids, f5, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "#f5.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle \n",
    "f = file('train_data_for2.save', 'rb')\n",
    "train_data_for2 = cPickle.load(f)\n",
    "f.close()\n",
    "f1 = file('train_data_y_for2.save', 'rb')\n",
    "train_data_y_for2 = cPickle.load(f1)\n",
    "f1.close()\n",
    "f2 = file('test_data_for2.save', 'rb')\n",
    "test_data_for2 = cPickle.load(f2)\n",
    "f2.close()\n",
    "f3 = file('ids.save', 'rb')\n",
    "ids = cPickle.load(f3)\n",
    "f3.close()\n",
    "scaler2 = StandardScaler()\n",
    "train_data_for2_std = scaler2.fit_transform(train_data_for2.toarray())\n",
    "test_data_for2_std = scaler2.transform(test_data_for2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pxgb1 = Process(target=process2, args=(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble1\", layers, ids))\n",
    "pxgb1.start()\n",
    "pxgb1.join()\n",
    "\n",
    "pxgb2 = Process(target=process2, args=(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble2\", layers, ids))\n",
    "pxgb2.start()\n",
    "pxgb2.join()\n",
    "\n",
    "pxgb3 = Process(target=process2, args=(train_data_for2, train_data_for2_std, train_data_y_for2, \\\n",
    "             test_data_for2, test_data_for2_std, \"mega_ensemble3\", layers, ids))\n",
    "pxgb3.start()\n",
    "pxgb3.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean from two submission files\n",
    "df1 = pd.read_csv('mega_ensemble4_1451246644669', index_col= 'VisitNumber')\n",
    "df2 = pd.read_csv('mega_ensemble_no_upc_fln_1451222117007', index_col='VisitNumber')\n",
    "df_concat = pd.concat((df1, df2))\n",
    "by_row_index = df_concat.groupby(df_concat.index)\n",
    "df_means = by_row_index.mean()\n",
    "\n",
    "millis = int(round(time.time() * 1000))\n",
    "filename = 'mega_ensemble_ensemble%d'%(millis)\n",
    "df_means.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717459295878\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, y, test_size=0.5, random_state=56)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train.tolist())\n",
    "dtest = xgb.DMatrix(X_test, label=y_test.tolist())\n",
    "\n",
    "xgb_params = {'max_depth': 8,\n",
    "          'objective': 'multi:softprob',\n",
    "          'eval_metric': 'mlogloss',\n",
    "          'num_class': 38,\n",
    "          'subsample': 0.35,\n",
    "          'colsample_bytree': 1,\n",
    "          'eta': 0.1}\n",
    "num_rounds = 2000\n",
    "\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "bstc = xgb.train(xgb_params, dtrain, num_rounds, evals=watchlist, verbose_eval=False, early_stopping_rounds=25)\n",
    "\n",
    "\n",
    "predictv2 = bstc.predict(xgb.DMatrix(X_test))\n",
    "print(loss_function(predictv2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission to file improved_1451748103378.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(feature_matrix, label=y)\n",
    "\"\"\"\n",
    " xgb_params = {}\n",
    "    # use softmax multi-class classification\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    # scale weight of positive examples\n",
    "    param['eta'] = 0.05\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 38\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    #param['min_child_weight'] = 2\n",
    "    param['subsample'] = 0.9\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['gamma'] = 1\n",
    "    \n",
    "num_rounds = 4200\n",
    "\"\"\"\n",
    "xgb_params = {'max_depth': 8,\n",
    "          'objective': 'multi:softprob',\n",
    "          'eval_metric': 'mlogloss',\n",
    "          'num_class': 38,\n",
    "          'subsample': 0.35,\n",
    "          'colsample_bytree': 1,\n",
    "          'eta': 0.1}\n",
    "num_rounds = 2000\n",
    "\n",
    "bstc = xgb.train(xgb_params, dtrain, num_rounds, verbose_eval=False)\n",
    "\n",
    "pred = bstc.predict(xgb.DMatrix(feature_matrix_test))\n",
    "make_submission_ensemble(pred, ids, encoder, \"improved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
