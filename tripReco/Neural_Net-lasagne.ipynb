{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(y, pred):\n",
    "    eps = 1e-15\n",
    "    total = 0.\n",
    "    for i in range(len(y)):\n",
    "        p = max(min(pred[i][y[i]], (1 - eps)), eps)\n",
    "        total += math.log(p)\n",
    "    return -(total/len(y))\n",
    "\n",
    "def OneHotEncoder(col):\n",
    "    #(values,counts) = np.unique(col,return_counts=True)\n",
    "    #values[np.argsort(counts)[-2000:]]\n",
    "    col = np.nan_to_num(col)\n",
    "    uniques = np.unique(col)\n",
    "    keymap = dict((key, i) for i, key in enumerate(uniques))\n",
    "    \n",
    "    total_pts = len(col)\n",
    "    num_labels = len(uniques)\n",
    "    \n",
    "    spmat = sparse.lil_matrix((total_pts, num_labels))\n",
    "    for j, val in enumerate(col):\n",
    "        if val[0] in keymap:\n",
    "            spmat[j, keymap[val[0]]] = 1\n",
    "    return keymap, spmat\n",
    "\n",
    "def getY(col):\n",
    "    uniques = np.unique(col)\n",
    "    keymap = dict((key, i) for i, key in enumerate(uniques))\n",
    "    newY = np.array([keymap[x] for x in col])\n",
    "    return keymap, newY\n",
    "def prepareForCountVector(df, columnName, dictCount=2000):\n",
    "    col = df[columnName].dropna()\n",
    "    col = col.fillna('')\n",
    "\n",
    "    counts = col.value_counts()\n",
    "    topk_dict = counts.iloc[0:min(dictCount, len(col))].index\n",
    "    \n",
    "    topk_dict = set(topk_dict).union(set(topk_dict))\n",
    "    topk = df[columnName].apply(lambda x: '%s%d'%(columnName, x) if x in topk_dict else '%sother'%(columnName))\n",
    " \n",
    "    topk_se = pd.Series(topk, name=columnName)\n",
    "    df_topk = pd.concat([topk_se, df['VisitNumber']], axis=1)\n",
    "    return topk_dict, df_topk\n",
    "\n",
    "def getCountVector(df, columnName, isWords, vec=None):\n",
    "    if isWords:\n",
    "        df[columnName] = df[columnName].fillna('')\n",
    "    df_topk_gpy = df.groupby('VisitNumber')\n",
    "    df_topk_list = df_topk_gpy.apply(lambda x: list(x[columnName]))\n",
    "    topk_flat = df_topk_list.str.join(' ')\n",
    "    \n",
    "    if not vec: \n",
    "        vec = CountVectorizer() \n",
    "        vec.fit(topk_flat)    \n",
    "    \n",
    "    wc = vec.transform(topk_flat)\n",
    "    wcar = wc.toarray()\n",
    "    \n",
    "    words_count = topk_flat.apply(lambda x : len(x.split(' '))).reshape(-1,1)\n",
    "    ret = None\n",
    "    if isWords:\n",
    "        words_len = topk_flat.apply(lambda x : len(x)).reshape(-1,1)\n",
    "        ret = np.column_stack([wcar, words_count, words_len])\n",
    "    else:\n",
    "        ret = np.column_stack([wcar, words_count])\n",
    "    \n",
    "    return vec, ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TripType\tVisitNumber\tWeekday\tUpc\tScanCount\tDepartmentDescription\tFinelineNumber\n",
    "train_df = pd.read_csv('train.csv')\n",
    "num_train = np.shape(train_df)[0]\n",
    "\n",
    "df_y = train_df[['VisitNumber', 'TripType']].groupby('VisitNumber').first()\n",
    "df_y = df_y.reset_index()\n",
    "\n",
    "y_map, y = getY(df_y.TripType)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_w = train_df[['VisitNumber', 'Weekday']].groupby('VisitNumber').first()\n",
    "df_w = df_w.reset_index()\n",
    "week = OneHotEncoder(df_w.Weekday)\n",
    "\n",
    "is_wknd = np.array(df_w['Weekday']=='Sunday')\n",
    "is_wknd = is_wknd.reshape(-1,1)\n",
    "\n",
    "df_upc = prepareForCountVector(train_df, 'Upc')\n",
    "upc = getCountVector(df_upc[1], 'Upc', False)\n",
    "\n",
    "df_fln = prepareForCountVector(train_df, 'FinelineNumber')\n",
    "fln = getCountVector(df_fln[1], 'FinelineNumber', False)\n",
    "\n",
    "words = getCountVector(train_df, 'DepartmentDescription', True)\n",
    "\n",
    "df_ScanCount = train_df[['VisitNumber', 'ScanCount']].groupby('VisitNumber').sum()\n",
    "df_ScanCount = df_ScanCount.reset_index()\n",
    "scancount = np.array(df_ScanCount.ScanCount)\n",
    "scancount = scancount.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'tocsr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e0162389c6be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfeature_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfeature_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mfeature_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'tocsr'"
     ]
    }
   ],
   "source": [
    "feature_matrix = []\n",
    "feature_matrix.append(week[1])\n",
    "feature_matrix.append(is_wknd)\n",
    "feature_matrix.append(upc[1])\n",
    "feature_matrix.append(fln[1])\n",
    "feature_matrix.append(words[1])\n",
    "feature_matrix.append(scancount)\n",
    "\n",
    "feature_matrix = sparse.hstack(feature_matrix).tocsr()\n",
    "feature_matrix = StandardScaler().fit_transform(feature_matrix.toarray())\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95674, 4127)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = StandardScaler().fit_transform(feature_matrix.toarray())\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(feature_matrix, y, test_size=0.33, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    input_len = inputs.shape[0] \n",
    "    assert input_len == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(input_len)\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, input_len - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 4127),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "                network, width, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 37, nonlinearity=softmax)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "network = build_custom_mlp(input_var)\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "#loss = loss_function(target_var, prediction)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "    \n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Bad input argument to theano function with name \"<ipython-input-38-df75b12cf68a>:22\"  at index 1(0-based)', 'TensorType(int32, vector) cannot store a value of dtype int64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to int32, or 2) set \"allow_input_downcast=True\" when calling \"function\".', array([ 5, 24,  5,  5, 31,  2,  5, 27, 27, 36,  4,  6, 30, 12, 23, 19,  3,\n       21,  7, 16, 17, 32,  6, 28,  5, 36,  5,  5, 37,  4, 16,  5, 36, 30,\n        5, 30,  5, 28, 31, 33,  5, 12, 31,  5, 29, 18,  6, 32, 19, 27, 32,\n        4,  0, 17, 34,  5, 30, 10, 32,  9, 31, 32,  4, 17, 17, 32, 37, 32,\n       19, 32,  6, 31,  4,  5, 34, 17, 30, 37,  0, 24,  6,  5,  5, 31,  4,\n       19, 22,  5,  5, 31, 28, 28,  5, 31,  6, 22, 37, 17,  4, 17, 24, 27,\n       31,  4,  0, 37,  5, 31, 21, 31, 32, 37,  4, 34, 21,  6, 37, 32, 16,\n       37,  2,  6,  6, 30, 28, 17,  6, 16,  6, 34,  6, 32, 22,  5, 31, 28,\n       30,  6,  5, 25, 35,  6,  1,  6, 32,  5, 31, 32,  6, 14,  2,  6,  6,\n       16,  6,  4, 28, 30,  5, 31, 34, 35,  6, 24, 17, 31, 31, 22, 31, 28,\n        2, 17,  2, 31, 12,  6,  0,  2, 25, 31, 32,  5, 12, 32,  2,  2, 30,\n       37, 32,  2,  5, 34, 26, 27, 13, 31, 28, 34,  6, 31, 31, 29, 24,  0,\n        4, 25, 31, 31, 37,  6, 27,  4, 37, 31,  5, 30, 32, 29, 31, 13, 31,\n        6,  4, 37, 21,  4, 31,  0,  5, 31, 22, 37,  5,  6, 14, 22, 31,  5,\n       28, 33, 22,  4,  6,  4,  4, 37,  2, 29,  4,  4, 27, 37, 31, 37,  6,\n        9, 11,  2, 24, 34, 17,  6, 17,  2, 31, 34, 31, 30,  6,  6, 14,  5,\n        0,  6, 32, 30,  5, 29, 24, 31,  2, 32, 31,  5,  5, 37, 16, 31,  6,\n       37,  5,  5,  6,  5, 22,  5, 37, 32, 34,  0,  4, 11,  4,  6, 37,  6,\n       23, 18, 29, 17, 37,  3, 16,  4,  4, 32,  6, 27, 28, 19, 37,  5, 34,\n       10, 32, 17, 29, 24, 32,  2, 21, 31, 29, 31,  0,  6, 30, 26, 17,  1,\n       37, 17,  4,  5, 24, 30, 17, 31, 29, 32, 16,  5,  5,  5,  4, 31, 37,\n        9, 32, 37, 31, 37,  6, 25,  5,  4, 37,  4, 32, 28, 28, 17, 27, 31,\n       37,  3, 31,  0, 32, 32,  5, 31, 24, 13, 37,  5,  4, 23,  4, 37, 37,\n        2, 31, 37,  4,  6, 16,  6,  2, 34,  0,  6, 32,  5, 32,  1, 37, 23,\n       29,  3, 30, 30,  4, 35,  4, 30, 24, 27,  6, 30,  0,  5, 16,  5,  6,\n        4,  5, 33,  9, 31, 24, 36, 21, 31, 37, 19, 35, 31, 26, 37, 31, 37,\n        5, 16,  5, 30, 30, 23,  0,  5,  6, 37,  0, 37,  5,  6,  5, 37, 37,\n        5,  5, 30, 13, 31,  7,  5, 17, 31, 31, 32, 37, 37, 14,  5,  0, 31,\n       37, 37,  5,  5,  4, 12,  0,  6,  4,  6, 35,  2,  2, 37,  0, 37, 31,\n       30,  4,  2, 31,  5, 26, 37]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-909bc7fa3599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/atulkumar/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[1;32m    785\u001b[0m                             \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/atulkumar/anaconda/lib/python2.7/site-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0;34m'\"function\".'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                             % (self, data.dtype, self.dtype))\n\u001b[0;32m--> 139\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 elif (allow_downcast is None and\n\u001b[1;32m    141\u001b[0m                         \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Bad input argument to theano function with name \"<ipython-input-38-df75b12cf68a>:22\"  at index 1(0-based)', 'TensorType(int32, vector) cannot store a value of dtype int64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to int32, or 2) set \"allow_input_downcast=True\" when calling \"function\".', array([ 5, 24,  5,  5, 31,  2,  5, 27, 27, 36,  4,  6, 30, 12, 23, 19,  3,\n       21,  7, 16, 17, 32,  6, 28,  5, 36,  5,  5, 37,  4, 16,  5, 36, 30,\n        5, 30,  5, 28, 31, 33,  5, 12, 31,  5, 29, 18,  6, 32, 19, 27, 32,\n        4,  0, 17, 34,  5, 30, 10, 32,  9, 31, 32,  4, 17, 17, 32, 37, 32,\n       19, 32,  6, 31,  4,  5, 34, 17, 30, 37,  0, 24,  6,  5,  5, 31,  4,\n       19, 22,  5,  5, 31, 28, 28,  5, 31,  6, 22, 37, 17,  4, 17, 24, 27,\n       31,  4,  0, 37,  5, 31, 21, 31, 32, 37,  4, 34, 21,  6, 37, 32, 16,\n       37,  2,  6,  6, 30, 28, 17,  6, 16,  6, 34,  6, 32, 22,  5, 31, 28,\n       30,  6,  5, 25, 35,  6,  1,  6, 32,  5, 31, 32,  6, 14,  2,  6,  6,\n       16,  6,  4, 28, 30,  5, 31, 34, 35,  6, 24, 17, 31, 31, 22, 31, 28,\n        2, 17,  2, 31, 12,  6,  0,  2, 25, 31, 32,  5, 12, 32,  2,  2, 30,\n       37, 32,  2,  5, 34, 26, 27, 13, 31, 28, 34,  6, 31, 31, 29, 24,  0,\n        4, 25, 31, 31, 37,  6, 27,  4, 37, 31,  5, 30, 32, 29, 31, 13, 31,\n        6,  4, 37, 21,  4, 31,  0,  5, 31, 22, 37,  5,  6, 14, 22, 31,  5,\n       28, 33, 22,  4,  6,  4,  4, 37,  2, 29,  4,  4, 27, 37, 31, 37,  6,\n        9, 11,  2, 24, 34, 17,  6, 17,  2, 31, 34, 31, 30,  6,  6, 14,  5,\n        0,  6, 32, 30,  5, 29, 24, 31,  2, 32, 31,  5,  5, 37, 16, 31,  6,\n       37,  5,  5,  6,  5, 22,  5, 37, 32, 34,  0,  4, 11,  4,  6, 37,  6,\n       23, 18, 29, 17, 37,  3, 16,  4,  4, 32,  6, 27, 28, 19, 37,  5, 34,\n       10, 32, 17, 29, 24, 32,  2, 21, 31, 29, 31,  0,  6, 30, 26, 17,  1,\n       37, 17,  4,  5, 24, 30, 17, 31, 29, 32, 16,  5,  5,  5,  4, 31, 37,\n        9, 32, 37, 31, 37,  6, 25,  5,  4, 37,  4, 32, 28, 28, 17, 27, 31,\n       37,  3, 31,  0, 32, 32,  5, 31, 24, 13, 37,  5,  4, 23,  4, 37, 37,\n        2, 31, 37,  4,  6, 16,  6,  2, 34,  0,  6, 32,  5, 32,  1, 37, 23,\n       29,  3, 30, 30,  4, 35,  4, 30, 24, 27,  6, 30,  0,  5, 16,  5,  6,\n        4,  5, 33,  9, 31, 24, 36, 21, 31, 37, 19, 35, 31, 26, 37, 31, 37,\n        5, 16,  5, 30, 30, 23,  0,  5,  6, 37,  0, 37,  5,  6,  5, 37, 37,\n        5,  5, 30, 13, 31,  7,  5, 17, 31, 31, 32, 37, 37, 14,  5,  0, 31,\n       37, 37,  5,  5,  4, 12,  0,  6,  4,  6, 35,  2,  2, 37,  0, 37, 31,\n       30,  4,  2, 31,  5, 26, 37]))"
     ]
    }
   ],
   "source": [
    "num_epochs=5 #500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
